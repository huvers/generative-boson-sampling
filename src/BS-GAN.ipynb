{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, save_img, img_to_array\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'boson/'\n",
    "RUN_FOLDER = '../run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/data.sav'\n",
    "\n",
    "with open(data_file, \"rb\") as f:\n",
    "    X, Y = pickle.load(f)\n",
    "    \n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "split = round(len(X)*.9)\n",
    "\n",
    "x_train = X[:split]\n",
    "y_train = Y[:split]\n",
    "\n",
    "x_test  = X[split:]\n",
    "y_test  = Y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 35)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Dense Layer GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_num = 16\n",
    "\n",
    "gan = GAN(input_dim = (y_train.shape[1],)\n",
    "        , discriminator_dense_layers = [64,64,32,128]\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0008\n",
    "        , generator_dense_layers = [64,64,32,y_train.shape[1]]\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = z_num\n",
    "        )\n",
    "\n",
    "if mode == 'build':\n",
    "    gan.save(RUN_FOLDER)\n",
    "else:\n",
    "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 35)]              0         \n",
      "_________________________________________________________________\n",
      "discriminator_dense_0 (Dense (None, 64)                2304      \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "discriminator_dense_1 (Dense (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "discriminator_dense_2 (Dense (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "discriminator_dense_3 (Dense (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_g_layer_0 (Dense)      (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_87 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_g_layer_1 (Dense)      (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_88 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_g_layer_2 (Dense)      (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_89 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_g_layer_3 (Dense)      (None, 35)                1155      \n",
      "_________________________________________________________________\n",
      "activation_90 (Activation)   (None, 35)                0         \n",
      "=================================================================\n",
      "Total params: 9,187\n",
      "Trainable params: 8,835\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5000\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.696)(R 0.695, F 0.698)] [D acc: (0.188)(0.188, 0.188)] [G loss: 0.693] [G acc: 0.500]\n",
      "1 [D loss: (0.693)(R 0.693, F 0.693)] [D acc: (0.438)(0.562, 0.312)] [G loss: 0.717] [G acc: 0.188]\n",
      "2 [D loss: (0.693)(R 0.694, F 0.691)] [D acc: (0.500)(0.375, 0.625)] [G loss: 0.719] [G acc: 0.125]\n",
      "3 [D loss: (0.693)(R 0.694, F 0.692)] [D acc: (0.500)(0.438, 0.562)] [G loss: 0.719] [G acc: 0.000]\n",
      "4 [D loss: (0.690)(R 0.695, F 0.685)] [D acc: (0.500)(0.312, 0.688)] [G loss: 0.720] [G acc: 0.062]\n",
      "5 [D loss: (0.689)(R 0.695, F 0.684)] [D acc: (0.562)(0.312, 0.812)] [G loss: 0.739] [G acc: 0.000]\n",
      "6 [D loss: (0.690)(R 0.696, F 0.684)] [D acc: (0.500)(0.188, 0.812)] [G loss: 0.740] [G acc: 0.000]\n",
      "7 [D loss: (0.685)(R 0.696, F 0.674)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.763] [G acc: 0.000]\n",
      "8 [D loss: (0.684)(R 0.696, F 0.672)] [D acc: (0.469)(0.000, 0.938)] [G loss: 0.787] [G acc: 0.000]\n",
      "9 [D loss: (0.681)(R 0.698, F 0.664)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.799] [G acc: 0.000]\n",
      "10 [D loss: (0.678)(R 0.700, F 0.656)] [D acc: (0.531)(0.062, 1.000)] [G loss: 0.778] [G acc: 0.000]\n",
      "11 [D loss: (0.670)(R 0.699, F 0.641)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.819] [G acc: 0.000]\n",
      "12 [D loss: (0.664)(R 0.700, F 0.628)] [D acc: (0.531)(0.062, 1.000)] [G loss: 0.806] [G acc: 0.000]\n",
      "13 [D loss: (0.666)(R 0.702, F 0.631)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.818] [G acc: 0.000]\n",
      "14 [D loss: (0.647)(R 0.702, F 0.591)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.896] [G acc: 0.000]\n",
      "15 [D loss: (0.642)(R 0.709, F 0.576)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.889] [G acc: 0.000]\n",
      "16 [D loss: (0.638)(R 0.707, F 0.568)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.914] [G acc: 0.000]\n",
      "17 [D loss: (0.629)(R 0.705, F 0.553)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.929] [G acc: 0.000]\n",
      "18 [D loss: (0.617)(R 0.714, F 0.521)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.990] [G acc: 0.000]\n",
      "19 [D loss: (0.623)(R 0.711, F 0.536)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.066] [G acc: 0.000]\n",
      "20 [D loss: (0.601)(R 0.716, F 0.487)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.175] [G acc: 0.000]\n",
      "21 [D loss: (0.584)(R 0.713, F 0.456)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.180] [G acc: 0.000]\n",
      "22 [D loss: (0.581)(R 0.711, F 0.452)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.218] [G acc: 0.000]\n",
      "23 [D loss: (0.578)(R 0.719, F 0.437)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.226] [G acc: 0.000]\n",
      "24 [D loss: (0.558)(R 0.714, F 0.403)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.298] [G acc: 0.000]\n",
      "25 [D loss: (0.563)(R 0.716, F 0.409)] [D acc: (0.531)(0.062, 1.000)] [G loss: 1.319] [G acc: 0.000]\n",
      "26 [D loss: (0.535)(R 0.714, F 0.356)] [D acc: (0.562)(0.125, 1.000)] [G loss: 1.639] [G acc: 0.000]\n",
      "27 [D loss: (0.537)(R 0.725, F 0.349)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.445] [G acc: 0.000]\n",
      "28 [D loss: (0.497)(R 0.726, F 0.268)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.656] [G acc: 0.000]\n",
      "29 [D loss: (0.541)(R 0.731, F 0.350)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.677] [G acc: 0.000]\n",
      "30 [D loss: (0.484)(R 0.724, F 0.245)] [D acc: (0.500)(0.000, 1.000)] [G loss: 1.448] [G acc: 0.000]\n",
      "31 [D loss: (0.483)(R 0.720, F 0.246)] [D acc: (0.531)(0.062, 1.000)] [G loss: 1.590] [G acc: 0.000]\n",
      "32 [D loss: (0.471)(R 0.715, F 0.228)] [D acc: (0.562)(0.125, 1.000)] [G loss: 2.040] [G acc: 0.000]\n",
      "33 [D loss: (0.480)(R 0.715, F 0.245)] [D acc: (0.562)(0.125, 1.000)] [G loss: 1.890] [G acc: 0.000]\n",
      "34 [D loss: (0.469)(R 0.720, F 0.219)] [D acc: (0.594)(0.188, 1.000)] [G loss: 2.070] [G acc: 0.000]\n",
      "35 [D loss: (0.482)(R 0.717, F 0.247)] [D acc: (0.531)(0.062, 1.000)] [G loss: 1.942] [G acc: 0.000]\n",
      "36 [D loss: (0.449)(R 0.709, F 0.189)] [D acc: (0.688)(0.375, 1.000)] [G loss: 2.515] [G acc: 0.000]\n",
      "37 [D loss: (0.445)(R 0.708, F 0.183)] [D acc: (0.625)(0.250, 1.000)] [G loss: 2.485] [G acc: 0.000]\n",
      "38 [D loss: (0.467)(R 0.726, F 0.208)] [D acc: (0.594)(0.188, 1.000)] [G loss: 2.487] [G acc: 0.000]\n",
      "39 [D loss: (0.457)(R 0.716, F 0.199)] [D acc: (0.625)(0.250, 1.000)] [G loss: 2.477] [G acc: 0.000]\n",
      "40 [D loss: (0.440)(R 0.720, F 0.159)] [D acc: (0.562)(0.125, 1.000)] [G loss: 2.290] [G acc: 0.000]\n",
      "41 [D loss: (0.458)(R 0.703, F 0.212)] [D acc: (0.781)(0.562, 1.000)] [G loss: 2.891] [G acc: 0.000]\n",
      "42 [D loss: (0.429)(R 0.726, F 0.131)] [D acc: (0.625)(0.250, 1.000)] [G loss: 2.631] [G acc: 0.000]\n",
      "43 [D loss: (0.400)(R 0.701, F 0.099)] [D acc: (0.812)(0.625, 1.000)] [G loss: 3.157] [G acc: 0.000]\n",
      "44 [D loss: (0.455)(R 0.724, F 0.187)] [D acc: (0.688)(0.375, 1.000)] [G loss: 2.662] [G acc: 0.000]\n",
      "45 [D loss: (0.413)(R 0.719, F 0.107)] [D acc: (0.812)(0.625, 1.000)] [G loss: 3.223] [G acc: 0.000]\n",
      "46 [D loss: (0.387)(R 0.698, F 0.076)] [D acc: (0.875)(0.750, 1.000)] [G loss: 3.076] [G acc: 0.000]\n",
      "47 [D loss: (0.403)(R 0.714, F 0.093)] [D acc: (0.812)(0.625, 1.000)] [G loss: 2.817] [G acc: 0.000]\n",
      "48 [D loss: (0.380)(R 0.692, F 0.067)] [D acc: (0.875)(0.750, 1.000)] [G loss: 3.118] [G acc: 0.000]\n",
      "49 [D loss: (0.408)(R 0.690, F 0.126)] [D acc: (0.875)(0.750, 1.000)] [G loss: 3.374] [G acc: 0.000]\n",
      "50 [D loss: (0.381)(R 0.708, F 0.055)] [D acc: (0.906)(0.812, 1.000)] [G loss: 3.393] [G acc: 0.000]\n",
      "51 [D loss: (0.374)(R 0.684, F 0.064)] [D acc: (0.938)(0.875, 1.000)] [G loss: 4.048] [G acc: 0.000]\n",
      "52 [D loss: (0.389)(R 0.702, F 0.077)] [D acc: (0.906)(0.812, 1.000)] [G loss: 3.795] [G acc: 0.000]\n",
      "53 [D loss: (0.369)(R 0.685, F 0.053)] [D acc: (0.906)(0.812, 1.000)] [G loss: 3.974] [G acc: 0.000]\n",
      "54 [D loss: (0.367)(R 0.693, F 0.041)] [D acc: (0.844)(0.688, 1.000)] [G loss: 4.002] [G acc: 0.000]\n",
      "55 [D loss: (0.385)(R 0.680, F 0.089)] [D acc: (0.938)(0.875, 1.000)] [G loss: 4.052] [G acc: 0.000]\n",
      "56 [D loss: (0.355)(R 0.673, F 0.036)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.991] [G acc: 0.000]\n",
      "57 [D loss: (0.368)(R 0.687, F 0.049)] [D acc: (0.875)(0.750, 1.000)] [G loss: 4.369] [G acc: 0.000]\n",
      "58 [D loss: (0.356)(R 0.685, F 0.027)] [D acc: (0.938)(0.875, 1.000)] [G loss: 4.554] [G acc: 0.000]\n",
      "59 [D loss: (0.374)(R 0.674, F 0.073)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.341] [G acc: 0.000]\n",
      "60 [D loss: (0.372)(R 0.665, F 0.079)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.000] [G acc: 0.000]\n",
      "61 [D loss: (0.350)(R 0.667, F 0.032)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.029] [G acc: 0.000]\n",
      "62 [D loss: (0.352)(R 0.667, F 0.037)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.674] [G acc: 0.000]\n",
      "63 [D loss: (0.362)(R 0.664, F 0.059)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.802] [G acc: 0.000]\n",
      "64 [D loss: (0.347)(R 0.680, F 0.014)] [D acc: (0.906)(0.812, 1.000)] [G loss: 4.671] [G acc: 0.000]\n",
      "65 [D loss: (0.372)(R 0.670, F 0.075)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.031] [G acc: 0.000]\n",
      "66 [D loss: (0.352)(R 0.670, F 0.033)] [D acc: (0.875)(0.750, 1.000)] [G loss: 6.469] [G acc: 0.000]\n",
      "67 [D loss: (0.346)(R 0.662, F 0.031)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.372] [G acc: 0.000]\n",
      "68 [D loss: (0.366)(R 0.680, F 0.052)] [D acc: (0.906)(0.812, 1.000)] [G loss: 5.757] [G acc: 0.000]\n",
      "69 [D loss: (0.335)(R 0.656, F 0.015)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.662] [G acc: 0.000]\n",
      "70 [D loss: (0.345)(R 0.649, F 0.041)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.922] [G acc: 0.000]\n",
      "71 [D loss: (0.342)(R 0.648, F 0.037)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.852] [G acc: 0.000]\n",
      "72 [D loss: (0.343)(R 0.657, F 0.030)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.288] [G acc: 0.000]\n",
      "73 [D loss: (0.355)(R 0.692, F 0.019)] [D acc: (0.875)(0.750, 1.000)] [G loss: 7.953] [G acc: 0.000]\n",
      "74 [D loss: (0.345)(R 0.660, F 0.030)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.072] [G acc: 0.000]\n",
      "75 [D loss: (0.334)(R 0.642, F 0.025)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.956] [G acc: 0.000]\n",
      "76 [D loss: (0.336)(R 0.645, F 0.027)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.517] [G acc: 0.000]\n",
      "77 [D loss: (0.345)(R 0.639, F 0.051)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.048] [G acc: 0.000]\n",
      "78 [D loss: (0.329)(R 0.639, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.191] [G acc: 0.000]\n",
      "79 [D loss: (0.334)(R 0.652, F 0.016)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.944] [G acc: 0.000]\n",
      "80 [D loss: (0.323)(R 0.632, F 0.014)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.203] [G acc: 0.000]\n",
      "81 [D loss: (0.327)(R 0.641, F 0.014)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.725] [G acc: 0.000]\n",
      "82 [D loss: (0.334)(R 0.636, F 0.031)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.404] [G acc: 0.000]\n",
      "83 [D loss: (0.318)(R 0.629, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.985] [G acc: 0.000]\n",
      "84 [D loss: (0.328)(R 0.623, F 0.032)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.346] [G acc: 0.000]\n",
      "85 [D loss: (0.318)(R 0.629, F 0.008)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.548] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 [D loss: (0.334)(R 0.659, F 0.010)] [D acc: (0.938)(0.875, 1.000)] [G loss: 8.748] [G acc: 0.000]\n",
      "87 [D loss: (0.329)(R 0.647, F 0.011)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.472] [G acc: 0.000]\n",
      "88 [D loss: (0.322)(R 0.625, F 0.018)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.080] [G acc: 0.000]\n",
      "89 [D loss: (0.312)(R 0.617, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.440] [G acc: 0.000]\n",
      "90 [D loss: (0.318)(R 0.618, F 0.018)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.946] [G acc: 0.000]\n",
      "91 [D loss: (0.319)(R 0.630, F 0.008)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.498] [G acc: 0.000]\n",
      "92 [D loss: (0.325)(R 0.623, F 0.026)] [D acc: (0.938)(0.875, 1.000)] [G loss: 9.906] [G acc: 0.000]\n",
      "93 [D loss: (0.302)(R 0.596, F 0.009)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.104] [G acc: 0.000]\n",
      "94 [D loss: (0.314)(R 0.602, F 0.026)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.452] [G acc: 0.000]\n",
      "95 [D loss: (0.301)(R 0.595, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.938] [G acc: 0.000]\n",
      "96 [D loss: (0.304)(R 0.603, F 0.006)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.059] [G acc: 0.000]\n",
      "97 [D loss: (0.310)(R 0.603, F 0.018)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.135] [G acc: 0.000]\n",
      "98 [D loss: (0.297)(R 0.587, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.047] [G acc: 0.000]\n",
      "99 [D loss: (0.297)(R 0.586, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.118] [G acc: 0.000]\n",
      "100 [D loss: (0.294)(R 0.584, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.886] [G acc: 0.000]\n",
      "101 [D loss: (0.299)(R 0.586, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.771] [G acc: 0.000]\n",
      "102 [D loss: (0.291)(R 0.576, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.746] [G acc: 0.000]\n",
      "103 [D loss: (0.293)(R 0.571, F 0.015)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.595] [G acc: 0.000]\n",
      "104 [D loss: (0.289)(R 0.565, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.966] [G acc: 0.000]\n",
      "105 [D loss: (0.290)(R 0.576, F 0.003)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.618] [G acc: 0.000]\n",
      "106 [D loss: (0.287)(R 0.574, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.915] [G acc: 0.000]\n",
      "107 [D loss: (0.290)(R 0.557, F 0.023)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.977] [G acc: 0.000]\n",
      "108 [D loss: (0.287)(R 0.562, F 0.011)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.833] [G acc: 0.000]\n",
      "109 [D loss: (0.279)(R 0.557, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.999] [G acc: 0.000]\n",
      "110 [D loss: (0.290)(R 0.564, F 0.015)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.106] [G acc: 0.000]\n",
      "111 [D loss: (0.277)(R 0.549, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.789] [G acc: 0.000]\n",
      "112 [D loss: (0.276)(R 0.553, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.196] [G acc: 0.000]\n",
      "113 [D loss: (0.275)(R 0.532, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.989] [G acc: 0.000]\n",
      "114 [D loss: (0.283)(R 0.538, F 0.028)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.116] [G acc: 0.000]\n",
      "115 [D loss: (0.271)(R 0.537, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.644] [G acc: 0.000]\n",
      "116 [D loss: (0.265)(R 0.526, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.476] [G acc: 0.000]\n",
      "117 [D loss: (0.287)(R 0.538, F 0.036)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.996] [G acc: 0.000]\n",
      "118 [D loss: (0.267)(R 0.531, F 0.003)] [D acc: (0.969)(0.938, 1.000)] [G loss: 11.563] [G acc: 0.000]\n",
      "119 [D loss: (0.286)(R 0.539, F 0.032)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.859] [G acc: 0.000]\n",
      "120 [D loss: (0.254)(R 0.503, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.476] [G acc: 0.000]\n",
      "121 [D loss: (0.265)(R 0.530, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.157] [G acc: 0.000]\n",
      "122 [D loss: (0.256)(R 0.512, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.772] [G acc: 0.000]\n",
      "123 [D loss: (0.261)(R 0.497, F 0.026)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.600] [G acc: 0.000]\n",
      "124 [D loss: (0.263)(R 0.521, F 0.004)] [D acc: (0.969)(0.938, 1.000)] [G loss: 11.275] [G acc: 0.000]\n",
      "125 [D loss: (0.263)(R 0.487, F 0.039)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.113] [G acc: 0.000]\n",
      "126 [D loss: (0.289)(R 0.522, F 0.056)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.202] [G acc: 0.000]\n",
      "127 [D loss: (0.251)(R 0.499, F 0.002)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.938] [G acc: 0.000]\n",
      "128 [D loss: (0.265)(R 0.529, F 0.002)] [D acc: (0.969)(0.938, 1.000)] [G loss: 10.317] [G acc: 0.000]\n",
      "129 [D loss: (0.246)(R 0.486, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.940] [G acc: 0.000]\n",
      "130 [D loss: (0.246)(R 0.490, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.474] [G acc: 0.000]\n",
      "131 [D loss: (0.255)(R 0.510, F 0.001)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.632] [G acc: 0.000]\n",
      "132 [D loss: (0.243)(R 0.464, F 0.021)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.059] [G acc: 0.000]\n",
      "133 [D loss: (0.244)(R 0.472, F 0.016)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.970] [G acc: 0.000]\n",
      "134 [D loss: (0.291)(R 0.483, F 0.100)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.765] [G acc: 0.000]\n",
      "135 [D loss: (0.225)(R 0.449, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.258] [G acc: 0.000]\n",
      "136 [D loss: (0.216)(R 0.431, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.203] [G acc: 0.000]\n",
      "137 [D loss: (0.245)(R 0.440, F 0.049)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.844] [G acc: 0.000]\n",
      "138 [D loss: (0.226)(R 0.445, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.331] [G acc: 0.000]\n",
      "139 [D loss: (0.235)(R 0.459, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.906] [G acc: 0.000]\n",
      "140 [D loss: (0.218)(R 0.430, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.054] [G acc: 0.000]\n",
      "141 [D loss: (0.213)(R 0.425, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.036] [G acc: 0.000]\n",
      "142 [D loss: (0.228)(R 0.445, F 0.012)] [D acc: (0.969)(0.938, 1.000)] [G loss: 10.523] [G acc: 0.000]\n",
      "143 [D loss: (0.201)(R 0.401, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.743] [G acc: 0.000]\n",
      "144 [D loss: (0.201)(R 0.401, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.512] [G acc: 0.000]\n",
      "145 [D loss: (0.210)(R 0.419, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.903] [G acc: 0.000]\n",
      "146 [D loss: (0.194)(R 0.386, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.922] [G acc: 0.000]\n",
      "147 [D loss: (0.207)(R 0.411, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.453] [G acc: 0.000]\n",
      "148 [D loss: (0.196)(R 0.391, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.667] [G acc: 0.000]\n",
      "149 [D loss: (0.198)(R 0.388, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.534] [G acc: 0.000]\n",
      "150 [D loss: (0.218)(R 0.393, F 0.043)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.317] [G acc: 0.062]\n",
      "151 [D loss: (0.207)(R 0.412, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.702] [G acc: 0.000]\n",
      "152 [D loss: (0.215)(R 0.364, F 0.066)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.696] [G acc: 0.000]\n",
      "153 [D loss: (0.223)(R 0.395, F 0.050)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.220] [G acc: 0.000]\n",
      "154 [D loss: (0.182)(R 0.362, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.469] [G acc: 0.000]\n",
      "155 [D loss: (0.183)(R 0.365, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.605] [G acc: 0.000]\n",
      "156 [D loss: (0.183)(R 0.350, F 0.016)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.268] [G acc: 0.000]\n",
      "157 [D loss: (0.173)(R 0.345, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.465] [G acc: 0.000]\n",
      "158 [D loss: (0.181)(R 0.359, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.318] [G acc: 0.000]\n",
      "159 [D loss: (0.181)(R 0.346, F 0.016)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.408] [G acc: 0.000]\n",
      "160 [D loss: (0.152)(R 0.302, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.995] [G acc: 0.000]\n",
      "161 [D loss: (0.175)(R 0.347, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.052] [G acc: 0.000]\n",
      "162 [D loss: (0.167)(R 0.309, F 0.025)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.033] [G acc: 0.000]\n",
      "163 [D loss: (0.168)(R 0.336, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.461] [G acc: 0.000]\n",
      "164 [D loss: (0.196)(R 0.323, F 0.070)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.961] [G acc: 0.000]\n",
      "165 [D loss: (0.181)(R 0.312, F 0.051)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.605] [G acc: 0.000]\n",
      "166 [D loss: (0.153)(R 0.305, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.162] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 [D loss: (0.151)(R 0.297, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.811] [G acc: 0.000]\n",
      "168 [D loss: (0.138)(R 0.277, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.610] [G acc: 0.000]\n",
      "169 [D loss: (0.158)(R 0.316, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.215] [G acc: 0.000]\n",
      "170 [D loss: (0.147)(R 0.285, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.941] [G acc: 0.000]\n",
      "171 [D loss: (0.143)(R 0.286, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.274] [G acc: 0.000]\n",
      "172 [D loss: (0.148)(R 0.296, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.348] [G acc: 0.062]\n",
      "173 [D loss: (0.166)(R 0.305, F 0.026)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.104] [G acc: 0.000]\n",
      "174 [D loss: (0.131)(R 0.259, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.424] [G acc: 0.000]\n",
      "175 [D loss: (0.150)(R 0.299, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.717] [G acc: 0.000]\n",
      "176 [D loss: (0.132)(R 0.255, F 0.009)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.551] [G acc: 0.000]\n",
      "177 [D loss: (0.102)(R 0.202, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.101] [G acc: 0.000]\n",
      "178 [D loss: (0.119)(R 0.237, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.773] [G acc: 0.000]\n",
      "179 [D loss: (0.123)(R 0.215, F 0.030)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.265] [G acc: 0.000]\n",
      "180 [D loss: (0.132)(R 0.235, F 0.030)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.615] [G acc: 0.000]\n",
      "181 [D loss: (0.129)(R 0.252, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.708] [G acc: 0.000]\n",
      "182 [D loss: (0.114)(R 0.218, F 0.009)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.076] [G acc: 0.000]\n",
      "183 [D loss: (0.123)(R 0.246, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.293] [G acc: 0.000]\n",
      "184 [D loss: (0.109)(R 0.201, F 0.017)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.942] [G acc: 0.000]\n",
      "185 [D loss: (0.107)(R 0.212, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.736] [G acc: 0.000]\n",
      "186 [D loss: (0.114)(R 0.206, F 0.021)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.354] [G acc: 0.000]\n",
      "187 [D loss: (0.165)(R 0.184, F 0.147)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.765] [G acc: 0.000]\n",
      "188 [D loss: (0.092)(R 0.178, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.437] [G acc: 0.000]\n",
      "189 [D loss: (0.150)(R 0.232, F 0.067)] [D acc: (0.969)(1.000, 0.938)] [G loss: 15.080] [G acc: 0.000]\n",
      "190 [D loss: (0.150)(R 0.212, F 0.088)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.946] [G acc: 0.000]\n",
      "191 [D loss: (0.082)(R 0.163, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.885] [G acc: 0.000]\n",
      "192 [D loss: (0.141)(R 0.204, F 0.077)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.750] [G acc: 0.000]\n",
      "193 [D loss: (0.098)(R 0.146, F 0.049)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.280] [G acc: 0.000]\n",
      "194 [D loss: (0.104)(R 0.172, F 0.036)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.371] [G acc: 0.000]\n",
      "195 [D loss: (0.103)(R 0.204, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.889] [G acc: 0.000]\n",
      "196 [D loss: (0.080)(R 0.157, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.917] [G acc: 0.062]\n",
      "197 [D loss: (0.084)(R 0.143, F 0.025)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.582] [G acc: 0.000]\n",
      "198 [D loss: (0.073)(R 0.145, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.411] [G acc: 0.062]\n",
      "199 [D loss: (0.095)(R 0.190, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.762] [G acc: 0.000]\n",
      "200 [D loss: (0.075)(R 0.150, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.665] [G acc: 0.062]\n",
      "201 [D loss: (0.085)(R 0.169, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.646] [G acc: 0.000]\n",
      "202 [D loss: (0.089)(R 0.172, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.812] [G acc: 0.000]\n",
      "203 [D loss: (0.080)(R 0.146, F 0.015)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.954] [G acc: 0.000]\n",
      "204 [D loss: (0.122)(R 0.243, F 0.001)] [D acc: (0.969)(0.938, 1.000)] [G loss: 12.955] [G acc: 0.000]\n",
      "205 [D loss: (0.066)(R 0.133, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.335] [G acc: 0.000]\n",
      "206 [D loss: (0.061)(R 0.113, F 0.010)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.435] [G acc: 0.062]\n",
      "207 [D loss: (0.127)(R 0.144, F 0.110)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.699] [G acc: 0.000]\n",
      "208 [D loss: (0.074)(R 0.149, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.986] [G acc: 0.000]\n",
      "209 [D loss: (0.070)(R 0.117, F 0.023)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.931] [G acc: 0.000]\n",
      "210 [D loss: (0.070)(R 0.139, F 0.000)] [D acc: (0.969)(0.938, 1.000)] [G loss: 13.447] [G acc: 0.000]\n",
      "211 [D loss: (0.053)(R 0.106, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.336] [G acc: 0.062]\n",
      "212 [D loss: (0.076)(R 0.153, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.409] [G acc: 0.000]\n",
      "213 [D loss: (0.065)(R 0.128, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.201] [G acc: 0.062]\n",
      "214 [D loss: (0.038)(R 0.077, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.141] [G acc: 0.062]\n",
      "215 [D loss: (0.035)(R 0.070, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.922] [G acc: 0.000]\n",
      "216 [D loss: (0.044)(R 0.086, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.779] [G acc: 0.000]\n",
      "217 [D loss: (0.140)(R 0.103, F 0.176)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.738] [G acc: 0.000]\n",
      "218 [D loss: (0.067)(R 0.134, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.845] [G acc: 0.000]\n",
      "219 [D loss: (0.201)(R 0.094, F 0.307)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.122] [G acc: 0.062]\n",
      "220 [D loss: (0.077)(R 0.078, F 0.076)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.159] [G acc: 0.000]\n",
      "221 [D loss: (0.043)(R 0.085, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.800] [G acc: 0.000]\n",
      "222 [D loss: (0.184)(R 0.132, F 0.236)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.682] [G acc: 0.062]\n",
      "223 [D loss: (0.129)(R 0.118, F 0.140)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.223] [G acc: 0.000]\n",
      "224 [D loss: (0.082)(R 0.133, F 0.032)] [D acc: (0.969)(0.938, 1.000)] [G loss: 13.485] [G acc: 0.000]\n",
      "225 [D loss: (0.047)(R 0.088, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.546] [G acc: 0.000]\n",
      "226 [D loss: (0.128)(R 0.123, F 0.132)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.481] [G acc: 0.000]\n",
      "227 [D loss: (0.048)(R 0.094, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.887] [G acc: 0.000]\n",
      "228 [D loss: (0.166)(R 0.139, F 0.193)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.569] [G acc: 0.000]\n",
      "229 [D loss: (0.060)(R 0.104, F 0.016)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.794] [G acc: 0.000]\n",
      "230 [D loss: (0.060)(R 0.117, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.916] [G acc: 0.062]\n",
      "231 [D loss: (0.111)(R 0.104, F 0.119)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.156] [G acc: 0.000]\n",
      "232 [D loss: (0.159)(R 0.227, F 0.091)] [D acc: (0.938)(0.938, 0.938)] [G loss: 14.479] [G acc: 0.000]\n",
      "233 [D loss: (0.119)(R 0.094, F 0.143)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.219] [G acc: 0.000]\n",
      "234 [D loss: (0.117)(R 0.170, F 0.064)] [D acc: (0.938)(0.938, 0.938)] [G loss: 13.609] [G acc: 0.000]\n",
      "235 [D loss: (0.074)(R 0.119, F 0.030)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.627] [G acc: 0.000]\n",
      "236 [D loss: (0.040)(R 0.080, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 15.269] [G acc: 0.000]\n",
      "237 [D loss: (0.074)(R 0.106, F 0.042)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.528] [G acc: 0.000]\n",
      "238 [D loss: (0.054)(R 0.109, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.704] [G acc: 0.000]\n",
      "239 [D loss: (0.047)(R 0.095, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.881] [G acc: 0.000]\n",
      "240 [D loss: (0.045)(R 0.087, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.129] [G acc: 0.062]\n",
      "241 [D loss: (0.078)(R 0.092, F 0.063)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.435] [G acc: 0.000]\n",
      "242 [D loss: (0.056)(R 0.111, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.631] [G acc: 0.000]\n",
      "243 [D loss: (0.070)(R 0.116, F 0.024)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.997] [G acc: 0.000]\n",
      "244 [D loss: (0.084)(R 0.113, F 0.056)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.347] [G acc: 0.000]\n",
      "245 [D loss: (0.234)(R 0.467, F 0.001)] [D acc: (0.969)(0.938, 1.000)] [G loss: 14.883] [G acc: 0.000]\n",
      "246 [D loss: (0.100)(R 0.070, F 0.131)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.260] [G acc: 0.000]\n",
      "247 [D loss: (0.041)(R 0.082, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.526] [G acc: 0.000]\n",
      "248 [D loss: (0.060)(R 0.092, F 0.028)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.748] [G acc: 0.000]\n",
      "249 [D loss: (0.062)(R 0.124, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.292] [G acc: 0.062]\n",
      "250 [D loss: (0.031)(R 0.062, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.542] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 [D loss: (0.134)(R 0.183, F 0.085)] [D acc: (0.938)(0.938, 0.938)] [G loss: 14.696] [G acc: 0.000]\n",
      "252 [D loss: (0.052)(R 0.104, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.887] [G acc: 0.000]\n",
      "253 [D loss: (0.031)(R 0.058, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.784] [G acc: 0.000]\n",
      "254 [D loss: (0.042)(R 0.084, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.143] [G acc: 0.000]\n",
      "255 [D loss: (0.104)(R 0.201, F 0.007)] [D acc: (0.969)(0.938, 1.000)] [G loss: 13.171] [G acc: 0.000]\n",
      "256 [D loss: (0.037)(R 0.074, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.327] [G acc: 0.000]\n",
      "257 [D loss: (0.197)(R 0.080, F 0.314)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.864] [G acc: 0.000]\n",
      "258 [D loss: (0.059)(R 0.078, F 0.039)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.831] [G acc: 0.000]\n",
      "259 [D loss: (0.040)(R 0.079, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.514] [G acc: 0.000]\n",
      "260 [D loss: (0.086)(R 0.138, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.120] [G acc: 0.000]\n",
      "261 [D loss: (0.054)(R 0.107, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.363] [G acc: 0.062]\n",
      "262 [D loss: (0.163)(R 0.085, F 0.241)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.950] [G acc: 0.000]\n",
      "263 [D loss: (0.163)(R 0.047, F 0.280)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.427] [G acc: 0.062]\n",
      "264 [D loss: (0.137)(R 0.219, F 0.054)] [D acc: (0.969)(0.938, 1.000)] [G loss: 14.348] [G acc: 0.062]\n",
      "265 [D loss: (0.043)(R 0.081, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.930] [G acc: 0.000]\n",
      "266 [D loss: (0.079)(R 0.158, F 0.000)] [D acc: (0.969)(0.938, 1.000)] [G loss: 11.280] [G acc: 0.125]\n",
      "267 [D loss: (0.038)(R 0.073, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.973] [G acc: 0.062]\n",
      "268 [D loss: (0.041)(R 0.075, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.100] [G acc: 0.000]\n",
      "269 [D loss: (0.034)(R 0.068, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.871] [G acc: 0.000]\n",
      "270 [D loss: (0.123)(R 0.241, F 0.006)] [D acc: (0.969)(0.938, 1.000)] [G loss: 14.321] [G acc: 0.000]\n",
      "271 [D loss: (0.046)(R 0.083, F 0.010)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.989] [G acc: 0.000]\n",
      "272 [D loss: (0.050)(R 0.047, F 0.053)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.436] [G acc: 0.062]\n",
      "273 [D loss: (0.039)(R 0.078, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.034] [G acc: 0.000]\n",
      "274 [D loss: (0.037)(R 0.064, F 0.010)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.971] [G acc: 0.000]\n",
      "275 [D loss: (0.021)(R 0.041, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.430] [G acc: 0.000]\n",
      "276 [D loss: (0.143)(R 0.223, F 0.063)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.567] [G acc: 0.062]\n",
      "277 [D loss: (0.100)(R 0.092, F 0.109)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.140] [G acc: 0.062]\n",
      "278 [D loss: (0.076)(R 0.053, F 0.100)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.568] [G acc: 0.062]\n",
      "279 [D loss: (0.081)(R 0.053, F 0.109)] [D acc: (0.969)(1.000, 0.938)] [G loss: 15.332] [G acc: 0.000]\n",
      "280 [D loss: (0.045)(R 0.083, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.482] [G acc: 0.000]\n",
      "281 [D loss: (0.037)(R 0.073, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.347] [G acc: 0.000]\n",
      "282 [D loss: (0.205)(R 0.066, F 0.345)] [D acc: (0.938)(1.000, 0.875)] [G loss: 12.408] [G acc: 0.000]\n",
      "283 [D loss: (0.028)(R 0.056, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.739] [G acc: 0.000]\n",
      "284 [D loss: (0.032)(R 0.063, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.713] [G acc: 0.000]\n",
      "285 [D loss: (0.241)(R 0.074, F 0.408)] [D acc: (0.938)(1.000, 0.875)] [G loss: 14.719] [G acc: 0.000]\n",
      "286 [D loss: (0.026)(R 0.046, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 15.082] [G acc: 0.000]\n",
      "287 [D loss: (0.035)(R 0.069, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.093] [G acc: 0.000]\n",
      "288 [D loss: (0.063)(R 0.039, F 0.086)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.797] [G acc: 0.062]\n",
      "289 [D loss: (0.039)(R 0.078, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.144] [G acc: 0.188]\n",
      "290 [D loss: (0.211)(R 0.129, F 0.294)] [D acc: (0.938)(1.000, 0.875)] [G loss: 12.069] [G acc: 0.000]\n",
      "291 [D loss: (0.052)(R 0.062, F 0.042)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.875] [G acc: 0.000]\n",
      "292 [D loss: (0.053)(R 0.044, F 0.062)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.448] [G acc: 0.125]\n",
      "293 [D loss: (0.115)(R 0.066, F 0.164)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.621] [G acc: 0.000]\n",
      "294 [D loss: (0.097)(R 0.069, F 0.126)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.611] [G acc: 0.125]\n",
      "295 [D loss: (0.034)(R 0.068, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.852] [G acc: 0.062]\n",
      "296 [D loss: (0.035)(R 0.068, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.356] [G acc: 0.062]\n",
      "297 [D loss: (0.111)(R 0.039, F 0.183)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.555] [G acc: 0.062]\n",
      "298 [D loss: (0.166)(R 0.043, F 0.288)] [D acc: (0.938)(1.000, 0.875)] [G loss: 14.443] [G acc: 0.000]\n",
      "299 [D loss: (0.290)(R 0.071, F 0.508)] [D acc: (0.938)(1.000, 0.875)] [G loss: 14.772] [G acc: 0.000]\n",
      "300 [D loss: (0.066)(R 0.129, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.951] [G acc: 0.000]\n",
      "301 [D loss: (0.186)(R 0.305, F 0.068)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.939] [G acc: 0.062]\n",
      "302 [D loss: (0.062)(R 0.110, F 0.013)] [D acc: (0.969)(0.938, 1.000)] [G loss: 13.561] [G acc: 0.062]\n",
      "303 [D loss: (0.108)(R 0.078, F 0.137)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.429] [G acc: 0.062]\n",
      "304 [D loss: (0.069)(R 0.076, F 0.061)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.925] [G acc: 0.062]\n",
      "305 [D loss: (0.036)(R 0.065, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.986] [G acc: 0.062]\n",
      "306 [D loss: (0.052)(R 0.065, F 0.040)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.520] [G acc: 0.000]\n",
      "307 [D loss: (0.026)(R 0.052, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.275] [G acc: 0.000]\n",
      "308 [D loss: (0.160)(R 0.053, F 0.267)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.431] [G acc: 0.062]\n",
      "309 [D loss: (0.035)(R 0.068, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.541] [G acc: 0.188]\n",
      "310 [D loss: (0.139)(R 0.254, F 0.023)] [D acc: (0.969)(0.938, 1.000)] [G loss: 13.267] [G acc: 0.000]\n",
      "311 [D loss: (0.068)(R 0.127, F 0.008)] [D acc: (0.969)(0.938, 1.000)] [G loss: 13.384] [G acc: 0.000]\n",
      "312 [D loss: (0.408)(R 0.203, F 0.612)] [D acc: (0.906)(0.938, 0.875)] [G loss: 11.668] [G acc: 0.125]\n",
      "313 [D loss: (0.346)(R 0.075, F 0.617)] [D acc: (0.906)(1.000, 0.812)] [G loss: 12.767] [G acc: 0.125]\n",
      "314 [D loss: (0.137)(R 0.093, F 0.181)] [D acc: (0.969)(1.000, 0.938)] [G loss: 11.942] [G acc: 0.062]\n",
      "315 [D loss: (0.019)(R 0.038, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.036] [G acc: 0.000]\n",
      "316 [D loss: (0.120)(R 0.031, F 0.209)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.856] [G acc: 0.125]\n",
      "317 [D loss: (0.167)(R 0.332, F 0.001)] [D acc: (0.906)(0.812, 1.000)] [G loss: 11.243] [G acc: 0.062]\n",
      "318 [D loss: (0.110)(R 0.047, F 0.174)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.819] [G acc: 0.000]\n",
      "319 [D loss: (0.176)(R 0.149, F 0.203)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.663] [G acc: 0.062]\n",
      "320 [D loss: (0.028)(R 0.054, F 0.002)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.640] [G acc: 0.062]\n",
      "321 [D loss: (0.107)(R 0.040, F 0.173)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.350] [G acc: 0.000]\n",
      "322 [D loss: (0.058)(R 0.061, F 0.055)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.174] [G acc: 0.125]\n",
      "323 [D loss: (0.129)(R 0.052, F 0.205)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.894] [G acc: 0.062]\n",
      "324 [D loss: (0.196)(R 0.123, F 0.269)] [D acc: (0.938)(0.938, 0.938)] [G loss: 11.448] [G acc: 0.000]\n",
      "325 [D loss: (0.143)(R 0.103, F 0.183)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.644] [G acc: 0.062]\n",
      "326 [D loss: (0.036)(R 0.069, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.191] [G acc: 0.188]\n",
      "327 [D loss: (0.067)(R 0.103, F 0.031)] [D acc: (0.969)(0.938, 1.000)] [G loss: 12.466] [G acc: 0.000]\n",
      "328 [D loss: (0.188)(R 0.152, F 0.225)] [D acc: (0.938)(0.938, 0.938)] [G loss: 13.663] [G acc: 0.062]\n",
      "329 [D loss: (0.039)(R 0.067, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.921] [G acc: 0.062]\n",
      "330 [D loss: (0.161)(R 0.053, F 0.269)] [D acc: (0.938)(1.000, 0.875)] [G loss: 12.703] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331 [D loss: (0.174)(R 0.052, F 0.297)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.420] [G acc: 0.062]\n",
      "332 [D loss: (0.117)(R 0.233, F 0.000)] [D acc: (0.969)(0.938, 1.000)] [G loss: 11.844] [G acc: 0.062]\n",
      "333 [D loss: (0.112)(R 0.056, F 0.168)] [D acc: (0.938)(1.000, 0.875)] [G loss: 14.432] [G acc: 0.000]\n",
      "334 [D loss: (0.174)(R 0.092, F 0.255)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.628] [G acc: 0.000]\n",
      "335 [D loss: (0.157)(R 0.041, F 0.274)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.162] [G acc: 0.000]\n",
      "336 [D loss: (0.039)(R 0.066, F 0.011)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.636] [G acc: 0.000]\n",
      "337 [D loss: (0.088)(R 0.065, F 0.111)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.605] [G acc: 0.062]\n",
      "338 [D loss: (0.084)(R 0.044, F 0.125)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.166] [G acc: 0.000]\n",
      "339 [D loss: (0.133)(R 0.062, F 0.204)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.628] [G acc: 0.000]\n",
      "340 [D loss: (0.343)(R 0.141, F 0.545)] [D acc: (0.844)(0.938, 0.750)] [G loss: 14.000] [G acc: 0.000]\n",
      "341 [D loss: (0.243)(R 0.087, F 0.398)] [D acc: (0.938)(1.000, 0.875)] [G loss: 14.555] [G acc: 0.000]\n",
      "342 [D loss: (0.031)(R 0.062, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 13.830] [G acc: 0.000]\n",
      "343 [D loss: (0.045)(R 0.090, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 14.466] [G acc: 0.062]\n",
      "344 [D loss: (0.192)(R 0.185, F 0.199)] [D acc: (0.938)(0.938, 0.938)] [G loss: 13.470] [G acc: 0.062]\n",
      "345 [D loss: (0.173)(R 0.080, F 0.266)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.777] [G acc: 0.062]\n",
      "346 [D loss: (0.087)(R 0.171, F 0.003)] [D acc: (0.969)(0.938, 1.000)] [G loss: 11.870] [G acc: 0.125]\n",
      "347 [D loss: (0.020)(R 0.040, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.316] [G acc: 0.062]\n",
      "348 [D loss: (0.097)(R 0.045, F 0.148)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.570] [G acc: 0.062]\n",
      "349 [D loss: (0.116)(R 0.060, F 0.172)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.962] [G acc: 0.000]\n",
      "350 [D loss: (0.194)(R 0.066, F 0.321)] [D acc: (0.938)(1.000, 0.875)] [G loss: 10.273] [G acc: 0.125]\n",
      "351 [D loss: (0.202)(R 0.262, F 0.141)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.586] [G acc: 0.062]\n",
      "352 [D loss: (0.167)(R 0.063, F 0.270)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.645] [G acc: 0.125]\n",
      "353 [D loss: (0.082)(R 0.029, F 0.134)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.162] [G acc: 0.062]\n",
      "354 [D loss: (0.532)(R 0.052, F 1.011)] [D acc: (0.875)(1.000, 0.750)] [G loss: 12.060] [G acc: 0.062]\n",
      "355 [D loss: (0.130)(R 0.087, F 0.174)] [D acc: (0.969)(1.000, 0.938)] [G loss: 11.309] [G acc: 0.125]\n",
      "356 [D loss: (0.168)(R 0.087, F 0.250)] [D acc: (0.938)(1.000, 0.875)] [G loss: 12.404] [G acc: 0.062]\n",
      "357 [D loss: (0.146)(R 0.111, F 0.182)] [D acc: (0.938)(0.938, 0.938)] [G loss: 14.079] [G acc: 0.000]\n",
      "358 [D loss: (0.137)(R 0.048, F 0.227)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.157] [G acc: 0.125]\n",
      "359 [D loss: (0.330)(R 0.101, F 0.559)] [D acc: (0.938)(1.000, 0.875)] [G loss: 12.915] [G acc: 0.062]\n",
      "360 [D loss: (0.114)(R 0.099, F 0.130)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.895] [G acc: 0.125]\n",
      "361 [D loss: (0.178)(R 0.310, F 0.047)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.064] [G acc: 0.062]\n",
      "362 [D loss: (0.093)(R 0.052, F 0.134)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.564] [G acc: 0.062]\n",
      "363 [D loss: (0.204)(R 0.358, F 0.050)] [D acc: (0.906)(0.875, 0.938)] [G loss: 11.300] [G acc: 0.125]\n",
      "364 [D loss: (0.239)(R 0.069, F 0.409)] [D acc: (0.938)(1.000, 0.875)] [G loss: 11.293] [G acc: 0.125]\n",
      "365 [D loss: (0.049)(R 0.097, F 0.000)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.084] [G acc: 0.000]\n",
      "366 [D loss: (0.213)(R 0.217, F 0.209)] [D acc: (0.938)(0.938, 0.938)] [G loss: 14.109] [G acc: 0.062]\n",
      "367 [D loss: (0.139)(R 0.058, F 0.219)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.212] [G acc: 0.062]\n",
      "368 [D loss: (0.151)(R 0.160, F 0.141)] [D acc: (0.906)(0.938, 0.875)] [G loss: 12.402] [G acc: 0.125]\n",
      "369 [D loss: (0.097)(R 0.042, F 0.152)] [D acc: (0.969)(1.000, 0.938)] [G loss: 11.836] [G acc: 0.125]\n",
      "370 [D loss: (0.120)(R 0.059, F 0.180)] [D acc: (0.969)(1.000, 0.938)] [G loss: 13.132] [G acc: 0.000]\n",
      "371 [D loss: (0.447)(R 0.063, F 0.831)] [D acc: (0.875)(1.000, 0.750)] [G loss: 12.868] [G acc: 0.062]\n",
      "372 [D loss: (0.129)(R 0.254, F 0.004)] [D acc: (0.969)(0.938, 1.000)] [G loss: 12.252] [G acc: 0.062]\n",
      "373 [D loss: (0.070)(R 0.114, F 0.027)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.856] [G acc: 0.000]\n",
      "374 [D loss: (0.371)(R 0.088, F 0.653)] [D acc: (0.906)(1.000, 0.812)] [G loss: 11.470] [G acc: 0.125]\n",
      "375 [D loss: (0.524)(R 0.250, F 0.799)] [D acc: (0.844)(0.938, 0.750)] [G loss: 10.081] [G acc: 0.188]\n",
      "376 [D loss: (0.149)(R 0.098, F 0.201)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.181] [G acc: 0.125]\n",
      "377 [D loss: (0.178)(R 0.070, F 0.286)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.481] [G acc: 0.188]\n",
      "378 [D loss: (0.426)(R 0.098, F 0.755)] [D acc: (0.844)(1.000, 0.688)] [G loss: 13.041] [G acc: 0.000]\n",
      "379 [D loss: (0.538)(R 0.370, F 0.705)] [D acc: (0.781)(0.875, 0.688)] [G loss: 11.660] [G acc: 0.125]\n",
      "380 [D loss: (0.313)(R 0.374, F 0.251)] [D acc: (0.906)(0.875, 0.938)] [G loss: 10.441] [G acc: 0.188]\n",
      "381 [D loss: (0.277)(R 0.194, F 0.359)] [D acc: (0.875)(0.938, 0.812)] [G loss: 12.594] [G acc: 0.125]\n",
      "382 [D loss: (0.168)(R 0.109, F 0.227)] [D acc: (0.938)(0.938, 0.938)] [G loss: 11.097] [G acc: 0.188]\n",
      "383 [D loss: (0.066)(R 0.101, F 0.031)] [D acc: (1.000)(1.000, 1.000)] [G loss: 12.256] [G acc: 0.125]\n",
      "384 [D loss: (0.300)(R 0.258, F 0.341)] [D acc: (0.875)(0.938, 0.812)] [G loss: 12.259] [G acc: 0.125]\n",
      "385 [D loss: (0.267)(R 0.083, F 0.451)] [D acc: (0.875)(1.000, 0.750)] [G loss: 12.485] [G acc: 0.062]\n",
      "386 [D loss: (0.152)(R 0.109, F 0.195)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.485] [G acc: 0.062]\n",
      "387 [D loss: (0.201)(R 0.112, F 0.289)] [D acc: (0.875)(0.938, 0.812)] [G loss: 12.030] [G acc: 0.062]\n",
      "388 [D loss: (0.298)(R 0.135, F 0.461)] [D acc: (0.875)(0.938, 0.812)] [G loss: 11.136] [G acc: 0.188]\n",
      "389 [D loss: (0.257)(R 0.231, F 0.283)] [D acc: (0.906)(0.938, 0.875)] [G loss: 11.704] [G acc: 0.125]\n",
      "390 [D loss: (0.299)(R 0.130, F 0.467)] [D acc: (0.844)(0.938, 0.750)] [G loss: 11.053] [G acc: 0.062]\n",
      "391 [D loss: (0.247)(R 0.102, F 0.392)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.351] [G acc: 0.000]\n",
      "392 [D loss: (0.229)(R 0.101, F 0.356)] [D acc: (0.906)(1.000, 0.812)] [G loss: 12.164] [G acc: 0.062]\n",
      "393 [D loss: (0.104)(R 0.113, F 0.096)] [D acc: (0.969)(1.000, 0.938)] [G loss: 11.483] [G acc: 0.188]\n",
      "394 [D loss: (0.429)(R 0.138, F 0.720)] [D acc: (0.781)(0.938, 0.625)] [G loss: 11.968] [G acc: 0.125]\n",
      "395 [D loss: (0.173)(R 0.118, F 0.229)] [D acc: (0.938)(1.000, 0.875)] [G loss: 13.392] [G acc: 0.000]\n",
      "396 [D loss: (0.785)(R 1.174, F 0.396)] [D acc: (0.875)(0.938, 0.812)] [G loss: 12.311] [G acc: 0.125]\n",
      "397 [D loss: (0.159)(R 0.111, F 0.206)] [D acc: (0.969)(1.000, 0.938)] [G loss: 14.458] [G acc: 0.062]\n",
      "398 [D loss: (0.217)(R 0.110, F 0.324)] [D acc: (0.906)(1.000, 0.812)] [G loss: 11.425] [G acc: 0.188]\n",
      "399 [D loss: (0.372)(R 0.444, F 0.300)] [D acc: (0.875)(0.875, 0.875)] [G loss: 10.993] [G acc: 0.188]\n",
      "400 [D loss: (0.302)(R 0.232, F 0.372)] [D acc: (0.844)(0.875, 0.812)] [G loss: 10.089] [G acc: 0.250]\n",
      "401 [D loss: (0.306)(R 0.154, F 0.459)] [D acc: (0.906)(1.000, 0.812)] [G loss: 11.122] [G acc: 0.125]\n",
      "402 [D loss: (0.350)(R 0.123, F 0.578)] [D acc: (0.875)(1.000, 0.750)] [G loss: 12.151] [G acc: 0.188]\n",
      "403 [D loss: (0.242)(R 0.242, F 0.242)] [D acc: (0.906)(0.938, 0.875)] [G loss: 10.400] [G acc: 0.188]\n",
      "404 [D loss: (0.219)(R 0.153, F 0.284)] [D acc: (0.906)(0.938, 0.875)] [G loss: 12.665] [G acc: 0.125]\n",
      "405 [D loss: (0.369)(R 0.093, F 0.646)] [D acc: (0.812)(1.000, 0.625)] [G loss: 10.452] [G acc: 0.188]\n",
      "406 [D loss: (0.614)(R 0.293, F 0.935)] [D acc: (0.781)(0.938, 0.625)] [G loss: 12.741] [G acc: 0.062]\n",
      "407 [D loss: (0.259)(R 0.336, F 0.182)] [D acc: (0.938)(0.938, 0.938)] [G loss: 10.755] [G acc: 0.188]\n",
      "408 [D loss: (0.773)(R 0.591, F 0.956)] [D acc: (0.719)(0.812, 0.625)] [G loss: 13.010] [G acc: 0.062]\n",
      "409 [D loss: (0.526)(R 0.104, F 0.947)] [D acc: (0.812)(1.000, 0.625)] [G loss: 12.294] [G acc: 0.125]\n",
      "410 [D loss: (0.182)(R 0.143, F 0.220)] [D acc: (0.938)(1.000, 0.875)] [G loss: 11.498] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411 [D loss: (0.347)(R 0.109, F 0.585)] [D acc: (0.844)(1.000, 0.688)] [G loss: 11.844] [G acc: 0.062]\n",
      "412 [D loss: (0.545)(R 0.391, F 0.699)] [D acc: (0.781)(0.875, 0.688)] [G loss: 10.591] [G acc: 0.188]\n",
      "413 [D loss: (0.523)(R 0.573, F 0.473)] [D acc: (0.812)(0.875, 0.750)] [G loss: 12.230] [G acc: 0.062]\n",
      "414 [D loss: (0.197)(R 0.175, F 0.219)] [D acc: (0.938)(1.000, 0.875)] [G loss: 11.710] [G acc: 0.188]\n",
      "415 [D loss: (0.409)(R 0.224, F 0.594)] [D acc: (0.844)(0.938, 0.750)] [G loss: 10.216] [G acc: 0.188]\n",
      "416 [D loss: (0.374)(R 0.155, F 0.592)] [D acc: (0.844)(1.000, 0.688)] [G loss: 11.776] [G acc: 0.125]\n",
      "417 [D loss: (0.373)(R 0.386, F 0.359)] [D acc: (0.875)(0.938, 0.812)] [G loss: 10.928] [G acc: 0.062]\n",
      "418 [D loss: (0.153)(R 0.120, F 0.186)] [D acc: (0.938)(1.000, 0.875)] [G loss: 11.937] [G acc: 0.125]\n",
      "419 [D loss: (0.390)(R 0.199, F 0.582)] [D acc: (0.844)(0.938, 0.750)] [G loss: 10.758] [G acc: 0.250]\n",
      "420 [D loss: (0.287)(R 0.142, F 0.432)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.873] [G acc: 0.250]\n",
      "421 [D loss: (0.344)(R 0.113, F 0.575)] [D acc: (0.875)(1.000, 0.750)] [G loss: 10.773] [G acc: 0.188]\n",
      "422 [D loss: (0.138)(R 0.131, F 0.145)] [D acc: (0.969)(1.000, 0.938)] [G loss: 12.812] [G acc: 0.125]\n",
      "423 [D loss: (0.201)(R 0.156, F 0.246)] [D acc: (0.938)(1.000, 0.875)] [G loss: 11.016] [G acc: 0.125]\n",
      "424 [D loss: (0.150)(R 0.185, F 0.116)] [D acc: (0.938)(0.938, 0.938)] [G loss: 10.545] [G acc: 0.188]\n",
      "425 [D loss: (0.304)(R 0.115, F 0.492)] [D acc: (0.844)(1.000, 0.688)] [G loss: 10.189] [G acc: 0.125]\n",
      "426 [D loss: (0.162)(R 0.287, F 0.037)] [D acc: (0.938)(0.875, 1.000)] [G loss: 13.122] [G acc: 0.125]\n",
      "427 [D loss: (0.281)(R 0.150, F 0.412)] [D acc: (0.875)(0.938, 0.812)] [G loss: 11.446] [G acc: 0.188]\n",
      "428 [D loss: (0.471)(R 0.140, F 0.801)] [D acc: (0.781)(1.000, 0.562)] [G loss: 12.427] [G acc: 0.125]\n",
      "429 [D loss: (0.469)(R 0.176, F 0.761)] [D acc: (0.844)(1.000, 0.688)] [G loss: 9.067] [G acc: 0.250]\n",
      "430 [D loss: (0.259)(R 0.156, F 0.362)] [D acc: (0.906)(1.000, 0.812)] [G loss: 10.255] [G acc: 0.188]\n",
      "431 [D loss: (0.210)(R 0.129, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.353] [G acc: 0.188]\n",
      "432 [D loss: (0.341)(R 0.218, F 0.465)] [D acc: (0.875)(0.938, 0.812)] [G loss: 10.451] [G acc: 0.250]\n",
      "433 [D loss: (0.247)(R 0.363, F 0.132)] [D acc: (0.938)(0.938, 0.938)] [G loss: 12.175] [G acc: 0.062]\n",
      "434 [D loss: (0.233)(R 0.176, F 0.290)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.943] [G acc: 0.250]\n",
      "435 [D loss: (0.365)(R 0.318, F 0.411)] [D acc: (0.875)(0.938, 0.812)] [G loss: 10.307] [G acc: 0.062]\n",
      "436 [D loss: (0.414)(R 0.181, F 0.646)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.953] [G acc: 0.312]\n",
      "437 [D loss: (0.305)(R 0.113, F 0.497)] [D acc: (0.906)(1.000, 0.812)] [G loss: 11.106] [G acc: 0.125]\n",
      "438 [D loss: (0.456)(R 0.155, F 0.757)] [D acc: (0.812)(1.000, 0.625)] [G loss: 10.558] [G acc: 0.125]\n",
      "439 [D loss: (0.380)(R 0.280, F 0.479)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.618] [G acc: 0.188]\n",
      "440 [D loss: (0.486)(R 0.236, F 0.736)] [D acc: (0.812)(0.875, 0.750)] [G loss: 11.368] [G acc: 0.188]\n",
      "441 [D loss: (0.553)(R 0.186, F 0.919)] [D acc: (0.750)(1.000, 0.500)] [G loss: 12.043] [G acc: 0.062]\n",
      "442 [D loss: (0.656)(R 0.294, F 1.018)] [D acc: (0.719)(0.938, 0.500)] [G loss: 9.450] [G acc: 0.188]\n",
      "443 [D loss: (0.200)(R 0.208, F 0.192)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.408] [G acc: 0.125]\n",
      "444 [D loss: (0.275)(R 0.196, F 0.354)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.236] [G acc: 0.312]\n",
      "445 [D loss: (0.434)(R 0.211, F 0.657)] [D acc: (0.844)(1.000, 0.688)] [G loss: 10.461] [G acc: 0.188]\n",
      "446 [D loss: (0.321)(R 0.172, F 0.469)] [D acc: (0.844)(1.000, 0.688)] [G loss: 11.643] [G acc: 0.125]\n",
      "447 [D loss: (0.384)(R 0.339, F 0.428)] [D acc: (0.812)(0.812, 0.812)] [G loss: 10.335] [G acc: 0.312]\n",
      "448 [D loss: (0.277)(R 0.159, F 0.396)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.397] [G acc: 0.188]\n",
      "449 [D loss: (0.276)(R 0.138, F 0.415)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.981] [G acc: 0.375]\n",
      "450 [D loss: (0.285)(R 0.149, F 0.422)] [D acc: (0.875)(1.000, 0.750)] [G loss: 11.284] [G acc: 0.188]\n",
      "451 [D loss: (0.720)(R 0.426, F 1.014)] [D acc: (0.688)(0.812, 0.562)] [G loss: 11.000] [G acc: 0.125]\n",
      "452 [D loss: (0.662)(R 0.672, F 0.651)] [D acc: (0.688)(0.750, 0.625)] [G loss: 9.638] [G acc: 0.250]\n",
      "453 [D loss: (0.242)(R 0.249, F 0.234)] [D acc: (0.938)(1.000, 0.875)] [G loss: 10.278] [G acc: 0.250]\n",
      "454 [D loss: (0.331)(R 0.251, F 0.412)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.890] [G acc: 0.188]\n",
      "455 [D loss: (0.501)(R 0.186, F 0.815)] [D acc: (0.812)(1.000, 0.625)] [G loss: 11.086] [G acc: 0.250]\n",
      "456 [D loss: (0.332)(R 0.186, F 0.477)] [D acc: (0.875)(1.000, 0.750)] [G loss: 11.969] [G acc: 0.062]\n",
      "457 [D loss: (0.689)(R 0.541, F 0.838)] [D acc: (0.750)(0.875, 0.625)] [G loss: 8.364] [G acc: 0.312]\n",
      "458 [D loss: (0.248)(R 0.438, F 0.058)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.103] [G acc: 0.188]\n",
      "459 [D loss: (0.239)(R 0.278, F 0.200)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.872] [G acc: 0.250]\n",
      "460 [D loss: (0.301)(R 0.264, F 0.339)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.904] [G acc: 0.250]\n",
      "461 [D loss: (0.334)(R 0.160, F 0.508)] [D acc: (0.875)(1.000, 0.750)] [G loss: 10.951] [G acc: 0.250]\n",
      "462 [D loss: (0.368)(R 0.143, F 0.592)] [D acc: (0.812)(1.000, 0.625)] [G loss: 10.163] [G acc: 0.125]\n",
      "463 [D loss: (0.413)(R 0.182, F 0.644)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.529] [G acc: 0.375]\n",
      "464 [D loss: (0.189)(R 0.191, F 0.187)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.445] [G acc: 0.188]\n",
      "465 [D loss: (0.393)(R 0.271, F 0.515)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.477] [G acc: 0.250]\n",
      "466 [D loss: (0.306)(R 0.145, F 0.467)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.860] [G acc: 0.188]\n",
      "467 [D loss: (0.242)(R 0.166, F 0.317)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.792] [G acc: 0.188]\n",
      "468 [D loss: (0.330)(R 0.209, F 0.451)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.415] [G acc: 0.312]\n",
      "469 [D loss: (0.508)(R 0.180, F 0.836)] [D acc: (0.812)(1.000, 0.625)] [G loss: 8.049] [G acc: 0.375]\n",
      "470 [D loss: (0.303)(R 0.226, F 0.380)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.825] [G acc: 0.312]\n",
      "471 [D loss: (0.486)(R 0.373, F 0.599)] [D acc: (0.781)(0.875, 0.688)] [G loss: 8.902] [G acc: 0.375]\n",
      "472 [D loss: (0.279)(R 0.225, F 0.334)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.874] [G acc: 0.312]\n",
      "473 [D loss: (0.390)(R 0.211, F 0.569)] [D acc: (0.844)(1.000, 0.688)] [G loss: 9.612] [G acc: 0.188]\n",
      "474 [D loss: (0.588)(R 0.309, F 0.868)] [D acc: (0.781)(0.938, 0.625)] [G loss: 9.346] [G acc: 0.312]\n",
      "475 [D loss: (0.439)(R 0.218, F 0.661)] [D acc: (0.781)(0.875, 0.688)] [G loss: 8.856] [G acc: 0.188]\n",
      "476 [D loss: (0.646)(R 0.268, F 1.025)] [D acc: (0.719)(0.938, 0.500)] [G loss: 9.315] [G acc: 0.312]\n",
      "477 [D loss: (0.434)(R 0.180, F 0.687)] [D acc: (0.781)(1.000, 0.562)] [G loss: 11.041] [G acc: 0.188]\n",
      "478 [D loss: (0.645)(R 0.197, F 1.094)] [D acc: (0.688)(0.938, 0.438)] [G loss: 10.394] [G acc: 0.250]\n",
      "479 [D loss: (0.218)(R 0.186, F 0.250)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.412] [G acc: 0.188]\n",
      "480 [D loss: (0.380)(R 0.219, F 0.540)] [D acc: (0.812)(1.000, 0.625)] [G loss: 8.672] [G acc: 0.312]\n",
      "481 [D loss: (0.322)(R 0.495, F 0.148)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.316] [G acc: 0.250]\n",
      "482 [D loss: (0.383)(R 0.224, F 0.541)] [D acc: (0.844)(1.000, 0.688)] [G loss: 10.096] [G acc: 0.250]\n",
      "483 [D loss: (0.187)(R 0.176, F 0.198)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.154] [G acc: 0.312]\n",
      "484 [D loss: (0.389)(R 0.208, F 0.570)] [D acc: (0.875)(1.000, 0.750)] [G loss: 9.573] [G acc: 0.312]\n",
      "485 [D loss: (0.395)(R 0.174, F 0.617)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.788] [G acc: 0.312]\n",
      "486 [D loss: (0.297)(R 0.181, F 0.412)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.377] [G acc: 0.250]\n",
      "487 [D loss: (0.324)(R 0.176, F 0.471)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.486] [G acc: 0.312]\n",
      "488 [D loss: (0.373)(R 0.232, F 0.513)] [D acc: (0.844)(1.000, 0.688)] [G loss: 11.802] [G acc: 0.188]\n",
      "489 [D loss: (0.233)(R 0.216, F 0.251)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.821] [G acc: 0.312]\n",
      "490 [D loss: (0.690)(R 0.331, F 1.049)] [D acc: (0.719)(0.938, 0.500)] [G loss: 8.398] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491 [D loss: (0.528)(R 0.472, F 0.585)] [D acc: (0.688)(0.750, 0.625)] [G loss: 8.718] [G acc: 0.250]\n",
      "492 [D loss: (0.484)(R 0.197, F 0.771)] [D acc: (0.812)(1.000, 0.625)] [G loss: 9.754] [G acc: 0.250]\n",
      "493 [D loss: (0.603)(R 0.161, F 1.045)] [D acc: (0.750)(1.000, 0.500)] [G loss: 10.997] [G acc: 0.250]\n",
      "494 [D loss: (0.247)(R 0.274, F 0.221)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.702] [G acc: 0.125]\n",
      "495 [D loss: (0.249)(R 0.214, F 0.283)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.470] [G acc: 0.312]\n",
      "496 [D loss: (0.388)(R 0.191, F 0.584)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.093] [G acc: 0.312]\n",
      "497 [D loss: (0.341)(R 0.256, F 0.426)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.678] [G acc: 0.188]\n",
      "498 [D loss: (0.319)(R 0.261, F 0.377)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.715] [G acc: 0.375]\n",
      "499 [D loss: (0.397)(R 0.196, F 0.598)] [D acc: (0.812)(1.000, 0.625)] [G loss: 11.032] [G acc: 0.188]\n",
      "500 [D loss: (0.377)(R 0.266, F 0.489)] [D acc: (0.812)(0.938, 0.688)] [G loss: 9.846] [G acc: 0.250]\n",
      "501 [D loss: (0.277)(R 0.202, F 0.352)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.934] [G acc: 0.375]\n",
      "502 [D loss: (0.479)(R 0.191, F 0.767)] [D acc: (0.781)(1.000, 0.562)] [G loss: 8.155] [G acc: 0.312]\n",
      "503 [D loss: (0.429)(R 0.220, F 0.638)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.822] [G acc: 0.375]\n",
      "504 [D loss: (0.399)(R 0.316, F 0.483)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.217] [G acc: 0.312]\n",
      "505 [D loss: (0.519)(R 0.269, F 0.768)] [D acc: (0.750)(0.938, 0.562)] [G loss: 8.069] [G acc: 0.312]\n",
      "506 [D loss: (0.438)(R 0.304, F 0.572)] [D acc: (0.781)(0.875, 0.688)] [G loss: 7.171] [G acc: 0.500]\n",
      "507 [D loss: (0.466)(R 0.200, F 0.733)] [D acc: (0.812)(1.000, 0.625)] [G loss: 7.923] [G acc: 0.375]\n",
      "508 [D loss: (0.356)(R 0.170, F 0.543)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.885] [G acc: 0.438]\n",
      "509 [D loss: (0.550)(R 0.198, F 0.903)] [D acc: (0.750)(1.000, 0.500)] [G loss: 8.934] [G acc: 0.312]\n",
      "510 [D loss: (0.587)(R 0.305, F 0.869)] [D acc: (0.719)(0.938, 0.500)] [G loss: 8.791] [G acc: 0.438]\n",
      "511 [D loss: (0.389)(R 0.246, F 0.533)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.093] [G acc: 0.375]\n",
      "512 [D loss: (0.578)(R 0.219, F 0.937)] [D acc: (0.750)(1.000, 0.500)] [G loss: 8.604] [G acc: 0.188]\n",
      "513 [D loss: (0.751)(R 0.886, F 0.615)] [D acc: (0.781)(0.938, 0.625)] [G loss: 9.175] [G acc: 0.312]\n",
      "514 [D loss: (0.388)(R 0.328, F 0.448)] [D acc: (0.844)(0.938, 0.750)] [G loss: 9.723] [G acc: 0.250]\n",
      "515 [D loss: (0.367)(R 0.244, F 0.489)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.652] [G acc: 0.438]\n",
      "516 [D loss: (0.417)(R 0.316, F 0.519)] [D acc: (0.781)(0.875, 0.688)] [G loss: 7.914] [G acc: 0.438]\n",
      "517 [D loss: (0.529)(R 0.334, F 0.724)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.575] [G acc: 0.438]\n",
      "518 [D loss: (0.479)(R 0.252, F 0.707)] [D acc: (0.812)(1.000, 0.625)] [G loss: 9.895] [G acc: 0.312]\n",
      "519 [D loss: (0.334)(R 0.274, F 0.395)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.349] [G acc: 0.375]\n",
      "520 [D loss: (0.568)(R 0.199, F 0.938)] [D acc: (0.719)(1.000, 0.438)] [G loss: 9.085] [G acc: 0.312]\n",
      "521 [D loss: (0.590)(R 0.442, F 0.737)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.759] [G acc: 0.375]\n",
      "522 [D loss: (0.329)(R 0.190, F 0.467)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.317] [G acc: 0.375]\n",
      "523 [D loss: (0.295)(R 0.219, F 0.370)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.610] [G acc: 0.438]\n",
      "524 [D loss: (0.265)(R 0.261, F 0.269)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.721] [G acc: 0.375]\n",
      "525 [D loss: (0.404)(R 0.333, F 0.474)] [D acc: (0.812)(0.938, 0.688)] [G loss: 8.283] [G acc: 0.375]\n",
      "526 [D loss: (0.292)(R 0.205, F 0.379)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.156] [G acc: 0.312]\n",
      "527 [D loss: (0.485)(R 0.235, F 0.736)] [D acc: (0.781)(1.000, 0.562)] [G loss: 8.559] [G acc: 0.312]\n",
      "528 [D loss: (0.412)(R 0.232, F 0.592)] [D acc: (0.812)(1.000, 0.625)] [G loss: 8.371] [G acc: 0.375]\n",
      "529 [D loss: (0.512)(R 0.332, F 0.693)] [D acc: (0.688)(0.875, 0.500)] [G loss: 7.438] [G acc: 0.375]\n",
      "530 [D loss: (0.267)(R 0.250, F 0.284)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.197] [G acc: 0.500]\n",
      "531 [D loss: (0.285)(R 0.201, F 0.368)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.288] [G acc: 0.375]\n",
      "532 [D loss: (0.368)(R 0.217, F 0.519)] [D acc: (0.844)(1.000, 0.688)] [G loss: 9.755] [G acc: 0.375]\n",
      "533 [D loss: (0.292)(R 0.225, F 0.359)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.208] [G acc: 0.312]\n",
      "534 [D loss: (0.319)(R 0.211, F 0.427)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.605] [G acc: 0.312]\n",
      "535 [D loss: (0.378)(R 0.272, F 0.484)] [D acc: (0.844)(1.000, 0.688)] [G loss: 9.474] [G acc: 0.250]\n",
      "536 [D loss: (0.444)(R 0.434, F 0.455)] [D acc: (0.781)(0.812, 0.750)] [G loss: 8.229] [G acc: 0.438]\n",
      "537 [D loss: (0.336)(R 0.424, F 0.249)] [D acc: (0.906)(0.938, 0.875)] [G loss: 12.461] [G acc: 0.188]\n",
      "538 [D loss: (0.326)(R 0.229, F 0.423)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.435] [G acc: 0.375]\n",
      "539 [D loss: (0.413)(R 0.244, F 0.581)] [D acc: (0.812)(1.000, 0.625)] [G loss: 8.903] [G acc: 0.312]\n",
      "540 [D loss: (0.428)(R 0.289, F 0.568)] [D acc: (0.781)(0.938, 0.625)] [G loss: 10.893] [G acc: 0.188]\n",
      "541 [D loss: (0.304)(R 0.313, F 0.295)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.401] [G acc: 0.438]\n",
      "542 [D loss: (0.418)(R 0.293, F 0.543)] [D acc: (0.812)(0.938, 0.688)] [G loss: 8.827] [G acc: 0.312]\n",
      "543 [D loss: (0.504)(R 0.334, F 0.674)] [D acc: (0.719)(0.938, 0.500)] [G loss: 8.160] [G acc: 0.438]\n",
      "544 [D loss: (0.528)(R 0.244, F 0.811)] [D acc: (0.750)(1.000, 0.500)] [G loss: 7.300] [G acc: 0.375]\n",
      "545 [D loss: (0.377)(R 0.275, F 0.479)] [D acc: (0.812)(0.938, 0.688)] [G loss: 10.055] [G acc: 0.188]\n",
      "546 [D loss: (0.286)(R 0.211, F 0.361)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.490] [G acc: 0.438]\n",
      "547 [D loss: (0.372)(R 0.244, F 0.500)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.741] [G acc: 0.375]\n",
      "548 [D loss: (0.469)(R 0.323, F 0.616)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.431] [G acc: 0.438]\n",
      "549 [D loss: (0.481)(R 0.292, F 0.671)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.901] [G acc: 0.375]\n",
      "550 [D loss: (0.394)(R 0.282, F 0.505)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.032] [G acc: 0.438]\n",
      "551 [D loss: (0.305)(R 0.233, F 0.377)] [D acc: (0.875)(1.000, 0.750)] [G loss: 10.424] [G acc: 0.188]\n",
      "552 [D loss: (0.403)(R 0.299, F 0.507)] [D acc: (0.812)(0.938, 0.688)] [G loss: 11.494] [G acc: 0.188]\n",
      "553 [D loss: (0.320)(R 0.292, F 0.347)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.784] [G acc: 0.375]\n",
      "554 [D loss: (0.642)(R 0.491, F 0.794)] [D acc: (0.719)(0.938, 0.500)] [G loss: 6.135] [G acc: 0.562]\n",
      "555 [D loss: (0.478)(R 0.265, F 0.692)] [D acc: (0.781)(1.000, 0.562)] [G loss: 8.453] [G acc: 0.438]\n",
      "556 [D loss: (0.548)(R 0.639, F 0.457)] [D acc: (0.812)(0.875, 0.750)] [G loss: 7.359] [G acc: 0.438]\n",
      "557 [D loss: (0.325)(R 0.218, F 0.431)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.441] [G acc: 0.500]\n",
      "558 [D loss: (0.490)(R 0.281, F 0.699)] [D acc: (0.750)(0.938, 0.562)] [G loss: 7.793] [G acc: 0.375]\n",
      "559 [D loss: (0.406)(R 0.258, F 0.554)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.913] [G acc: 0.562]\n",
      "560 [D loss: (0.444)(R 0.293, F 0.595)] [D acc: (0.781)(0.938, 0.625)] [G loss: 7.262] [G acc: 0.438]\n",
      "561 [D loss: (0.262)(R 0.225, F 0.299)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.455] [G acc: 0.312]\n",
      "562 [D loss: (0.427)(R 0.301, F 0.553)] [D acc: (0.812)(0.938, 0.688)] [G loss: 7.743] [G acc: 0.500]\n",
      "563 [D loss: (0.401)(R 0.238, F 0.563)] [D acc: (0.812)(1.000, 0.625)] [G loss: 11.113] [G acc: 0.125]\n",
      "564 [D loss: (0.324)(R 0.332, F 0.316)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.985] [G acc: 0.500]\n",
      "565 [D loss: (0.415)(R 0.269, F 0.561)] [D acc: (0.812)(1.000, 0.625)] [G loss: 10.707] [G acc: 0.250]\n",
      "566 [D loss: (0.431)(R 0.291, F 0.572)] [D acc: (0.781)(0.938, 0.625)] [G loss: 7.173] [G acc: 0.375]\n",
      "567 [D loss: (0.538)(R 0.565, F 0.512)] [D acc: (0.781)(0.875, 0.688)] [G loss: 7.798] [G acc: 0.500]\n",
      "568 [D loss: (0.404)(R 0.281, F 0.526)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.194] [G acc: 0.375]\n",
      "569 [D loss: (0.551)(R 0.259, F 0.844)] [D acc: (0.688)(1.000, 0.375)] [G loss: 8.100] [G acc: 0.375]\n",
      "570 [D loss: (0.338)(R 0.264, F 0.411)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.320] [G acc: 0.438]\n",
      "571 [D loss: (0.321)(R 0.238, F 0.405)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.584] [G acc: 0.438]\n",
      "572 [D loss: (0.613)(R 0.516, F 0.710)] [D acc: (0.750)(0.938, 0.562)] [G loss: 7.491] [G acc: 0.438]\n",
      "573 [D loss: (0.446)(R 0.293, F 0.600)] [D acc: (0.781)(1.000, 0.562)] [G loss: 9.529] [G acc: 0.375]\n",
      "574 [D loss: (0.359)(R 0.294, F 0.425)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.853] [G acc: 0.562]\n",
      "575 [D loss: (0.245)(R 0.290, F 0.201)] [D acc: (0.938)(1.000, 0.875)] [G loss: 12.860] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576 [D loss: (0.655)(R 0.256, F 1.054)] [D acc: (0.656)(1.000, 0.312)] [G loss: 8.074] [G acc: 0.438]\n",
      "577 [D loss: (0.386)(R 0.286, F 0.485)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.051] [G acc: 0.500]\n",
      "578 [D loss: (0.481)(R 0.334, F 0.627)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.739] [G acc: 0.562]\n",
      "579 [D loss: (0.581)(R 0.271, F 0.892)] [D acc: (0.656)(1.000, 0.312)] [G loss: 8.451] [G acc: 0.375]\n",
      "580 [D loss: (0.460)(R 0.306, F 0.613)] [D acc: (0.781)(1.000, 0.562)] [G loss: 8.166] [G acc: 0.250]\n",
      "581 [D loss: (0.347)(R 0.321, F 0.373)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.214] [G acc: 0.500]\n",
      "582 [D loss: (0.734)(R 0.539, F 0.929)] [D acc: (0.625)(0.875, 0.375)] [G loss: 6.700] [G acc: 0.438]\n",
      "583 [D loss: (0.490)(R 0.317, F 0.663)] [D acc: (0.750)(0.938, 0.562)] [G loss: 8.234] [G acc: 0.375]\n",
      "584 [D loss: (0.537)(R 0.407, F 0.668)] [D acc: (0.688)(0.812, 0.562)] [G loss: 7.399] [G acc: 0.438]\n",
      "585 [D loss: (0.380)(R 0.334, F 0.426)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.490] [G acc: 0.500]\n",
      "586 [D loss: (0.542)(R 0.348, F 0.737)] [D acc: (0.719)(0.938, 0.500)] [G loss: 10.845] [G acc: 0.250]\n",
      "587 [D loss: (0.551)(R 0.437, F 0.666)] [D acc: (0.688)(0.875, 0.500)] [G loss: 6.014] [G acc: 0.438]\n",
      "588 [D loss: (0.337)(R 0.286, F 0.389)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.809] [G acc: 0.438]\n",
      "589 [D loss: (0.449)(R 0.284, F 0.615)] [D acc: (0.781)(1.000, 0.562)] [G loss: 8.844] [G acc: 0.375]\n",
      "590 [D loss: (0.443)(R 0.313, F 0.572)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.826] [G acc: 0.312]\n",
      "591 [D loss: (0.353)(R 0.371, F 0.334)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.997] [G acc: 0.438]\n",
      "592 [D loss: (0.358)(R 0.247, F 0.470)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.073] [G acc: 0.438]\n",
      "593 [D loss: (0.376)(R 0.299, F 0.453)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.069] [G acc: 0.375]\n",
      "594 [D loss: (0.340)(R 0.268, F 0.412)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.245] [G acc: 0.250]\n",
      "595 [D loss: (0.530)(R 0.531, F 0.530)] [D acc: (0.781)(0.938, 0.625)] [G loss: 7.506] [G acc: 0.500]\n",
      "596 [D loss: (0.466)(R 0.358, F 0.574)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.550] [G acc: 0.375]\n",
      "597 [D loss: (0.469)(R 0.358, F 0.581)] [D acc: (0.750)(0.938, 0.562)] [G loss: 8.281] [G acc: 0.312]\n",
      "598 [D loss: (0.415)(R 0.318, F 0.512)] [D acc: (0.781)(0.875, 0.688)] [G loss: 8.784] [G acc: 0.312]\n",
      "599 [D loss: (0.359)(R 0.270, F 0.448)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.745] [G acc: 0.438]\n",
      "600 [D loss: (0.374)(R 0.390, F 0.359)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.178] [G acc: 0.438]\n",
      "601 [D loss: (0.439)(R 0.285, F 0.593)] [D acc: (0.781)(1.000, 0.562)] [G loss: 9.320] [G acc: 0.375]\n",
      "602 [D loss: (0.412)(R 0.274, F 0.550)] [D acc: (0.812)(1.000, 0.625)] [G loss: 8.053] [G acc: 0.375]\n",
      "603 [D loss: (0.237)(R 0.303, F 0.172)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.673] [G acc: 0.562]\n",
      "604 [D loss: (0.381)(R 0.267, F 0.494)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.775] [G acc: 0.562]\n",
      "605 [D loss: (0.277)(R 0.303, F 0.250)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.622] [G acc: 0.438]\n",
      "606 [D loss: (0.326)(R 0.318, F 0.334)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.971] [G acc: 0.500]\n",
      "607 [D loss: (0.555)(R 0.284, F 0.826)] [D acc: (0.719)(1.000, 0.438)] [G loss: 7.870] [G acc: 0.500]\n",
      "608 [D loss: (0.294)(R 0.278, F 0.310)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.188] [G acc: 0.500]\n",
      "609 [D loss: (0.307)(R 0.310, F 0.303)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.168] [G acc: 0.438]\n",
      "610 [D loss: (0.386)(R 0.509, F 0.262)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.659] [G acc: 0.375]\n",
      "611 [D loss: (0.314)(R 0.272, F 0.356)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.407] [G acc: 0.625]\n",
      "612 [D loss: (0.578)(R 0.318, F 0.839)] [D acc: (0.656)(0.938, 0.375)] [G loss: 7.401] [G acc: 0.375]\n",
      "613 [D loss: (0.386)(R 0.296, F 0.476)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.661] [G acc: 0.562]\n",
      "614 [D loss: (0.439)(R 0.268, F 0.610)] [D acc: (0.781)(1.000, 0.562)] [G loss: 7.947] [G acc: 0.375]\n",
      "615 [D loss: (0.435)(R 0.282, F 0.589)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.688] [G acc: 0.625]\n",
      "616 [D loss: (0.424)(R 0.246, F 0.602)] [D acc: (0.750)(1.000, 0.500)] [G loss: 7.988] [G acc: 0.375]\n",
      "617 [D loss: (0.535)(R 0.261, F 0.810)] [D acc: (0.719)(1.000, 0.438)] [G loss: 6.669] [G acc: 0.438]\n",
      "618 [D loss: (0.558)(R 0.358, F 0.759)] [D acc: (0.688)(0.938, 0.438)] [G loss: 6.236] [G acc: 0.500]\n",
      "619 [D loss: (0.368)(R 0.273, F 0.462)] [D acc: (0.844)(1.000, 0.688)] [G loss: 5.750] [G acc: 0.562]\n",
      "620 [D loss: (0.479)(R 0.447, F 0.511)] [D acc: (0.781)(0.938, 0.625)] [G loss: 7.179] [G acc: 0.500]\n",
      "621 [D loss: (0.400)(R 0.297, F 0.503)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.083] [G acc: 0.438]\n",
      "622 [D loss: (0.469)(R 0.273, F 0.666)] [D acc: (0.750)(1.000, 0.500)] [G loss: 6.941] [G acc: 0.562]\n",
      "623 [D loss: (0.565)(R 0.380, F 0.750)] [D acc: (0.688)(0.938, 0.438)] [G loss: 7.724] [G acc: 0.375]\n",
      "624 [D loss: (0.366)(R 0.350, F 0.382)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.909] [G acc: 0.375]\n",
      "625 [D loss: (0.364)(R 0.289, F 0.438)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.281] [G acc: 0.562]\n",
      "626 [D loss: (0.444)(R 0.448, F 0.440)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.898] [G acc: 0.438]\n",
      "627 [D loss: (0.385)(R 0.326, F 0.444)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.368] [G acc: 0.562]\n",
      "628 [D loss: (0.499)(R 0.340, F 0.658)] [D acc: (0.750)(1.000, 0.500)] [G loss: 5.986] [G acc: 0.625]\n",
      "629 [D loss: (0.543)(R 0.320, F 0.765)] [D acc: (0.719)(1.000, 0.438)] [G loss: 7.551] [G acc: 0.438]\n",
      "630 [D loss: (0.360)(R 0.302, F 0.418)] [D acc: (0.844)(1.000, 0.688)] [G loss: 5.284] [G acc: 0.562]\n",
      "631 [D loss: (0.560)(R 0.276, F 0.844)] [D acc: (0.719)(1.000, 0.438)] [G loss: 6.669] [G acc: 0.438]\n",
      "632 [D loss: (0.378)(R 0.302, F 0.455)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.145] [G acc: 0.562]\n",
      "633 [D loss: (0.533)(R 0.385, F 0.681)] [D acc: (0.688)(0.938, 0.438)] [G loss: 7.116] [G acc: 0.500]\n",
      "634 [D loss: (0.430)(R 0.340, F 0.520)] [D acc: (0.781)(1.000, 0.562)] [G loss: 7.205] [G acc: 0.438]\n",
      "635 [D loss: (0.505)(R 0.428, F 0.582)] [D acc: (0.719)(0.938, 0.500)] [G loss: 6.873] [G acc: 0.312]\n",
      "636 [D loss: (0.237)(R 0.310, F 0.165)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.406] [G acc: 0.375]\n",
      "637 [D loss: (0.444)(R 0.439, F 0.450)] [D acc: (0.781)(0.875, 0.688)] [G loss: 5.939] [G acc: 0.500]\n",
      "638 [D loss: (0.442)(R 0.310, F 0.575)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.316] [G acc: 0.500]\n",
      "639 [D loss: (0.549)(R 0.402, F 0.696)] [D acc: (0.688)(0.875, 0.500)] [G loss: 6.119] [G acc: 0.562]\n",
      "640 [D loss: (0.349)(R 0.325, F 0.372)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.175] [G acc: 0.562]\n",
      "641 [D loss: (0.424)(R 0.283, F 0.565)] [D acc: (0.781)(1.000, 0.562)] [G loss: 4.699] [G acc: 0.625]\n",
      "642 [D loss: (0.446)(R 0.305, F 0.586)] [D acc: (0.781)(1.000, 0.562)] [G loss: 7.482] [G acc: 0.500]\n",
      "643 [D loss: (0.438)(R 0.385, F 0.490)] [D acc: (0.781)(0.938, 0.625)] [G loss: 7.686] [G acc: 0.375]\n",
      "644 [D loss: (0.501)(R 0.327, F 0.674)] [D acc: (0.750)(1.000, 0.500)] [G loss: 7.528] [G acc: 0.438]\n",
      "645 [D loss: (0.490)(R 0.311, F 0.669)] [D acc: (0.750)(1.000, 0.500)] [G loss: 6.394] [G acc: 0.500]\n",
      "646 [D loss: (0.491)(R 0.306, F 0.676)] [D acc: (0.719)(1.000, 0.438)] [G loss: 5.122] [G acc: 0.625]\n",
      "647 [D loss: (0.506)(R 0.507, F 0.505)] [D acc: (0.688)(0.812, 0.562)] [G loss: 5.394] [G acc: 0.562]\n",
      "648 [D loss: (0.406)(R 0.293, F 0.520)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.638] [G acc: 0.500]\n",
      "649 [D loss: (0.469)(R 0.311, F 0.627)] [D acc: (0.750)(1.000, 0.500)] [G loss: 7.588] [G acc: 0.438]\n",
      "650 [D loss: (0.471)(R 0.334, F 0.608)] [D acc: (0.719)(0.938, 0.500)] [G loss: 7.120] [G acc: 0.438]\n",
      "651 [D loss: (0.442)(R 0.322, F 0.563)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.222] [G acc: 0.438]\n",
      "652 [D loss: (0.407)(R 0.343, F 0.470)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.578] [G acc: 0.625]\n",
      "653 [D loss: (0.414)(R 0.299, F 0.528)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.386] [G acc: 0.562]\n",
      "654 [D loss: (0.449)(R 0.335, F 0.564)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.880] [G acc: 0.500]\n",
      "655 [D loss: (0.476)(R 0.325, F 0.628)] [D acc: (0.781)(1.000, 0.562)] [G loss: 8.022] [G acc: 0.438]\n",
      "656 [D loss: (0.342)(R 0.327, F 0.358)] [D acc: (0.844)(1.000, 0.688)] [G loss: 8.390] [G acc: 0.312]\n",
      "657 [D loss: (0.352)(R 0.371, F 0.332)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.404] [G acc: 0.312]\n",
      "658 [D loss: (0.436)(R 0.366, F 0.506)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.612] [G acc: 0.562]\n",
      "659 [D loss: (0.612)(R 0.285, F 0.938)] [D acc: (0.625)(1.000, 0.250)] [G loss: 4.572] [G acc: 0.562]\n",
      "660 [D loss: (0.608)(R 0.369, F 0.848)] [D acc: (0.656)(1.000, 0.312)] [G loss: 5.637] [G acc: 0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 [D loss: (0.377)(R 0.361, F 0.392)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.016] [G acc: 0.625]\n",
      "662 [D loss: (0.397)(R 0.387, F 0.407)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.939] [G acc: 0.500]\n",
      "663 [D loss: (0.527)(R 0.329, F 0.725)] [D acc: (0.719)(1.000, 0.438)] [G loss: 5.416] [G acc: 0.500]\n",
      "664 [D loss: (0.427)(R 0.371, F 0.483)] [D acc: (0.781)(1.000, 0.562)] [G loss: 7.959] [G acc: 0.375]\n",
      "665 [D loss: (0.421)(R 0.335, F 0.507)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.929] [G acc: 0.500]\n",
      "666 [D loss: (0.525)(R 0.359, F 0.691)] [D acc: (0.719)(0.938, 0.500)] [G loss: 4.268] [G acc: 0.625]\n",
      "667 [D loss: (0.423)(R 0.304, F 0.541)] [D acc: (0.781)(1.000, 0.562)] [G loss: 7.617] [G acc: 0.375]\n",
      "668 [D loss: (0.498)(R 0.323, F 0.673)] [D acc: (0.719)(1.000, 0.438)] [G loss: 5.412] [G acc: 0.500]\n",
      "669 [D loss: (0.595)(R 0.372, F 0.818)] [D acc: (0.656)(1.000, 0.312)] [G loss: 6.703] [G acc: 0.500]\n",
      "670 [D loss: (0.430)(R 0.325, F 0.535)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.210] [G acc: 0.500]\n",
      "671 [D loss: (0.410)(R 0.335, F 0.484)] [D acc: (0.812)(1.000, 0.625)] [G loss: 7.548] [G acc: 0.438]\n",
      "672 [D loss: (0.486)(R 0.422, F 0.551)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.239] [G acc: 0.562]\n",
      "673 [D loss: (0.616)(R 0.333, F 0.898)] [D acc: (0.625)(0.938, 0.312)] [G loss: 8.208] [G acc: 0.312]\n",
      "674 [D loss: (0.481)(R 0.434, F 0.529)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.676] [G acc: 0.438]\n",
      "675 [D loss: (0.341)(R 0.309, F 0.374)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.603] [G acc: 0.500]\n",
      "676 [D loss: (0.444)(R 0.384, F 0.505)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.933] [G acc: 0.500]\n",
      "677 [D loss: (0.486)(R 0.349, F 0.623)] [D acc: (0.750)(1.000, 0.500)] [G loss: 9.642] [G acc: 0.375]\n",
      "678 [D loss: (0.667)(R 0.345, F 0.990)] [D acc: (0.594)(1.000, 0.188)] [G loss: 6.849] [G acc: 0.500]\n",
      "679 [D loss: (0.420)(R 0.417, F 0.423)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.417] [G acc: 0.625]\n",
      "680 [D loss: (0.636)(R 0.562, F 0.710)] [D acc: (0.656)(0.875, 0.438)] [G loss: 6.891] [G acc: 0.562]\n",
      "681 [D loss: (0.469)(R 0.434, F 0.504)] [D acc: (0.750)(0.938, 0.562)] [G loss: 9.688] [G acc: 0.250]\n",
      "682 [D loss: (0.479)(R 0.376, F 0.581)] [D acc: (0.719)(0.875, 0.562)] [G loss: 5.934] [G acc: 0.562]\n",
      "683 [D loss: (0.539)(R 0.353, F 0.725)] [D acc: (0.688)(1.000, 0.375)] [G loss: 5.543] [G acc: 0.562]\n",
      "684 [D loss: (0.432)(R 0.363, F 0.500)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.951] [G acc: 0.438]\n",
      "685 [D loss: (0.454)(R 0.331, F 0.577)] [D acc: (0.750)(1.000, 0.500)] [G loss: 8.895] [G acc: 0.375]\n",
      "686 [D loss: (0.507)(R 0.351, F 0.663)] [D acc: (0.750)(1.000, 0.500)] [G loss: 4.645] [G acc: 0.500]\n",
      "687 [D loss: (0.540)(R 0.317, F 0.763)] [D acc: (0.688)(1.000, 0.375)] [G loss: 7.590] [G acc: 0.438]\n",
      "688 [D loss: (0.434)(R 0.426, F 0.441)] [D acc: (0.781)(0.938, 0.625)] [G loss: 8.352] [G acc: 0.375]\n",
      "689 [D loss: (0.410)(R 0.344, F 0.476)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.829] [G acc: 0.438]\n",
      "690 [D loss: (0.640)(R 0.533, F 0.747)] [D acc: (0.656)(0.938, 0.375)] [G loss: 6.147] [G acc: 0.500]\n",
      "691 [D loss: (0.626)(R 0.531, F 0.722)] [D acc: (0.656)(0.938, 0.375)] [G loss: 5.877] [G acc: 0.500]\n",
      "692 [D loss: (0.401)(R 0.336, F 0.467)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.414] [G acc: 0.625]\n",
      "693 [D loss: (0.462)(R 0.441, F 0.482)] [D acc: (0.719)(0.875, 0.562)] [G loss: 8.896] [G acc: 0.375]\n",
      "694 [D loss: (0.418)(R 0.320, F 0.517)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.761] [G acc: 0.562]\n",
      "695 [D loss: (0.372)(R 0.351, F 0.394)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.947] [G acc: 0.500]\n",
      "696 [D loss: (0.592)(R 0.481, F 0.703)] [D acc: (0.656)(0.938, 0.375)] [G loss: 9.635] [G acc: 0.250]\n",
      "697 [D loss: (0.509)(R 0.426, F 0.591)] [D acc: (0.688)(0.875, 0.500)] [G loss: 6.185] [G acc: 0.438]\n",
      "698 [D loss: (0.565)(R 0.503, F 0.628)] [D acc: (0.625)(0.875, 0.375)] [G loss: 5.180] [G acc: 0.625]\n",
      "699 [D loss: (0.543)(R 0.452, F 0.633)] [D acc: (0.656)(0.875, 0.438)] [G loss: 6.888] [G acc: 0.500]\n",
      "700 [D loss: (0.496)(R 0.398, F 0.594)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.035] [G acc: 0.438]\n",
      "701 [D loss: (0.517)(R 0.427, F 0.606)] [D acc: (0.719)(0.938, 0.500)] [G loss: 6.123] [G acc: 0.312]\n",
      "702 [D loss: (0.511)(R 0.369, F 0.654)] [D acc: (0.656)(1.000, 0.312)] [G loss: 7.111] [G acc: 0.375]\n",
      "703 [D loss: (0.456)(R 0.486, F 0.427)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.547] [G acc: 0.375]\n",
      "704 [D loss: (0.480)(R 0.406, F 0.554)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.729] [G acc: 0.500]\n",
      "705 [D loss: (0.508)(R 0.397, F 0.619)] [D acc: (0.750)(1.000, 0.500)] [G loss: 5.070] [G acc: 0.688]\n",
      "706 [D loss: (0.566)(R 0.427, F 0.704)] [D acc: (0.688)(0.938, 0.438)] [G loss: 5.364] [G acc: 0.500]\n",
      "707 [D loss: (0.421)(R 0.376, F 0.466)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.917] [G acc: 0.438]\n",
      "708 [D loss: (0.586)(R 0.433, F 0.738)] [D acc: (0.656)(0.938, 0.375)] [G loss: 7.975] [G acc: 0.375]\n",
      "709 [D loss: (0.527)(R 0.398, F 0.655)] [D acc: (0.719)(1.000, 0.438)] [G loss: 7.056] [G acc: 0.438]\n",
      "710 [D loss: (0.445)(R 0.360, F 0.530)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.152] [G acc: 0.625]\n",
      "711 [D loss: (0.484)(R 0.375, F 0.593)] [D acc: (0.719)(0.938, 0.500)] [G loss: 6.198] [G acc: 0.375]\n",
      "712 [D loss: (0.432)(R 0.351, F 0.513)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.480] [G acc: 0.438]\n",
      "713 [D loss: (0.402)(R 0.356, F 0.449)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.144] [G acc: 0.562]\n",
      "714 [D loss: (0.554)(R 0.393, F 0.716)] [D acc: (0.719)(1.000, 0.438)] [G loss: 4.455] [G acc: 0.438]\n",
      "715 [D loss: (0.634)(R 0.491, F 0.777)] [D acc: (0.594)(0.875, 0.312)] [G loss: 8.960] [G acc: 0.375]\n",
      "716 [D loss: (0.555)(R 0.346, F 0.765)] [D acc: (0.656)(0.938, 0.375)] [G loss: 6.156] [G acc: 0.500]\n",
      "717 [D loss: (0.365)(R 0.503, F 0.226)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.356] [G acc: 0.375]\n",
      "718 [D loss: (0.547)(R 0.367, F 0.727)] [D acc: (0.656)(1.000, 0.312)] [G loss: 5.703] [G acc: 0.438]\n",
      "719 [D loss: (0.535)(R 0.405, F 0.665)] [D acc: (0.688)(0.875, 0.500)] [G loss: 7.957] [G acc: 0.500]\n",
      "720 [D loss: (0.463)(R 0.400, F 0.526)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.791] [G acc: 0.375]\n",
      "721 [D loss: (0.448)(R 0.390, F 0.507)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.102] [G acc: 0.375]\n",
      "722 [D loss: (0.605)(R 0.408, F 0.802)] [D acc: (0.594)(0.938, 0.250)] [G loss: 7.793] [G acc: 0.375]\n",
      "723 [D loss: (0.463)(R 0.474, F 0.451)] [D acc: (0.719)(0.875, 0.562)] [G loss: 4.899] [G acc: 0.625]\n",
      "724 [D loss: (0.406)(R 0.347, F 0.465)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.935] [G acc: 0.562]\n",
      "725 [D loss: (0.454)(R 0.442, F 0.466)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.061] [G acc: 0.562]\n",
      "726 [D loss: (0.493)(R 0.472, F 0.513)] [D acc: (0.781)(0.938, 0.625)] [G loss: 5.604] [G acc: 0.438]\n",
      "727 [D loss: (0.435)(R 0.361, F 0.508)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.052] [G acc: 0.500]\n",
      "728 [D loss: (0.520)(R 0.372, F 0.668)] [D acc: (0.688)(0.938, 0.438)] [G loss: 6.848] [G acc: 0.438]\n",
      "729 [D loss: (0.479)(R 0.338, F 0.621)] [D acc: (0.719)(1.000, 0.438)] [G loss: 4.580] [G acc: 0.625]\n",
      "730 [D loss: (0.369)(R 0.404, F 0.334)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.995] [G acc: 0.375]\n",
      "731 [D loss: (0.398)(R 0.403, F 0.392)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.099] [G acc: 0.625]\n",
      "732 [D loss: (0.480)(R 0.396, F 0.563)] [D acc: (0.719)(0.938, 0.500)] [G loss: 5.594] [G acc: 0.438]\n",
      "733 [D loss: (0.434)(R 0.393, F 0.474)] [D acc: (0.812)(1.000, 0.625)] [G loss: 8.847] [G acc: 0.250]\n",
      "734 [D loss: (0.496)(R 0.436, F 0.555)] [D acc: (0.688)(0.875, 0.500)] [G loss: 8.452] [G acc: 0.250]\n",
      "735 [D loss: (0.469)(R 0.387, F 0.552)] [D acc: (0.750)(0.938, 0.562)] [G loss: 4.950] [G acc: 0.625]\n",
      "736 [D loss: (0.340)(R 0.337, F 0.344)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.690] [G acc: 0.438]\n",
      "737 [D loss: (0.506)(R 0.417, F 0.596)] [D acc: (0.750)(1.000, 0.500)] [G loss: 5.787] [G acc: 0.500]\n",
      "738 [D loss: (0.361)(R 0.360, F 0.361)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.777] [G acc: 0.562]\n",
      "739 [D loss: (0.607)(R 0.376, F 0.839)] [D acc: (0.625)(1.000, 0.250)] [G loss: 8.014] [G acc: 0.188]\n",
      "740 [D loss: (0.414)(R 0.437, F 0.391)] [D acc: (0.781)(0.875, 0.688)] [G loss: 6.906] [G acc: 0.375]\n",
      "741 [D loss: (0.478)(R 0.373, F 0.582)] [D acc: (0.719)(0.938, 0.500)] [G loss: 6.181] [G acc: 0.500]\n",
      "742 [D loss: (0.423)(R 0.421, F 0.425)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.925] [G acc: 0.500]\n",
      "743 [D loss: (0.581)(R 0.492, F 0.671)] [D acc: (0.656)(0.875, 0.438)] [G loss: 6.090] [G acc: 0.562]\n",
      "744 [D loss: (0.497)(R 0.460, F 0.534)] [D acc: (0.719)(0.812, 0.625)] [G loss: 6.170] [G acc: 0.375]\n",
      "745 [D loss: (0.454)(R 0.365, F 0.542)] [D acc: (0.781)(1.000, 0.562)] [G loss: 7.031] [G acc: 0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746 [D loss: (0.516)(R 0.386, F 0.646)] [D acc: (0.719)(0.938, 0.500)] [G loss: 7.141] [G acc: 0.438]\n",
      "747 [D loss: (0.391)(R 0.371, F 0.412)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.889] [G acc: 0.438]\n",
      "748 [D loss: (0.529)(R 0.476, F 0.581)] [D acc: (0.688)(0.938, 0.438)] [G loss: 5.465] [G acc: 0.312]\n",
      "749 [D loss: (0.435)(R 0.367, F 0.503)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.828] [G acc: 0.438]\n",
      "750 [D loss: (0.505)(R 0.351, F 0.658)] [D acc: (0.719)(1.000, 0.438)] [G loss: 6.474] [G acc: 0.375]\n",
      "751 [D loss: (0.601)(R 0.471, F 0.731)] [D acc: (0.594)(0.812, 0.375)] [G loss: 4.704] [G acc: 0.688]\n",
      "752 [D loss: (0.627)(R 0.506, F 0.748)] [D acc: (0.594)(0.812, 0.375)] [G loss: 8.009] [G acc: 0.312]\n",
      "753 [D loss: (0.473)(R 0.452, F 0.493)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.082] [G acc: 0.625]\n",
      "754 [D loss: (0.508)(R 0.430, F 0.587)] [D acc: (0.688)(0.875, 0.500)] [G loss: 6.147] [G acc: 0.375]\n",
      "755 [D loss: (0.455)(R 0.429, F 0.482)] [D acc: (0.719)(0.938, 0.500)] [G loss: 4.781] [G acc: 0.438]\n",
      "756 [D loss: (0.417)(R 0.390, F 0.444)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.706] [G acc: 0.188]\n",
      "757 [D loss: (0.577)(R 0.423, F 0.731)] [D acc: (0.719)(0.938, 0.500)] [G loss: 7.980] [G acc: 0.375]\n",
      "758 [D loss: (0.494)(R 0.444, F 0.543)] [D acc: (0.688)(0.875, 0.500)] [G loss: 6.264] [G acc: 0.375]\n",
      "759 [D loss: (0.528)(R 0.442, F 0.614)] [D acc: (0.719)(0.875, 0.562)] [G loss: 6.098] [G acc: 0.500]\n",
      "760 [D loss: (0.556)(R 0.417, F 0.695)] [D acc: (0.656)(1.000, 0.312)] [G loss: 7.429] [G acc: 0.250]\n",
      "761 [D loss: (0.500)(R 0.444, F 0.556)] [D acc: (0.719)(0.875, 0.562)] [G loss: 5.589] [G acc: 0.375]\n",
      "762 [D loss: (0.391)(R 0.333, F 0.449)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.244] [G acc: 0.500]\n",
      "763 [D loss: (0.413)(R 0.363, F 0.463)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.317] [G acc: 0.375]\n",
      "764 [D loss: (0.391)(R 0.362, F 0.419)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.486] [G acc: 0.375]\n",
      "765 [D loss: (0.534)(R 0.454, F 0.614)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.130] [G acc: 0.500]\n",
      "766 [D loss: (0.375)(R 0.378, F 0.372)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.118] [G acc: 0.500]\n",
      "767 [D loss: (0.445)(R 0.402, F 0.487)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.317] [G acc: 0.438]\n",
      "768 [D loss: (0.422)(R 0.380, F 0.463)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.675] [G acc: 0.438]\n",
      "769 [D loss: (0.438)(R 0.410, F 0.465)] [D acc: (0.812)(0.938, 0.688)] [G loss: 7.394] [G acc: 0.312]\n",
      "770 [D loss: (0.431)(R 0.427, F 0.435)] [D acc: (0.750)(0.938, 0.562)] [G loss: 7.129] [G acc: 0.312]\n",
      "771 [D loss: (0.389)(R 0.417, F 0.362)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.453] [G acc: 0.438]\n",
      "772 [D loss: (0.452)(R 0.415, F 0.489)] [D acc: (0.781)(0.875, 0.688)] [G loss: 6.697] [G acc: 0.250]\n",
      "773 [D loss: (0.498)(R 0.417, F 0.578)] [D acc: (0.781)(1.000, 0.562)] [G loss: 6.249] [G acc: 0.375]\n",
      "774 [D loss: (0.488)(R 0.350, F 0.625)] [D acc: (0.750)(1.000, 0.500)] [G loss: 6.049] [G acc: 0.312]\n",
      "775 [D loss: (0.624)(R 0.518, F 0.729)] [D acc: (0.656)(0.875, 0.438)] [G loss: 6.303] [G acc: 0.312]\n",
      "776 [D loss: (0.362)(R 0.462, F 0.263)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.535] [G acc: 0.500]\n",
      "777 [D loss: (0.404)(R 0.349, F 0.459)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.321] [G acc: 0.375]\n",
      "778 [D loss: (0.371)(R 0.372, F 0.371)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.361] [G acc: 0.375]\n",
      "779 [D loss: (0.406)(R 0.382, F 0.430)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.577] [G acc: 0.188]\n",
      "780 [D loss: (0.472)(R 0.429, F 0.514)] [D acc: (0.719)(0.938, 0.500)] [G loss: 7.374] [G acc: 0.188]\n",
      "781 [D loss: (0.392)(R 0.392, F 0.393)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.928] [G acc: 0.250]\n",
      "782 [D loss: (0.612)(R 0.695, F 0.529)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.816] [G acc: 0.312]\n",
      "783 [D loss: (0.431)(R 0.396, F 0.466)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.505] [G acc: 0.250]\n",
      "784 [D loss: (0.463)(R 0.486, F 0.440)] [D acc: (0.719)(0.812, 0.625)] [G loss: 5.623] [G acc: 0.250]\n",
      "785 [D loss: (0.424)(R 0.428, F 0.419)] [D acc: (0.812)(1.000, 0.625)] [G loss: 7.727] [G acc: 0.188]\n",
      "786 [D loss: (0.310)(R 0.388, F 0.231)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.619] [G acc: 0.312]\n",
      "787 [D loss: (0.421)(R 0.353, F 0.489)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.995] [G acc: 0.500]\n",
      "788 [D loss: (0.339)(R 0.347, F 0.330)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.898] [G acc: 0.312]\n",
      "789 [D loss: (0.488)(R 0.426, F 0.550)] [D acc: (0.719)(0.938, 0.500)] [G loss: 7.414] [G acc: 0.375]\n",
      "790 [D loss: (0.414)(R 0.327, F 0.501)] [D acc: (0.781)(1.000, 0.562)] [G loss: 4.633] [G acc: 0.312]\n",
      "791 [D loss: (0.490)(R 0.398, F 0.582)] [D acc: (0.781)(1.000, 0.562)] [G loss: 4.604] [G acc: 0.312]\n",
      "792 [D loss: (0.334)(R 0.443, F 0.226)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.581] [G acc: 0.312]\n",
      "793 [D loss: (0.380)(R 0.360, F 0.400)] [D acc: (0.844)(1.000, 0.688)] [G loss: 5.257] [G acc: 0.312]\n",
      "794 [D loss: (0.361)(R 0.404, F 0.318)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.509] [G acc: 0.188]\n",
      "795 [D loss: (0.324)(R 0.368, F 0.280)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.273] [G acc: 0.250]\n",
      "796 [D loss: (0.434)(R 0.382, F 0.487)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.389] [G acc: 0.250]\n",
      "797 [D loss: (0.421)(R 0.403, F 0.438)] [D acc: (0.781)(0.875, 0.688)] [G loss: 6.969] [G acc: 0.375]\n",
      "798 [D loss: (0.464)(R 0.519, F 0.408)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.871] [G acc: 0.125]\n",
      "799 [D loss: (0.462)(R 0.449, F 0.475)] [D acc: (0.812)(0.938, 0.688)] [G loss: 8.190] [G acc: 0.188]\n",
      "800 [D loss: (0.454)(R 0.371, F 0.536)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.657] [G acc: 0.188]\n",
      "801 [D loss: (0.490)(R 0.411, F 0.570)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.671] [G acc: 0.188]\n",
      "802 [D loss: (0.372)(R 0.352, F 0.392)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.430] [G acc: 0.188]\n",
      "803 [D loss: (0.408)(R 0.429, F 0.388)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.305] [G acc: 0.250]\n",
      "804 [D loss: (0.491)(R 0.453, F 0.530)] [D acc: (0.750)(0.875, 0.625)] [G loss: 5.840] [G acc: 0.188]\n",
      "805 [D loss: (0.379)(R 0.343, F 0.415)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.132] [G acc: 0.500]\n",
      "806 [D loss: (0.390)(R 0.398, F 0.382)] [D acc: (0.906)(0.938, 0.875)] [G loss: 9.353] [G acc: 0.250]\n",
      "807 [D loss: (0.433)(R 0.282, F 0.583)] [D acc: (0.750)(1.000, 0.500)] [G loss: 5.646] [G acc: 0.250]\n",
      "808 [D loss: (0.379)(R 0.333, F 0.425)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.191] [G acc: 0.188]\n",
      "809 [D loss: (0.399)(R 0.461, F 0.336)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.493] [G acc: 0.125]\n",
      "810 [D loss: (0.342)(R 0.415, F 0.269)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.131] [G acc: 0.375]\n",
      "811 [D loss: (0.421)(R 0.341, F 0.501)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.997] [G acc: 0.125]\n",
      "812 [D loss: (0.421)(R 0.337, F 0.505)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.620] [G acc: 0.188]\n",
      "813 [D loss: (0.396)(R 0.376, F 0.416)] [D acc: (0.812)(0.875, 0.750)] [G loss: 7.125] [G acc: 0.312]\n",
      "814 [D loss: (0.421)(R 0.334, F 0.508)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.267] [G acc: 0.250]\n",
      "815 [D loss: (0.492)(R 0.431, F 0.553)] [D acc: (0.719)(0.875, 0.562)] [G loss: 5.335] [G acc: 0.250]\n",
      "816 [D loss: (0.395)(R 0.434, F 0.356)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.867] [G acc: 0.250]\n",
      "817 [D loss: (0.423)(R 0.384, F 0.462)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.699] [G acc: 0.188]\n",
      "818 [D loss: (0.462)(R 0.438, F 0.486)] [D acc: (0.781)(0.875, 0.688)] [G loss: 6.429] [G acc: 0.250]\n",
      "819 [D loss: (0.419)(R 0.452, F 0.385)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.028] [G acc: 0.125]\n",
      "820 [D loss: (0.349)(R 0.380, F 0.318)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.513] [G acc: 0.250]\n",
      "821 [D loss: (0.426)(R 0.421, F 0.432)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.529] [G acc: 0.000]\n",
      "822 [D loss: (0.300)(R 0.373, F 0.228)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.312] [G acc: 0.188]\n",
      "823 [D loss: (0.329)(R 0.366, F 0.291)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.831] [G acc: 0.188]\n",
      "824 [D loss: (0.395)(R 0.357, F 0.433)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.447] [G acc: 0.125]\n",
      "825 [D loss: (0.364)(R 0.347, F 0.380)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.624] [G acc: 0.125]\n",
      "826 [D loss: (0.272)(R 0.320, F 0.225)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.112] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827 [D loss: (0.431)(R 0.434, F 0.427)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.471] [G acc: 0.375]\n",
      "828 [D loss: (0.458)(R 0.325, F 0.591)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.253] [G acc: 0.125]\n",
      "829 [D loss: (0.275)(R 0.336, F 0.214)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.461] [G acc: 0.188]\n",
      "830 [D loss: (0.602)(R 0.785, F 0.418)] [D acc: (0.719)(0.625, 0.812)] [G loss: 7.029] [G acc: 0.000]\n",
      "831 [D loss: (0.356)(R 0.377, F 0.336)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.657] [G acc: 0.250]\n",
      "832 [D loss: (0.398)(R 0.304, F 0.492)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.350] [G acc: 0.188]\n",
      "833 [D loss: (0.305)(R 0.321, F 0.290)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.170] [G acc: 0.250]\n",
      "834 [D loss: (0.439)(R 0.374, F 0.504)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.033] [G acc: 0.250]\n",
      "835 [D loss: (0.410)(R 0.615, F 0.205)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.023] [G acc: 0.188]\n",
      "836 [D loss: (0.322)(R 0.331, F 0.313)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.399] [G acc: 0.125]\n",
      "837 [D loss: (0.318)(R 0.386, F 0.251)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.248] [G acc: 0.312]\n",
      "838 [D loss: (0.432)(R 0.460, F 0.403)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.866] [G acc: 0.125]\n",
      "839 [D loss: (0.279)(R 0.321, F 0.236)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.537] [G acc: 0.188]\n",
      "840 [D loss: (0.399)(R 0.418, F 0.381)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.388] [G acc: 0.250]\n",
      "841 [D loss: (0.334)(R 0.400, F 0.269)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.671] [G acc: 0.188]\n",
      "842 [D loss: (0.357)(R 0.363, F 0.350)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.577] [G acc: 0.062]\n",
      "843 [D loss: (0.310)(R 0.329, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.407] [G acc: 0.312]\n",
      "844 [D loss: (0.381)(R 0.262, F 0.500)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.702] [G acc: 0.125]\n",
      "845 [D loss: (0.263)(R 0.278, F 0.248)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.155] [G acc: 0.125]\n",
      "846 [D loss: (0.475)(R 0.601, F 0.350)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.831] [G acc: 0.000]\n",
      "847 [D loss: (0.256)(R 0.346, F 0.166)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.001] [G acc: 0.062]\n",
      "848 [D loss: (0.321)(R 0.359, F 0.283)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.666] [G acc: 0.000]\n",
      "849 [D loss: (0.348)(R 0.290, F 0.406)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.325] [G acc: 0.062]\n",
      "850 [D loss: (0.340)(R 0.357, F 0.323)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.519] [G acc: 0.125]\n",
      "851 [D loss: (0.245)(R 0.335, F 0.155)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.704] [G acc: 0.000]\n",
      "852 [D loss: (0.244)(R 0.297, F 0.192)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.063] [G acc: 0.125]\n",
      "853 [D loss: (0.327)(R 0.354, F 0.301)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.691] [G acc: 0.125]\n",
      "854 [D loss: (0.315)(R 0.357, F 0.274)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.122] [G acc: 0.125]\n",
      "855 [D loss: (0.258)(R 0.289, F 0.227)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.291] [G acc: 0.125]\n",
      "856 [D loss: (0.266)(R 0.284, F 0.248)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.948] [G acc: 0.125]\n",
      "857 [D loss: (0.195)(R 0.245, F 0.145)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.918] [G acc: 0.062]\n",
      "858 [D loss: (0.568)(R 0.785, F 0.351)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.002] [G acc: 0.375]\n",
      "859 [D loss: (0.312)(R 0.315, F 0.309)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.140] [G acc: 0.062]\n",
      "860 [D loss: (0.246)(R 0.336, F 0.156)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.260] [G acc: 0.125]\n",
      "861 [D loss: (0.355)(R 0.413, F 0.297)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.958] [G acc: 0.125]\n",
      "862 [D loss: (0.615)(R 0.878, F 0.353)] [D acc: (0.812)(0.812, 0.812)] [G loss: 5.868] [G acc: 0.250]\n",
      "863 [D loss: (0.315)(R 0.296, F 0.333)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.256] [G acc: 0.125]\n",
      "864 [D loss: (0.286)(R 0.305, F 0.266)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.387] [G acc: 0.062]\n",
      "865 [D loss: (0.226)(R 0.263, F 0.188)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.879] [G acc: 0.125]\n",
      "866 [D loss: (0.356)(R 0.330, F 0.381)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.851] [G acc: 0.188]\n",
      "867 [D loss: (0.262)(R 0.326, F 0.197)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.937] [G acc: 0.000]\n",
      "868 [D loss: (0.265)(R 0.339, F 0.190)] [D acc: (0.844)(0.812, 0.875)] [G loss: 6.166] [G acc: 0.188]\n",
      "869 [D loss: (0.292)(R 0.242, F 0.343)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.740] [G acc: 0.250]\n",
      "870 [D loss: (0.234)(R 0.238, F 0.229)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.866] [G acc: 0.062]\n",
      "871 [D loss: (0.385)(R 0.456, F 0.313)] [D acc: (0.781)(0.750, 0.812)] [G loss: 8.479] [G acc: 0.062]\n",
      "872 [D loss: (0.364)(R 0.443, F 0.286)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.070] [G acc: 0.188]\n",
      "873 [D loss: (0.385)(R 0.497, F 0.272)] [D acc: (0.844)(0.750, 0.938)] [G loss: 6.947] [G acc: 0.188]\n",
      "874 [D loss: (0.325)(R 0.264, F 0.387)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.835] [G acc: 0.125]\n",
      "875 [D loss: (0.291)(R 0.389, F 0.194)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.040] [G acc: 0.125]\n",
      "876 [D loss: (0.227)(R 0.222, F 0.232)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.180] [G acc: 0.000]\n",
      "877 [D loss: (0.301)(R 0.306, F 0.296)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.538] [G acc: 0.125]\n",
      "878 [D loss: (0.269)(R 0.368, F 0.170)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.793] [G acc: 0.250]\n",
      "879 [D loss: (0.410)(R 0.466, F 0.353)] [D acc: (0.844)(0.812, 0.875)] [G loss: 6.218] [G acc: 0.125]\n",
      "880 [D loss: (0.284)(R 0.306, F 0.263)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.128] [G acc: 0.125]\n",
      "881 [D loss: (0.417)(R 0.429, F 0.405)] [D acc: (0.781)(0.812, 0.750)] [G loss: 5.365] [G acc: 0.188]\n",
      "882 [D loss: (0.377)(R 0.292, F 0.461)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.840] [G acc: 0.125]\n",
      "883 [D loss: (0.279)(R 0.305, F 0.253)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.948] [G acc: 0.062]\n",
      "884 [D loss: (0.589)(R 0.823, F 0.355)] [D acc: (0.781)(0.688, 0.875)] [G loss: 5.088] [G acc: 0.312]\n",
      "885 [D loss: (0.439)(R 0.384, F 0.494)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.751] [G acc: 0.188]\n",
      "886 [D loss: (0.275)(R 0.271, F 0.278)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.135] [G acc: 0.000]\n",
      "887 [D loss: (0.328)(R 0.306, F 0.351)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.146] [G acc: 0.000]\n",
      "888 [D loss: (0.321)(R 0.382, F 0.261)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.076] [G acc: 0.125]\n",
      "889 [D loss: (0.268)(R 0.340, F 0.195)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.175] [G acc: 0.188]\n",
      "890 [D loss: (0.348)(R 0.273, F 0.423)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.682] [G acc: 0.062]\n",
      "891 [D loss: (0.424)(R 0.264, F 0.585)] [D acc: (0.781)(1.000, 0.562)] [G loss: 4.251] [G acc: 0.250]\n",
      "892 [D loss: (0.343)(R 0.328, F 0.358)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.291] [G acc: 0.062]\n",
      "893 [D loss: (0.452)(R 0.291, F 0.612)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.971] [G acc: 0.375]\n",
      "894 [D loss: (0.325)(R 0.422, F 0.228)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.065] [G acc: 0.188]\n",
      "895 [D loss: (0.339)(R 0.372, F 0.305)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.506] [G acc: 0.125]\n",
      "896 [D loss: (0.447)(R 0.483, F 0.410)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.536] [G acc: 0.125]\n",
      "897 [D loss: (0.329)(R 0.383, F 0.275)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.799] [G acc: 0.000]\n",
      "898 [D loss: (0.335)(R 0.291, F 0.380)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.955] [G acc: 0.188]\n",
      "899 [D loss: (0.351)(R 0.477, F 0.226)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.399] [G acc: 0.188]\n",
      "900 [D loss: (0.294)(R 0.325, F 0.263)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.892] [G acc: 0.062]\n",
      "901 [D loss: (0.440)(R 0.603, F 0.278)] [D acc: (0.844)(0.812, 0.875)] [G loss: 5.953] [G acc: 0.062]\n",
      "902 [D loss: (0.229)(R 0.337, F 0.121)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.198] [G acc: 0.188]\n",
      "903 [D loss: (0.244)(R 0.260, F 0.228)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.096] [G acc: 0.188]\n",
      "904 [D loss: (0.320)(R 0.423, F 0.217)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.617] [G acc: 0.250]\n",
      "905 [D loss: (0.299)(R 0.298, F 0.301)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.052] [G acc: 0.062]\n",
      "906 [D loss: (0.324)(R 0.247, F 0.401)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.487] [G acc: 0.062]\n",
      "907 [D loss: (0.374)(R 0.241, F 0.507)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.890] [G acc: 0.312]\n",
      "908 [D loss: (0.491)(R 0.391, F 0.590)] [D acc: (0.719)(0.812, 0.625)] [G loss: 7.337] [G acc: 0.125]\n",
      "909 [D loss: (0.414)(R 0.308, F 0.520)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.576] [G acc: 0.062]\n",
      "910 [D loss: (0.343)(R 0.353, F 0.334)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.412] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911 [D loss: (0.240)(R 0.327, F 0.152)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.885] [G acc: 0.312]\n",
      "912 [D loss: (0.225)(R 0.309, F 0.140)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.095] [G acc: 0.188]\n",
      "913 [D loss: (0.223)(R 0.257, F 0.189)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.324] [G acc: 0.062]\n",
      "914 [D loss: (0.465)(R 0.349, F 0.581)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.634] [G acc: 0.125]\n",
      "915 [D loss: (0.265)(R 0.268, F 0.263)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.026] [G acc: 0.312]\n",
      "916 [D loss: (0.426)(R 0.365, F 0.488)] [D acc: (0.781)(0.875, 0.688)] [G loss: 5.528] [G acc: 0.062]\n",
      "917 [D loss: (0.253)(R 0.183, F 0.323)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.790] [G acc: 0.375]\n",
      "918 [D loss: (0.216)(R 0.225, F 0.208)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.361] [G acc: 0.125]\n",
      "919 [D loss: (0.300)(R 0.316, F 0.285)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.351] [G acc: 0.250]\n",
      "920 [D loss: (0.295)(R 0.354, F 0.236)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.883] [G acc: 0.062]\n",
      "921 [D loss: (0.234)(R 0.260, F 0.207)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.417] [G acc: 0.250]\n",
      "922 [D loss: (0.325)(R 0.292, F 0.358)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.980] [G acc: 0.312]\n",
      "923 [D loss: (0.395)(R 0.407, F 0.383)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.333] [G acc: 0.188]\n",
      "924 [D loss: (0.260)(R 0.159, F 0.361)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.897] [G acc: 0.000]\n",
      "925 [D loss: (0.511)(R 0.303, F 0.720)] [D acc: (0.750)(0.875, 0.625)] [G loss: 7.321] [G acc: 0.062]\n",
      "926 [D loss: (0.350)(R 0.340, F 0.359)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.466] [G acc: 0.125]\n",
      "927 [D loss: (0.362)(R 0.392, F 0.333)] [D acc: (0.812)(0.812, 0.812)] [G loss: 5.061] [G acc: 0.188]\n",
      "928 [D loss: (0.264)(R 0.267, F 0.261)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.014] [G acc: 0.250]\n",
      "929 [D loss: (0.492)(R 0.511, F 0.473)] [D acc: (0.750)(0.750, 0.750)] [G loss: 6.315] [G acc: 0.125]\n",
      "930 [D loss: (0.290)(R 0.305, F 0.275)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.727] [G acc: 0.188]\n",
      "931 [D loss: (0.341)(R 0.303, F 0.379)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.029] [G acc: 0.125]\n",
      "932 [D loss: (0.313)(R 0.278, F 0.348)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.790] [G acc: 0.000]\n",
      "933 [D loss: (0.215)(R 0.234, F 0.197)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.038] [G acc: 0.188]\n",
      "934 [D loss: (0.332)(R 0.309, F 0.355)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.915] [G acc: 0.250]\n",
      "935 [D loss: (0.245)(R 0.316, F 0.173)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.938] [G acc: 0.188]\n",
      "936 [D loss: (0.202)(R 0.248, F 0.156)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.463] [G acc: 0.125]\n",
      "937 [D loss: (0.226)(R 0.289, F 0.162)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.209] [G acc: 0.125]\n",
      "938 [D loss: (0.314)(R 0.298, F 0.330)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.667] [G acc: 0.375]\n",
      "939 [D loss: (0.288)(R 0.217, F 0.359)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.722] [G acc: 0.375]\n",
      "940 [D loss: (0.240)(R 0.174, F 0.306)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.923] [G acc: 0.000]\n",
      "941 [D loss: (0.336)(R 0.236, F 0.436)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.691] [G acc: 0.062]\n",
      "942 [D loss: (0.223)(R 0.214, F 0.231)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.370] [G acc: 0.188]\n",
      "943 [D loss: (0.360)(R 0.222, F 0.497)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.286] [G acc: 0.312]\n",
      "944 [D loss: (0.277)(R 0.394, F 0.160)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.378] [G acc: 0.250]\n",
      "945 [D loss: (0.287)(R 0.329, F 0.246)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.116] [G acc: 0.062]\n",
      "946 [D loss: (0.327)(R 0.337, F 0.317)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.800] [G acc: 0.125]\n",
      "947 [D loss: (0.458)(R 0.313, F 0.603)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.618] [G acc: 0.062]\n",
      "948 [D loss: (0.347)(R 0.199, F 0.496)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.164] [G acc: 0.125]\n",
      "949 [D loss: (0.236)(R 0.280, F 0.193)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.275] [G acc: 0.125]\n",
      "950 [D loss: (0.254)(R 0.211, F 0.297)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.758] [G acc: 0.188]\n",
      "951 [D loss: (0.367)(R 0.327, F 0.407)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.552] [G acc: 0.188]\n",
      "952 [D loss: (0.425)(R 0.287, F 0.563)] [D acc: (0.812)(0.938, 0.688)] [G loss: 7.173] [G acc: 0.000]\n",
      "953 [D loss: (0.463)(R 0.588, F 0.338)] [D acc: (0.844)(0.812, 0.875)] [G loss: 7.790] [G acc: 0.062]\n",
      "954 [D loss: (0.305)(R 0.362, F 0.247)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.417] [G acc: 0.125]\n",
      "955 [D loss: (0.371)(R 0.312, F 0.430)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.415] [G acc: 0.000]\n",
      "956 [D loss: (0.138)(R 0.179, F 0.096)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.144] [G acc: 0.000]\n",
      "957 [D loss: (0.310)(R 0.528, F 0.091)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.116] [G acc: 0.000]\n",
      "958 [D loss: (0.311)(R 0.345, F 0.278)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.090] [G acc: 0.125]\n",
      "959 [D loss: (0.529)(R 0.318, F 0.741)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.265] [G acc: 0.188]\n",
      "960 [D loss: (0.257)(R 0.302, F 0.212)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.577] [G acc: 0.188]\n",
      "961 [D loss: (0.321)(R 0.346, F 0.295)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.993] [G acc: 0.062]\n",
      "962 [D loss: (0.589)(R 0.546, F 0.632)] [D acc: (0.688)(0.750, 0.625)] [G loss: 4.821] [G acc: 0.188]\n",
      "963 [D loss: (0.176)(R 0.246, F 0.106)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.297] [G acc: 0.250]\n",
      "964 [D loss: (0.468)(R 0.686, F 0.251)] [D acc: (0.906)(0.875, 0.938)] [G loss: 9.003] [G acc: 0.062]\n",
      "965 [D loss: (0.205)(R 0.194, F 0.215)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.625] [G acc: 0.125]\n",
      "966 [D loss: (0.246)(R 0.174, F 0.317)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.297] [G acc: 0.125]\n",
      "967 [D loss: (0.389)(R 0.465, F 0.313)] [D acc: (0.812)(0.750, 0.875)] [G loss: 8.085] [G acc: 0.062]\n",
      "968 [D loss: (0.400)(R 0.206, F 0.594)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.098] [G acc: 0.125]\n",
      "969 [D loss: (0.163)(R 0.253, F 0.073)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.304] [G acc: 0.250]\n",
      "970 [D loss: (0.264)(R 0.259, F 0.268)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.163] [G acc: 0.188]\n",
      "971 [D loss: (0.447)(R 0.443, F 0.452)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.965] [G acc: 0.125]\n",
      "972 [D loss: (0.216)(R 0.182, F 0.251)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.243] [G acc: 0.188]\n",
      "973 [D loss: (0.341)(R 0.242, F 0.439)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.155] [G acc: 0.062]\n",
      "974 [D loss: (0.199)(R 0.181, F 0.217)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.755] [G acc: 0.000]\n",
      "975 [D loss: (0.856)(R 1.300, F 0.412)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.536] [G acc: 0.125]\n",
      "976 [D loss: (0.345)(R 0.300, F 0.389)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.107] [G acc: 0.250]\n",
      "977 [D loss: (0.369)(R 0.432, F 0.306)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.933] [G acc: 0.125]\n",
      "978 [D loss: (0.305)(R 0.162, F 0.448)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.283] [G acc: 0.062]\n",
      "979 [D loss: (0.367)(R 0.350, F 0.383)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.423] [G acc: 0.250]\n",
      "980 [D loss: (0.320)(R 0.303, F 0.338)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.275] [G acc: 0.125]\n",
      "981 [D loss: (0.417)(R 0.288, F 0.546)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.157] [G acc: 0.000]\n",
      "982 [D loss: (0.241)(R 0.228, F 0.254)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.595] [G acc: 0.062]\n",
      "983 [D loss: (0.449)(R 0.342, F 0.556)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.452] [G acc: 0.062]\n",
      "984 [D loss: (0.193)(R 0.255, F 0.132)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.863] [G acc: 0.188]\n",
      "985 [D loss: (0.362)(R 0.391, F 0.333)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.520] [G acc: 0.188]\n",
      "986 [D loss: (0.346)(R 0.307, F 0.386)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.533] [G acc: 0.062]\n",
      "987 [D loss: (0.331)(R 0.203, F 0.459)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.446] [G acc: 0.062]\n",
      "988 [D loss: (0.185)(R 0.251, F 0.119)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.130] [G acc: 0.000]\n",
      "989 [D loss: (0.297)(R 0.261, F 0.333)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.268] [G acc: 0.125]\n",
      "990 [D loss: (0.170)(R 0.224, F 0.116)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.898] [G acc: 0.062]\n",
      "991 [D loss: (0.420)(R 0.303, F 0.536)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.350] [G acc: 0.125]\n",
      "992 [D loss: (0.340)(R 0.289, F 0.391)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.739] [G acc: 0.062]\n",
      "993 [D loss: (0.174)(R 0.201, F 0.148)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.092] [G acc: 0.000]\n",
      "994 [D loss: (0.345)(R 0.242, F 0.448)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.639] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995 [D loss: (0.397)(R 0.355, F 0.439)] [D acc: (0.812)(0.812, 0.812)] [G loss: 6.834] [G acc: 0.125]\n",
      "996 [D loss: (0.150)(R 0.153, F 0.147)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.272] [G acc: 0.000]\n",
      "997 [D loss: (0.279)(R 0.301, F 0.256)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.719] [G acc: 0.125]\n",
      "998 [D loss: (0.288)(R 0.227, F 0.349)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.179] [G acc: 0.125]\n",
      "999 [D loss: (0.322)(R 0.353, F 0.292)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.596] [G acc: 0.125]\n",
      "1000 [D loss: (0.267)(R 0.243, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.823] [G acc: 0.125]\n",
      "1001 [D loss: (0.447)(R 0.466, F 0.427)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.694] [G acc: 0.188]\n",
      "1002 [D loss: (0.475)(R 0.639, F 0.311)] [D acc: (0.781)(0.750, 0.812)] [G loss: 5.325] [G acc: 0.125]\n",
      "1003 [D loss: (0.295)(R 0.246, F 0.344)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.942] [G acc: 0.062]\n",
      "1004 [D loss: (0.200)(R 0.177, F 0.223)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.340] [G acc: 0.125]\n",
      "1005 [D loss: (0.372)(R 0.551, F 0.192)] [D acc: (0.875)(0.812, 0.938)] [G loss: 5.407] [G acc: 0.125]\n",
      "1006 [D loss: (0.417)(R 0.294, F 0.540)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.533] [G acc: 0.062]\n",
      "1007 [D loss: (0.345)(R 0.224, F 0.467)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.952] [G acc: 0.125]\n",
      "1008 [D loss: (0.429)(R 0.710, F 0.147)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.018] [G acc: 0.062]\n",
      "1009 [D loss: (0.472)(R 0.397, F 0.547)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.496] [G acc: 0.062]\n",
      "1010 [D loss: (0.269)(R 0.173, F 0.364)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.016] [G acc: 0.188]\n",
      "1011 [D loss: (0.413)(R 0.360, F 0.466)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.804] [G acc: 0.125]\n",
      "1012 [D loss: (0.234)(R 0.232, F 0.236)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.010] [G acc: 0.125]\n",
      "1013 [D loss: (0.313)(R 0.385, F 0.240)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.723] [G acc: 0.125]\n",
      "1014 [D loss: (0.297)(R 0.301, F 0.293)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.086] [G acc: 0.062]\n",
      "1015 [D loss: (0.315)(R 0.327, F 0.302)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.796] [G acc: 0.188]\n",
      "1016 [D loss: (0.210)(R 0.211, F 0.209)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.308] [G acc: 0.000]\n",
      "1017 [D loss: (0.288)(R 0.358, F 0.218)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.735] [G acc: 0.125]\n",
      "1018 [D loss: (0.219)(R 0.188, F 0.249)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.426] [G acc: 0.062]\n",
      "1019 [D loss: (0.227)(R 0.277, F 0.177)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.882] [G acc: 0.188]\n",
      "1020 [D loss: (0.307)(R 0.314, F 0.300)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.873] [G acc: 0.125]\n",
      "1021 [D loss: (0.252)(R 0.236, F 0.268)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.123] [G acc: 0.125]\n",
      "1022 [D loss: (0.165)(R 0.149, F 0.180)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.563] [G acc: 0.188]\n",
      "1023 [D loss: (0.285)(R 0.302, F 0.268)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.860] [G acc: 0.250]\n",
      "1024 [D loss: (0.265)(R 0.246, F 0.285)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.446] [G acc: 0.125]\n",
      "1025 [D loss: (0.309)(R 0.338, F 0.280)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.049] [G acc: 0.250]\n",
      "1026 [D loss: (0.341)(R 0.244, F 0.438)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.761] [G acc: 0.188]\n",
      "1027 [D loss: (0.249)(R 0.282, F 0.215)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.690] [G acc: 0.125]\n",
      "1028 [D loss: (0.254)(R 0.234, F 0.273)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.910] [G acc: 0.250]\n",
      "1029 [D loss: (0.160)(R 0.171, F 0.148)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.652] [G acc: 0.188]\n",
      "1030 [D loss: (0.382)(R 0.202, F 0.561)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.363] [G acc: 0.188]\n",
      "1031 [D loss: (0.318)(R 0.244, F 0.392)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.271] [G acc: 0.188]\n",
      "1032 [D loss: (0.168)(R 0.176, F 0.160)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.094] [G acc: 0.188]\n",
      "1033 [D loss: (0.154)(R 0.172, F 0.136)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.240] [G acc: 0.188]\n",
      "1034 [D loss: (0.182)(R 0.231, F 0.133)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.507] [G acc: 0.188]\n",
      "1035 [D loss: (0.366)(R 0.275, F 0.457)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.601] [G acc: 0.062]\n",
      "1036 [D loss: (0.366)(R 0.427, F 0.305)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.622] [G acc: 0.188]\n",
      "1037 [D loss: (0.373)(R 0.574, F 0.171)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.986] [G acc: 0.062]\n",
      "1038 [D loss: (0.280)(R 0.207, F 0.353)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.303] [G acc: 0.062]\n",
      "1039 [D loss: (0.142)(R 0.198, F 0.087)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.956] [G acc: 0.062]\n",
      "1040 [D loss: (0.351)(R 0.373, F 0.330)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.261] [G acc: 0.062]\n",
      "1041 [D loss: (0.247)(R 0.255, F 0.240)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.657] [G acc: 0.125]\n",
      "1042 [D loss: (0.424)(R 0.176, F 0.672)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.406] [G acc: 0.188]\n",
      "1043 [D loss: (0.418)(R 0.225, F 0.612)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.436] [G acc: 0.062]\n",
      "1044 [D loss: (0.172)(R 0.289, F 0.054)] [D acc: (0.969)(0.938, 1.000)] [G loss: 10.061] [G acc: 0.000]\n",
      "1045 [D loss: (0.188)(R 0.307, F 0.070)] [D acc: (0.938)(0.875, 1.000)] [G loss: 8.120] [G acc: 0.000]\n",
      "1046 [D loss: (0.166)(R 0.173, F 0.160)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.535] [G acc: 0.000]\n",
      "1047 [D loss: (0.198)(R 0.246, F 0.149)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.492] [G acc: 0.125]\n",
      "1048 [D loss: (0.189)(R 0.130, F 0.248)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.054] [G acc: 0.125]\n",
      "1049 [D loss: (0.191)(R 0.151, F 0.231)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.939] [G acc: 0.062]\n",
      "1050 [D loss: (0.268)(R 0.333, F 0.202)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.515] [G acc: 0.062]\n",
      "1051 [D loss: (0.201)(R 0.215, F 0.188)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.234] [G acc: 0.000]\n",
      "1052 [D loss: (0.297)(R 0.225, F 0.370)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.501] [G acc: 0.188]\n",
      "1053 [D loss: (0.193)(R 0.137, F 0.249)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.625] [G acc: 0.125]\n",
      "1054 [D loss: (0.282)(R 0.297, F 0.267)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.995] [G acc: 0.000]\n",
      "1055 [D loss: (0.280)(R 0.142, F 0.418)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.921] [G acc: 0.062]\n",
      "1056 [D loss: (0.272)(R 0.205, F 0.338)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.713] [G acc: 0.125]\n",
      "1057 [D loss: (0.133)(R 0.163, F 0.102)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.441] [G acc: 0.062]\n",
      "1058 [D loss: (0.132)(R 0.161, F 0.102)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.569] [G acc: 0.125]\n",
      "1059 [D loss: (0.142)(R 0.245, F 0.040)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.790] [G acc: 0.062]\n",
      "1060 [D loss: (0.568)(R 0.851, F 0.285)] [D acc: (0.812)(0.750, 0.875)] [G loss: 8.487] [G acc: 0.000]\n",
      "1061 [D loss: (0.220)(R 0.267, F 0.173)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.555] [G acc: 0.125]\n",
      "1062 [D loss: (0.299)(R 0.418, F 0.180)] [D acc: (0.906)(0.875, 0.938)] [G loss: 9.286] [G acc: 0.062]\n",
      "1063 [D loss: (0.292)(R 0.342, F 0.242)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.514] [G acc: 0.250]\n",
      "1064 [D loss: (0.353)(R 0.228, F 0.479)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.757] [G acc: 0.000]\n",
      "1065 [D loss: (0.461)(R 0.167, F 0.755)] [D acc: (0.812)(1.000, 0.625)] [G loss: 9.223] [G acc: 0.062]\n",
      "1066 [D loss: (0.229)(R 0.240, F 0.218)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.658] [G acc: 0.188]\n",
      "1067 [D loss: (0.218)(R 0.168, F 0.267)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.129] [G acc: 0.188]\n",
      "1068 [D loss: (0.408)(R 0.270, F 0.545)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.554] [G acc: 0.125]\n",
      "1069 [D loss: (0.197)(R 0.159, F 0.236)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.119] [G acc: 0.188]\n",
      "1070 [D loss: (0.358)(R 0.457, F 0.260)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.728] [G acc: 0.188]\n",
      "1071 [D loss: (0.195)(R 0.275, F 0.116)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.525] [G acc: 0.000]\n",
      "1072 [D loss: (0.322)(R 0.437, F 0.207)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.604] [G acc: 0.062]\n",
      "1073 [D loss: (0.309)(R 0.204, F 0.415)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.371] [G acc: 0.125]\n",
      "1074 [D loss: (0.213)(R 0.234, F 0.193)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.405] [G acc: 0.000]\n",
      "1075 [D loss: (0.297)(R 0.187, F 0.408)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.908] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1076 [D loss: (0.243)(R 0.429, F 0.056)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.162] [G acc: 0.062]\n",
      "1077 [D loss: (0.202)(R 0.185, F 0.218)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.135] [G acc: 0.188]\n",
      "1078 [D loss: (0.314)(R 0.117, F 0.512)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.667] [G acc: 0.062]\n",
      "1079 [D loss: (0.292)(R 0.482, F 0.102)] [D acc: (0.906)(0.812, 1.000)] [G loss: 6.287] [G acc: 0.125]\n",
      "1080 [D loss: (0.242)(R 0.175, F 0.308)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.777] [G acc: 0.062]\n",
      "1081 [D loss: (0.239)(R 0.265, F 0.214)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.839] [G acc: 0.062]\n",
      "1082 [D loss: (0.213)(R 0.184, F 0.241)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.465] [G acc: 0.062]\n",
      "1083 [D loss: (0.247)(R 0.381, F 0.113)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.217] [G acc: 0.062]\n",
      "1084 [D loss: (0.668)(R 0.405, F 0.930)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.314] [G acc: 0.062]\n",
      "1085 [D loss: (0.235)(R 0.236, F 0.235)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.044] [G acc: 0.000]\n",
      "1086 [D loss: (0.230)(R 0.224, F 0.236)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.848] [G acc: 0.188]\n",
      "1087 [D loss: (0.114)(R 0.133, F 0.095)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.240] [G acc: 0.062]\n",
      "1088 [D loss: (0.119)(R 0.184, F 0.053)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.965] [G acc: 0.062]\n",
      "1089 [D loss: (0.323)(R 0.357, F 0.289)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.835] [G acc: 0.188]\n",
      "1090 [D loss: (0.308)(R 0.231, F 0.386)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.404] [G acc: 0.062]\n",
      "1091 [D loss: (0.545)(R 0.384, F 0.706)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.347] [G acc: 0.062]\n",
      "1092 [D loss: (0.420)(R 0.228, F 0.612)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.569] [G acc: 0.000]\n",
      "1093 [D loss: (0.208)(R 0.161, F 0.255)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.669] [G acc: 0.125]\n",
      "1094 [D loss: (0.279)(R 0.424, F 0.134)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.474] [G acc: 0.125]\n",
      "1095 [D loss: (0.242)(R 0.185, F 0.298)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.295] [G acc: 0.125]\n",
      "1096 [D loss: (0.317)(R 0.469, F 0.165)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.353] [G acc: 0.375]\n",
      "1097 [D loss: (0.134)(R 0.178, F 0.090)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.946] [G acc: 0.000]\n",
      "1098 [D loss: (0.211)(R 0.151, F 0.271)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.233] [G acc: 0.125]\n",
      "1099 [D loss: (0.512)(R 0.743, F 0.281)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.238] [G acc: 0.188]\n",
      "1100 [D loss: (0.262)(R 0.188, F 0.335)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.298] [G acc: 0.188]\n",
      "1101 [D loss: (0.385)(R 0.282, F 0.487)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.799] [G acc: 0.125]\n",
      "1102 [D loss: (0.428)(R 0.502, F 0.354)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.709] [G acc: 0.188]\n",
      "1103 [D loss: (0.226)(R 0.166, F 0.285)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.841] [G acc: 0.188]\n",
      "1104 [D loss: (0.195)(R 0.363, F 0.027)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.492] [G acc: 0.062]\n",
      "1105 [D loss: (0.233)(R 0.153, F 0.314)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.171] [G acc: 0.188]\n",
      "1106 [D loss: (0.263)(R 0.256, F 0.270)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.251] [G acc: 0.125]\n",
      "1107 [D loss: (0.303)(R 0.167, F 0.439)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.286] [G acc: 0.000]\n",
      "1108 [D loss: (0.118)(R 0.193, F 0.042)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.442] [G acc: 0.000]\n",
      "1109 [D loss: (0.254)(R 0.124, F 0.384)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.357] [G acc: 0.125]\n",
      "1110 [D loss: (0.221)(R 0.141, F 0.301)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.312] [G acc: 0.125]\n",
      "1111 [D loss: (0.168)(R 0.162, F 0.175)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.094] [G acc: 0.125]\n",
      "1112 [D loss: (0.242)(R 0.142, F 0.341)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.459] [G acc: 0.062]\n",
      "1113 [D loss: (0.309)(R 0.278, F 0.341)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.923] [G acc: 0.125]\n",
      "1114 [D loss: (0.133)(R 0.123, F 0.144)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.573] [G acc: 0.125]\n",
      "1115 [D loss: (0.331)(R 0.478, F 0.185)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.202] [G acc: 0.000]\n",
      "1116 [D loss: (0.293)(R 0.124, F 0.463)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.912] [G acc: 0.188]\n",
      "1117 [D loss: (0.217)(R 0.184, F 0.250)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.447] [G acc: 0.000]\n",
      "1118 [D loss: (0.330)(R 0.446, F 0.213)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.620] [G acc: 0.062]\n",
      "1119 [D loss: (0.122)(R 0.133, F 0.111)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.353] [G acc: 0.188]\n",
      "1120 [D loss: (0.196)(R 0.126, F 0.265)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.025] [G acc: 0.188]\n",
      "1121 [D loss: (0.182)(R 0.233, F 0.130)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.418] [G acc: 0.188]\n",
      "1122 [D loss: (0.298)(R 0.296, F 0.301)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.603] [G acc: 0.062]\n",
      "1123 [D loss: (0.281)(R 0.176, F 0.386)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.444] [G acc: 0.000]\n",
      "1124 [D loss: (0.221)(R 0.206, F 0.235)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.027] [G acc: 0.000]\n",
      "1125 [D loss: (0.104)(R 0.161, F 0.047)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.845] [G acc: 0.125]\n",
      "1126 [D loss: (0.608)(R 1.089, F 0.127)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.120] [G acc: 0.062]\n",
      "1127 [D loss: (0.215)(R 0.140, F 0.290)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.828] [G acc: 0.062]\n",
      "1128 [D loss: (0.269)(R 0.351, F 0.188)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.178] [G acc: 0.062]\n",
      "1129 [D loss: (0.219)(R 0.114, F 0.324)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.530] [G acc: 0.188]\n",
      "1130 [D loss: (0.183)(R 0.140, F 0.225)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.408] [G acc: 0.062]\n",
      "1131 [D loss: (0.302)(R 0.334, F 0.269)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.777] [G acc: 0.000]\n",
      "1132 [D loss: (0.108)(R 0.113, F 0.102)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.819] [G acc: 0.000]\n",
      "1133 [D loss: (0.218)(R 0.406, F 0.030)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.856] [G acc: 0.062]\n",
      "1134 [D loss: (0.399)(R 0.328, F 0.470)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.570] [G acc: 0.000]\n",
      "1135 [D loss: (0.157)(R 0.195, F 0.120)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.766] [G acc: 0.000]\n",
      "1136 [D loss: (0.101)(R 0.126, F 0.077)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.356] [G acc: 0.062]\n",
      "1137 [D loss: (0.189)(R 0.300, F 0.078)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.752] [G acc: 0.125]\n",
      "1138 [D loss: (0.180)(R 0.226, F 0.134)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.209] [G acc: 0.000]\n",
      "1139 [D loss: (0.214)(R 0.345, F 0.083)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.306] [G acc: 0.125]\n",
      "1140 [D loss: (0.103)(R 0.124, F 0.082)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.118] [G acc: 0.062]\n",
      "1141 [D loss: (0.259)(R 0.311, F 0.206)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.995] [G acc: 0.125]\n",
      "1142 [D loss: (0.184)(R 0.160, F 0.208)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.335] [G acc: 0.000]\n",
      "1143 [D loss: (0.300)(R 0.096, F 0.504)] [D acc: (0.875)(1.000, 0.750)] [G loss: 9.003] [G acc: 0.000]\n",
      "1144 [D loss: (0.175)(R 0.297, F 0.052)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.623] [G acc: 0.062]\n",
      "1145 [D loss: (0.276)(R 0.139, F 0.414)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.368] [G acc: 0.188]\n",
      "1146 [D loss: (0.185)(R 0.219, F 0.152)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.162] [G acc: 0.125]\n",
      "1147 [D loss: (0.219)(R 0.095, F 0.344)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.126] [G acc: 0.188]\n",
      "1148 [D loss: (0.312)(R 0.349, F 0.275)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.342] [G acc: 0.000]\n",
      "1149 [D loss: (0.143)(R 0.098, F 0.187)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.616] [G acc: 0.000]\n",
      "1150 [D loss: (0.199)(R 0.298, F 0.100)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.005] [G acc: 0.062]\n",
      "1151 [D loss: (0.433)(R 0.459, F 0.407)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.648] [G acc: 0.062]\n",
      "1152 [D loss: (0.254)(R 0.142, F 0.366)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.171] [G acc: 0.062]\n",
      "1153 [D loss: (0.173)(R 0.134, F 0.212)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.516] [G acc: 0.000]\n",
      "1154 [D loss: (0.046)(R 0.074, F 0.017)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.933] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155 [D loss: (0.138)(R 0.223, F 0.054)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.154] [G acc: 0.312]\n",
      "1156 [D loss: (0.172)(R 0.115, F 0.229)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.439] [G acc: 0.000]\n",
      "1157 [D loss: (0.304)(R 0.376, F 0.232)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.394] [G acc: 0.125]\n",
      "1158 [D loss: (0.214)(R 0.178, F 0.250)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.937] [G acc: 0.125]\n",
      "1159 [D loss: (0.719)(R 1.143, F 0.295)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.695] [G acc: 0.125]\n",
      "1160 [D loss: (0.103)(R 0.099, F 0.107)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.718] [G acc: 0.125]\n",
      "1161 [D loss: (0.134)(R 0.261, F 0.006)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.826] [G acc: 0.125]\n",
      "1162 [D loss: (0.131)(R 0.166, F 0.095)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.309] [G acc: 0.000]\n",
      "1163 [D loss: (0.061)(R 0.102, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.246] [G acc: 0.188]\n",
      "1164 [D loss: (0.361)(R 0.438, F 0.284)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.335] [G acc: 0.125]\n",
      "1165 [D loss: (0.284)(R 0.199, F 0.369)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.864] [G acc: 0.000]\n",
      "1166 [D loss: (0.421)(R 0.232, F 0.610)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.958] [G acc: 0.250]\n",
      "1167 [D loss: (0.313)(R 0.126, F 0.501)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.083] [G acc: 0.062]\n",
      "1168 [D loss: (0.200)(R 0.192, F 0.207)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.797] [G acc: 0.250]\n",
      "1169 [D loss: (0.205)(R 0.196, F 0.215)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.244] [G acc: 0.062]\n",
      "1170 [D loss: (0.435)(R 0.530, F 0.339)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.786] [G acc: 0.062]\n",
      "1171 [D loss: (0.173)(R 0.144, F 0.202)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.634] [G acc: 0.000]\n",
      "1172 [D loss: (0.184)(R 0.136, F 0.232)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.880] [G acc: 0.062]\n",
      "1173 [D loss: (0.112)(R 0.126, F 0.099)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.808] [G acc: 0.125]\n",
      "1174 [D loss: (0.193)(R 0.352, F 0.035)] [D acc: (0.906)(0.812, 1.000)] [G loss: 6.844] [G acc: 0.188]\n",
      "1175 [D loss: (0.149)(R 0.232, F 0.067)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.160] [G acc: 0.062]\n",
      "1176 [D loss: (0.185)(R 0.116, F 0.254)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.989] [G acc: 0.125]\n",
      "1177 [D loss: (0.163)(R 0.119, F 0.207)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.026] [G acc: 0.125]\n",
      "1178 [D loss: (0.274)(R 0.255, F 0.294)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.507] [G acc: 0.188]\n",
      "1179 [D loss: (0.286)(R 0.475, F 0.096)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.959] [G acc: 0.125]\n",
      "1180 [D loss: (0.168)(R 0.112, F 0.223)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.609] [G acc: 0.062]\n",
      "1181 [D loss: (0.199)(R 0.116, F 0.282)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.900] [G acc: 0.062]\n",
      "1182 [D loss: (0.473)(R 0.585, F 0.360)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.434] [G acc: 0.000]\n",
      "1183 [D loss: (0.377)(R 0.346, F 0.408)] [D acc: (0.844)(0.875, 0.812)] [G loss: 9.060] [G acc: 0.000]\n",
      "1184 [D loss: (0.165)(R 0.149, F 0.180)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.542] [G acc: 0.125]\n",
      "1185 [D loss: (0.192)(R 0.297, F 0.086)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.838] [G acc: 0.000]\n",
      "1186 [D loss: (0.225)(R 0.188, F 0.261)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.626] [G acc: 0.062]\n",
      "1187 [D loss: (0.186)(R 0.243, F 0.129)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.138] [G acc: 0.250]\n",
      "1188 [D loss: (0.271)(R 0.191, F 0.351)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.125] [G acc: 0.125]\n",
      "1189 [D loss: (0.476)(R 0.433, F 0.519)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.677] [G acc: 0.188]\n",
      "1190 [D loss: (0.168)(R 0.149, F 0.188)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.932] [G acc: 0.125]\n",
      "1191 [D loss: (0.159)(R 0.188, F 0.130)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.440] [G acc: 0.062]\n",
      "1192 [D loss: (0.182)(R 0.174, F 0.191)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.724] [G acc: 0.125]\n",
      "1193 [D loss: (0.421)(R 0.546, F 0.296)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.065] [G acc: 0.062]\n",
      "1194 [D loss: (0.421)(R 0.462, F 0.379)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.181] [G acc: 0.188]\n",
      "1195 [D loss: (0.152)(R 0.106, F 0.199)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.600] [G acc: 0.062]\n",
      "1196 [D loss: (0.071)(R 0.093, F 0.049)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.611] [G acc: 0.000]\n",
      "1197 [D loss: (0.278)(R 0.376, F 0.179)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.939] [G acc: 0.125]\n",
      "1198 [D loss: (0.282)(R 0.096, F 0.468)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.944] [G acc: 0.062]\n",
      "1199 [D loss: (0.343)(R 0.483, F 0.203)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.744] [G acc: 0.000]\n",
      "1200 [D loss: (0.200)(R 0.131, F 0.270)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.826] [G acc: 0.062]\n",
      "1201 [D loss: (0.375)(R 0.377, F 0.373)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.379] [G acc: 0.062]\n",
      "1202 [D loss: (0.342)(R 0.434, F 0.250)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.339] [G acc: 0.062]\n",
      "1203 [D loss: (0.177)(R 0.256, F 0.097)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.062] [G acc: 0.062]\n",
      "1204 [D loss: (0.293)(R 0.367, F 0.220)] [D acc: (0.875)(0.875, 0.875)] [G loss: 9.423] [G acc: 0.000]\n",
      "1205 [D loss: (0.348)(R 0.348, F 0.348)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.225] [G acc: 0.000]\n",
      "1206 [D loss: (0.181)(R 0.078, F 0.284)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.561] [G acc: 0.062]\n",
      "1207 [D loss: (0.202)(R 0.166, F 0.239)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.702] [G acc: 0.125]\n",
      "1208 [D loss: (0.253)(R 0.176, F 0.331)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.474] [G acc: 0.125]\n",
      "1209 [D loss: (0.128)(R 0.122, F 0.134)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.609] [G acc: 0.188]\n",
      "1210 [D loss: (0.325)(R 0.445, F 0.205)] [D acc: (0.875)(0.812, 0.938)] [G loss: 5.731] [G acc: 0.188]\n",
      "1211 [D loss: (0.404)(R 0.437, F 0.371)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.393] [G acc: 0.062]\n",
      "1212 [D loss: (0.167)(R 0.122, F 0.213)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.877] [G acc: 0.125]\n",
      "1213 [D loss: (0.305)(R 0.436, F 0.173)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.749] [G acc: 0.125]\n",
      "1214 [D loss: (0.094)(R 0.109, F 0.078)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.805] [G acc: 0.000]\n",
      "1215 [D loss: (0.158)(R 0.158, F 0.159)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.760] [G acc: 0.125]\n",
      "1216 [D loss: (0.147)(R 0.146, F 0.148)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.032] [G acc: 0.062]\n",
      "1217 [D loss: (0.447)(R 0.467, F 0.427)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.735] [G acc: 0.062]\n",
      "1218 [D loss: (0.120)(R 0.212, F 0.028)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.054] [G acc: 0.062]\n",
      "1219 [D loss: (0.144)(R 0.098, F 0.189)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.408] [G acc: 0.062]\n",
      "1220 [D loss: (0.106)(R 0.155, F 0.057)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.553] [G acc: 0.062]\n",
      "1221 [D loss: (0.152)(R 0.115, F 0.189)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.406] [G acc: 0.000]\n",
      "1222 [D loss: (0.244)(R 0.236, F 0.252)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.539] [G acc: 0.062]\n",
      "1223 [D loss: (0.325)(R 0.096, F 0.554)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.878] [G acc: 0.125]\n",
      "1224 [D loss: (0.192)(R 0.135, F 0.250)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.990] [G acc: 0.000]\n",
      "1225 [D loss: (0.254)(R 0.174, F 0.335)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.608] [G acc: 0.000]\n",
      "1226 [D loss: (0.154)(R 0.284, F 0.024)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.206] [G acc: 0.125]\n",
      "1227 [D loss: (0.109)(R 0.122, F 0.096)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.149] [G acc: 0.062]\n",
      "1228 [D loss: (0.424)(R 0.536, F 0.312)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.489] [G acc: 0.062]\n",
      "1229 [D loss: (0.115)(R 0.104, F 0.126)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.838] [G acc: 0.062]\n",
      "1230 [D loss: (0.231)(R 0.268, F 0.194)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.939] [G acc: 0.188]\n",
      "1231 [D loss: (0.191)(R 0.142, F 0.241)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.870] [G acc: 0.188]\n",
      "1232 [D loss: (0.370)(R 0.731, F 0.009)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.866] [G acc: 0.000]\n",
      "1233 [D loss: (0.212)(R 0.129, F 0.296)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.713] [G acc: 0.062]\n",
      "1234 [D loss: (0.530)(R 1.037, F 0.022)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.533] [G acc: 0.000]\n",
      "1235 [D loss: (0.137)(R 0.254, F 0.020)] [D acc: (0.938)(0.875, 1.000)] [G loss: 8.838] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236 [D loss: (0.130)(R 0.189, F 0.071)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.446] [G acc: 0.188]\n",
      "1237 [D loss: (0.300)(R 0.182, F 0.418)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.412] [G acc: 0.062]\n",
      "1238 [D loss: (0.179)(R 0.077, F 0.281)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.437] [G acc: 0.062]\n",
      "1239 [D loss: (0.105)(R 0.089, F 0.121)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.587] [G acc: 0.000]\n",
      "1240 [D loss: (0.150)(R 0.127, F 0.174)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.150] [G acc: 0.000]\n",
      "1241 [D loss: (0.147)(R 0.152, F 0.141)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.214] [G acc: 0.062]\n",
      "1242 [D loss: (0.188)(R 0.127, F 0.249)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.876] [G acc: 0.062]\n",
      "1243 [D loss: (0.153)(R 0.162, F 0.144)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.505] [G acc: 0.000]\n",
      "1244 [D loss: (0.140)(R 0.103, F 0.177)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.634] [G acc: 0.000]\n",
      "1245 [D loss: (0.437)(R 0.163, F 0.712)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.905] [G acc: 0.250]\n",
      "1246 [D loss: (0.199)(R 0.119, F 0.278)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.150] [G acc: 0.000]\n",
      "1247 [D loss: (0.101)(R 0.100, F 0.101)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.151] [G acc: 0.062]\n",
      "1248 [D loss: (0.270)(R 0.239, F 0.300)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.776] [G acc: 0.000]\n",
      "1249 [D loss: (0.080)(R 0.126, F 0.034)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.545] [G acc: 0.000]\n",
      "1250 [D loss: (0.227)(R 0.167, F 0.287)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.480] [G acc: 0.062]\n",
      "1251 [D loss: (0.135)(R 0.110, F 0.161)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.048] [G acc: 0.062]\n",
      "1252 [D loss: (0.109)(R 0.086, F 0.132)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.570] [G acc: 0.125]\n",
      "1253 [D loss: (0.305)(R 0.084, F 0.527)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.455] [G acc: 0.062]\n",
      "1254 [D loss: (0.356)(R 0.675, F 0.038)] [D acc: (0.875)(0.750, 1.000)] [G loss: 7.238] [G acc: 0.062]\n",
      "1255 [D loss: (0.263)(R 0.167, F 0.360)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.653] [G acc: 0.062]\n",
      "1256 [D loss: (0.564)(R 0.277, F 0.852)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.989] [G acc: 0.000]\n",
      "1257 [D loss: (0.202)(R 0.114, F 0.290)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.996] [G acc: 0.125]\n",
      "1258 [D loss: (0.146)(R 0.166, F 0.126)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.250] [G acc: 0.000]\n",
      "1259 [D loss: (0.155)(R 0.218, F 0.091)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.398] [G acc: 0.250]\n",
      "1260 [D loss: (0.145)(R 0.100, F 0.189)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.471] [G acc: 0.062]\n",
      "1261 [D loss: (0.365)(R 0.078, F 0.652)] [D acc: (0.844)(1.000, 0.688)] [G loss: 5.736] [G acc: 0.125]\n",
      "1262 [D loss: (0.208)(R 0.081, F 0.335)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.891] [G acc: 0.062]\n",
      "1263 [D loss: (0.057)(R 0.100, F 0.014)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.007] [G acc: 0.125]\n",
      "1264 [D loss: (0.342)(R 0.263, F 0.420)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.197] [G acc: 0.062]\n",
      "1265 [D loss: (0.309)(R 0.280, F 0.337)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.032] [G acc: 0.125]\n",
      "1266 [D loss: (0.130)(R 0.104, F 0.157)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.223] [G acc: 0.062]\n",
      "1267 [D loss: (0.245)(R 0.222, F 0.268)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.649] [G acc: 0.062]\n",
      "1268 [D loss: (0.270)(R 0.241, F 0.300)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.778] [G acc: 0.062]\n",
      "1269 [D loss: (0.384)(R 0.573, F 0.195)] [D acc: (0.875)(0.812, 0.938)] [G loss: 9.237] [G acc: 0.000]\n",
      "1270 [D loss: (0.239)(R 0.384, F 0.095)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.588] [G acc: 0.000]\n",
      "1271 [D loss: (0.197)(R 0.143, F 0.251)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.173] [G acc: 0.188]\n",
      "1272 [D loss: (0.223)(R 0.091, F 0.356)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.561] [G acc: 0.000]\n",
      "1273 [D loss: (0.325)(R 0.148, F 0.501)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.211] [G acc: 0.062]\n",
      "1274 [D loss: (0.090)(R 0.145, F 0.034)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.253] [G acc: 0.062]\n",
      "1275 [D loss: (0.190)(R 0.148, F 0.231)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.393] [G acc: 0.062]\n",
      "1276 [D loss: (0.078)(R 0.135, F 0.021)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.922] [G acc: 0.062]\n",
      "1277 [D loss: (0.355)(R 0.160, F 0.550)] [D acc: (0.906)(1.000, 0.812)] [G loss: 10.579] [G acc: 0.000]\n",
      "1278 [D loss: (0.079)(R 0.092, F 0.066)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.105] [G acc: 0.000]\n",
      "1279 [D loss: (0.112)(R 0.128, F 0.096)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.554] [G acc: 0.000]\n",
      "1280 [D loss: (0.199)(R 0.317, F 0.080)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.467] [G acc: 0.000]\n",
      "1281 [D loss: (0.345)(R 0.344, F 0.346)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.249] [G acc: 0.062]\n",
      "1282 [D loss: (0.251)(R 0.110, F 0.391)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.670] [G acc: 0.062]\n",
      "1283 [D loss: (0.203)(R 0.073, F 0.332)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.175] [G acc: 0.125]\n",
      "1284 [D loss: (0.339)(R 0.350, F 0.328)] [D acc: (0.812)(0.812, 0.812)] [G loss: 7.169] [G acc: 0.062]\n",
      "1285 [D loss: (0.272)(R 0.097, F 0.446)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.082] [G acc: 0.125]\n",
      "1286 [D loss: (0.426)(R 0.494, F 0.358)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.334] [G acc: 0.125]\n",
      "1287 [D loss: (0.268)(R 0.139, F 0.397)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.660] [G acc: 0.062]\n",
      "1288 [D loss: (0.173)(R 0.103, F 0.243)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.956] [G acc: 0.062]\n",
      "1289 [D loss: (0.273)(R 0.255, F 0.291)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.384] [G acc: 0.125]\n",
      "1290 [D loss: (0.385)(R 0.642, F 0.129)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.945] [G acc: 0.062]\n",
      "1291 [D loss: (0.376)(R 0.455, F 0.297)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.097] [G acc: 0.125]\n",
      "1292 [D loss: (0.131)(R 0.122, F 0.141)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.477] [G acc: 0.062]\n",
      "1293 [D loss: (0.184)(R 0.221, F 0.147)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.436] [G acc: 0.062]\n",
      "1294 [D loss: (0.260)(R 0.083, F 0.438)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.626] [G acc: 0.125]\n",
      "1295 [D loss: (0.190)(R 0.075, F 0.306)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.894] [G acc: 0.062]\n",
      "1296 [D loss: (0.264)(R 0.197, F 0.330)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.896] [G acc: 0.125]\n",
      "1297 [D loss: (0.080)(R 0.122, F 0.038)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.089] [G acc: 0.062]\n",
      "1298 [D loss: (0.336)(R 0.503, F 0.168)] [D acc: (0.875)(0.812, 0.938)] [G loss: 7.468] [G acc: 0.125]\n",
      "1299 [D loss: (0.269)(R 0.221, F 0.317)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.284] [G acc: 0.062]\n",
      "1300 [D loss: (0.126)(R 0.138, F 0.114)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.496] [G acc: 0.000]\n",
      "1301 [D loss: (0.235)(R 0.349, F 0.122)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.001] [G acc: 0.188]\n",
      "1302 [D loss: (0.081)(R 0.121, F 0.041)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.896] [G acc: 0.000]\n",
      "1303 [D loss: (0.183)(R 0.215, F 0.152)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.932] [G acc: 0.125]\n",
      "1304 [D loss: (0.113)(R 0.146, F 0.080)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.505] [G acc: 0.125]\n",
      "1305 [D loss: (0.275)(R 0.121, F 0.428)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.977] [G acc: 0.000]\n",
      "1306 [D loss: (0.464)(R 0.324, F 0.603)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.949] [G acc: 0.000]\n",
      "1307 [D loss: (0.247)(R 0.202, F 0.292)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.852] [G acc: 0.000]\n",
      "1308 [D loss: (0.153)(R 0.129, F 0.178)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.398] [G acc: 0.062]\n",
      "1309 [D loss: (0.166)(R 0.123, F 0.209)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.498] [G acc: 0.125]\n",
      "1310 [D loss: (0.244)(R 0.156, F 0.333)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.761] [G acc: 0.000]\n",
      "1311 [D loss: (0.106)(R 0.102, F 0.110)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.618] [G acc: 0.000]\n",
      "1312 [D loss: (0.275)(R 0.293, F 0.258)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.840] [G acc: 0.062]\n",
      "1313 [D loss: (0.084)(R 0.112, F 0.056)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.961] [G acc: 0.062]\n",
      "1314 [D loss: (0.221)(R 0.118, F 0.325)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.819] [G acc: 0.062]\n",
      "1315 [D loss: (0.105)(R 0.207, F 0.004)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.888] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316 [D loss: (0.133)(R 0.167, F 0.100)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.901] [G acc: 0.000]\n",
      "1317 [D loss: (0.228)(R 0.272, F 0.183)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.810] [G acc: 0.062]\n",
      "1318 [D loss: (0.386)(R 0.608, F 0.164)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.318] [G acc: 0.062]\n",
      "1319 [D loss: (0.098)(R 0.082, F 0.114)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.859] [G acc: 0.062]\n",
      "1320 [D loss: (0.135)(R 0.070, F 0.199)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.827] [G acc: 0.062]\n",
      "1321 [D loss: (0.280)(R 0.199, F 0.361)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.080] [G acc: 0.000]\n",
      "1322 [D loss: (0.123)(R 0.116, F 0.129)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.433] [G acc: 0.125]\n",
      "1323 [D loss: (0.244)(R 0.081, F 0.406)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.853] [G acc: 0.062]\n",
      "1324 [D loss: (0.250)(R 0.108, F 0.393)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.590] [G acc: 0.125]\n",
      "1325 [D loss: (0.127)(R 0.156, F 0.097)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.030] [G acc: 0.062]\n",
      "1326 [D loss: (0.123)(R 0.132, F 0.115)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.258] [G acc: 0.125]\n",
      "1327 [D loss: (0.188)(R 0.200, F 0.176)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.104] [G acc: 0.125]\n",
      "1328 [D loss: (0.236)(R 0.436, F 0.037)] [D acc: (0.938)(0.875, 1.000)] [G loss: 8.063] [G acc: 0.062]\n",
      "1329 [D loss: (0.123)(R 0.094, F 0.153)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.771] [G acc: 0.062]\n",
      "1330 [D loss: (0.246)(R 0.189, F 0.304)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.726] [G acc: 0.000]\n",
      "1331 [D loss: (0.062)(R 0.104, F 0.020)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.255] [G acc: 0.062]\n",
      "1332 [D loss: (0.314)(R 0.123, F 0.506)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.009] [G acc: 0.125]\n",
      "1333 [D loss: (0.248)(R 0.270, F 0.227)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.833] [G acc: 0.000]\n",
      "1334 [D loss: (0.200)(R 0.274, F 0.125)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.083] [G acc: 0.188]\n",
      "1335 [D loss: (0.096)(R 0.093, F 0.100)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.910] [G acc: 0.125]\n",
      "1336 [D loss: (0.120)(R 0.191, F 0.049)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.965] [G acc: 0.188]\n",
      "1337 [D loss: (0.141)(R 0.100, F 0.182)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.313] [G acc: 0.000]\n",
      "1338 [D loss: (0.137)(R 0.069, F 0.205)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.098] [G acc: 0.062]\n",
      "1339 [D loss: (0.293)(R 0.080, F 0.507)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.528] [G acc: 0.000]\n",
      "1340 [D loss: (0.085)(R 0.099, F 0.072)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.368] [G acc: 0.000]\n",
      "1341 [D loss: (0.381)(R 0.117, F 0.645)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.800] [G acc: 0.000]\n",
      "1342 [D loss: (0.390)(R 0.572, F 0.208)] [D acc: (0.875)(0.812, 0.938)] [G loss: 7.044] [G acc: 0.188]\n",
      "1343 [D loss: (0.082)(R 0.143, F 0.020)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.576] [G acc: 0.062]\n",
      "1344 [D loss: (0.252)(R 0.179, F 0.325)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.517] [G acc: 0.188]\n",
      "1345 [D loss: (0.405)(R 0.136, F 0.674)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.758] [G acc: 0.062]\n",
      "1346 [D loss: (0.123)(R 0.093, F 0.153)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.429] [G acc: 0.125]\n",
      "1347 [D loss: (0.205)(R 0.230, F 0.181)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.430] [G acc: 0.062]\n",
      "1348 [D loss: (0.072)(R 0.124, F 0.020)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.545] [G acc: 0.000]\n",
      "1349 [D loss: (0.135)(R 0.118, F 0.151)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.626] [G acc: 0.125]\n",
      "1350 [D loss: (0.179)(R 0.207, F 0.150)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.901] [G acc: 0.125]\n",
      "1351 [D loss: (0.225)(R 0.408, F 0.043)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.107] [G acc: 0.062]\n",
      "1352 [D loss: (0.250)(R 0.257, F 0.243)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.309] [G acc: 0.062]\n",
      "1353 [D loss: (0.034)(R 0.058, F 0.010)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.810] [G acc: 0.250]\n",
      "1354 [D loss: (0.290)(R 0.220, F 0.360)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.830] [G acc: 0.188]\n",
      "1355 [D loss: (0.092)(R 0.085, F 0.100)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.307] [G acc: 0.000]\n",
      "1356 [D loss: (0.133)(R 0.145, F 0.122)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.442] [G acc: 0.125]\n",
      "1357 [D loss: (0.192)(R 0.079, F 0.305)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.368] [G acc: 0.062]\n",
      "1358 [D loss: (0.306)(R 0.178, F 0.435)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.161] [G acc: 0.000]\n",
      "1359 [D loss: (0.118)(R 0.108, F 0.128)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.138] [G acc: 0.062]\n",
      "1360 [D loss: (0.144)(R 0.116, F 0.172)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.763] [G acc: 0.062]\n",
      "1361 [D loss: (0.060)(R 0.116, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.855] [G acc: 0.062]\n",
      "1362 [D loss: (0.298)(R 0.290, F 0.306)] [D acc: (0.875)(0.875, 0.875)] [G loss: 10.033] [G acc: 0.000]\n",
      "1363 [D loss: (0.135)(R 0.131, F 0.139)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.640] [G acc: 0.062]\n",
      "1364 [D loss: (0.469)(R 0.093, F 0.845)] [D acc: (0.812)(1.000, 0.625)] [G loss: 7.324] [G acc: 0.125]\n",
      "1365 [D loss: (0.430)(R 0.563, F 0.298)] [D acc: (0.844)(0.812, 0.875)] [G loss: 9.619] [G acc: 0.125]\n",
      "1366 [D loss: (0.329)(R 0.217, F 0.442)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.584] [G acc: 0.062]\n",
      "1367 [D loss: (0.114)(R 0.119, F 0.109)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.815] [G acc: 0.000]\n",
      "1368 [D loss: (0.291)(R 0.103, F 0.480)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.226] [G acc: 0.000]\n",
      "1369 [D loss: (0.060)(R 0.088, F 0.032)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.777] [G acc: 0.000]\n",
      "1370 [D loss: (0.200)(R 0.392, F 0.008)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.749] [G acc: 0.000]\n",
      "1371 [D loss: (0.215)(R 0.120, F 0.311)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.258] [G acc: 0.062]\n",
      "1372 [D loss: (0.087)(R 0.145, F 0.029)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.451] [G acc: 0.062]\n",
      "1373 [D loss: (0.076)(R 0.121, F 0.031)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.256] [G acc: 0.062]\n",
      "1374 [D loss: (0.124)(R 0.130, F 0.117)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.982] [G acc: 0.062]\n",
      "1375 [D loss: (0.293)(R 0.295, F 0.291)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.464] [G acc: 0.125]\n",
      "1376 [D loss: (0.273)(R 0.151, F 0.396)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.414] [G acc: 0.000]\n",
      "1377 [D loss: (0.225)(R 0.256, F 0.193)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.118] [G acc: 0.062]\n",
      "1378 [D loss: (0.111)(R 0.088, F 0.133)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.475] [G acc: 0.000]\n",
      "1379 [D loss: (0.270)(R 0.386, F 0.155)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.916] [G acc: 0.125]\n",
      "1380 [D loss: (0.266)(R 0.298, F 0.234)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.829] [G acc: 0.250]\n",
      "1381 [D loss: (0.192)(R 0.104, F 0.281)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.555] [G acc: 0.062]\n",
      "1382 [D loss: (0.259)(R 0.095, F 0.423)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.676] [G acc: 0.000]\n",
      "1383 [D loss: (0.128)(R 0.126, F 0.129)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.686] [G acc: 0.188]\n",
      "1384 [D loss: (0.118)(R 0.194, F 0.042)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.145] [G acc: 0.062]\n",
      "1385 [D loss: (0.365)(R 0.243, F 0.487)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.451] [G acc: 0.062]\n",
      "1386 [D loss: (0.139)(R 0.049, F 0.230)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.211] [G acc: 0.062]\n",
      "1387 [D loss: (0.109)(R 0.098, F 0.120)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.964] [G acc: 0.000]\n",
      "1388 [D loss: (0.138)(R 0.213, F 0.063)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.024] [G acc: 0.000]\n",
      "1389 [D loss: (0.281)(R 0.325, F 0.238)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.564] [G acc: 0.000]\n",
      "1390 [D loss: (0.040)(R 0.074, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.047] [G acc: 0.062]\n",
      "1391 [D loss: (0.103)(R 0.181, F 0.026)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.118] [G acc: 0.062]\n",
      "1392 [D loss: (0.139)(R 0.107, F 0.171)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.014] [G acc: 0.125]\n",
      "1393 [D loss: (0.238)(R 0.129, F 0.348)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.261] [G acc: 0.062]\n",
      "1394 [D loss: (0.169)(R 0.118, F 0.220)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.362] [G acc: 0.062]\n",
      "1395 [D loss: (0.220)(R 0.225, F 0.215)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.787] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1396 [D loss: (0.214)(R 0.340, F 0.087)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.144] [G acc: 0.125]\n",
      "1397 [D loss: (0.177)(R 0.313, F 0.040)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.639] [G acc: 0.062]\n",
      "1398 [D loss: (0.079)(R 0.106, F 0.052)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.285] [G acc: 0.062]\n",
      "1399 [D loss: (0.296)(R 0.249, F 0.344)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.106] [G acc: 0.062]\n",
      "1400 [D loss: (0.068)(R 0.071, F 0.065)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.327] [G acc: 0.125]\n",
      "1401 [D loss: (0.110)(R 0.051, F 0.168)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.650] [G acc: 0.062]\n",
      "1402 [D loss: (0.147)(R 0.134, F 0.159)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.883] [G acc: 0.125]\n",
      "1403 [D loss: (0.073)(R 0.111, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.982] [G acc: 0.000]\n",
      "1404 [D loss: (0.351)(R 0.096, F 0.606)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.838] [G acc: 0.062]\n",
      "1405 [D loss: (0.101)(R 0.174, F 0.029)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.840] [G acc: 0.062]\n",
      "1406 [D loss: (0.140)(R 0.138, F 0.142)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.541] [G acc: 0.000]\n",
      "1407 [D loss: (0.163)(R 0.093, F 0.233)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.797] [G acc: 0.000]\n",
      "1408 [D loss: (0.038)(R 0.072, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.128] [G acc: 0.125]\n",
      "1409 [D loss: (0.122)(R 0.176, F 0.068)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.731] [G acc: 0.000]\n",
      "1410 [D loss: (0.186)(R 0.218, F 0.154)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.289] [G acc: 0.062]\n",
      "1411 [D loss: (0.106)(R 0.118, F 0.093)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.981] [G acc: 0.062]\n",
      "1412 [D loss: (0.457)(R 0.320, F 0.595)] [D acc: (0.812)(0.938, 0.688)] [G loss: 7.691] [G acc: 0.062]\n",
      "1413 [D loss: (0.436)(R 0.395, F 0.477)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.588] [G acc: 0.000]\n",
      "1414 [D loss: (0.072)(R 0.048, F 0.097)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.525] [G acc: 0.250]\n",
      "1415 [D loss: (0.137)(R 0.085, F 0.189)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.598] [G acc: 0.000]\n",
      "1416 [D loss: (0.116)(R 0.095, F 0.137)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.525] [G acc: 0.062]\n",
      "1417 [D loss: (0.123)(R 0.203, F 0.042)] [D acc: (0.969)(0.938, 1.000)] [G loss: 10.275] [G acc: 0.000]\n",
      "1418 [D loss: (0.260)(R 0.232, F 0.289)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.727] [G acc: 0.000]\n",
      "1419 [D loss: (0.075)(R 0.070, F 0.081)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.362] [G acc: 0.000]\n",
      "1420 [D loss: (0.125)(R 0.107, F 0.144)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.907] [G acc: 0.125]\n",
      "1421 [D loss: (0.174)(R 0.067, F 0.281)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.712] [G acc: 0.000]\n",
      "1422 [D loss: (0.315)(R 0.406, F 0.225)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.390] [G acc: 0.125]\n",
      "1423 [D loss: (0.070)(R 0.110, F 0.031)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.922] [G acc: 0.000]\n",
      "1424 [D loss: (0.060)(R 0.065, F 0.055)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.386] [G acc: 0.000]\n",
      "1425 [D loss: (0.155)(R 0.085, F 0.226)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.285] [G acc: 0.062]\n",
      "1426 [D loss: (0.118)(R 0.221, F 0.016)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.749] [G acc: 0.000]\n",
      "1427 [D loss: (0.093)(R 0.075, F 0.110)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.736] [G acc: 0.000]\n",
      "1428 [D loss: (0.142)(R 0.154, F 0.130)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.138] [G acc: 0.062]\n",
      "1429 [D loss: (0.359)(R 0.610, F 0.107)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.981] [G acc: 0.125]\n",
      "1430 [D loss: (0.113)(R 0.060, F 0.167)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.912] [G acc: 0.188]\n",
      "1431 [D loss: (0.037)(R 0.050, F 0.024)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.221] [G acc: 0.062]\n",
      "1432 [D loss: (0.161)(R 0.163, F 0.159)] [D acc: (0.938)(0.938, 0.938)] [G loss: 10.483] [G acc: 0.000]\n",
      "1433 [D loss: (0.042)(R 0.070, F 0.014)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.513] [G acc: 0.000]\n",
      "1434 [D loss: (0.079)(R 0.083, F 0.075)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.127] [G acc: 0.062]\n",
      "1435 [D loss: (0.152)(R 0.108, F 0.196)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.521] [G acc: 0.000]\n",
      "1436 [D loss: (0.049)(R 0.098, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.276] [G acc: 0.000]\n",
      "1437 [D loss: (0.217)(R 0.117, F 0.317)] [D acc: (0.938)(1.000, 0.875)] [G loss: 10.363] [G acc: 0.062]\n",
      "1438 [D loss: (0.088)(R 0.043, F 0.134)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.615] [G acc: 0.062]\n",
      "1439 [D loss: (0.295)(R 0.461, F 0.128)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.140] [G acc: 0.062]\n",
      "1440 [D loss: (0.063)(R 0.081, F 0.044)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.816] [G acc: 0.000]\n",
      "1441 [D loss: (0.294)(R 0.049, F 0.539)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.410] [G acc: 0.125]\n",
      "1442 [D loss: (0.074)(R 0.080, F 0.069)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.625] [G acc: 0.062]\n",
      "1443 [D loss: (0.093)(R 0.171, F 0.014)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.685] [G acc: 0.062]\n",
      "1444 [D loss: (0.130)(R 0.065, F 0.195)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.332] [G acc: 0.062]\n",
      "1445 [D loss: (0.426)(R 0.466, F 0.385)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.753] [G acc: 0.062]\n",
      "1446 [D loss: (0.078)(R 0.153, F 0.003)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.478] [G acc: 0.062]\n",
      "1447 [D loss: (0.071)(R 0.069, F 0.072)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.217] [G acc: 0.000]\n",
      "1448 [D loss: (0.193)(R 0.088, F 0.298)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.690] [G acc: 0.000]\n",
      "1449 [D loss: (0.341)(R 0.355, F 0.328)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.982] [G acc: 0.062]\n",
      "1450 [D loss: (0.209)(R 0.046, F 0.372)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.504] [G acc: 0.000]\n",
      "1451 [D loss: (0.102)(R 0.138, F 0.065)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.944] [G acc: 0.062]\n",
      "1452 [D loss: (0.037)(R 0.051, F 0.024)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.533] [G acc: 0.000]\n",
      "1453 [D loss: (0.086)(R 0.137, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.623] [G acc: 0.062]\n",
      "1454 [D loss: (0.074)(R 0.110, F 0.039)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.268] [G acc: 0.125]\n",
      "1455 [D loss: (0.154)(R 0.074, F 0.234)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.248] [G acc: 0.000]\n",
      "1456 [D loss: (0.208)(R 0.079, F 0.337)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.879] [G acc: 0.062]\n",
      "1457 [D loss: (0.196)(R 0.201, F 0.190)] [D acc: (0.938)(0.938, 0.938)] [G loss: 10.331] [G acc: 0.062]\n",
      "1458 [D loss: (0.223)(R 0.412, F 0.035)] [D acc: (0.906)(0.812, 1.000)] [G loss: 7.769] [G acc: 0.062]\n",
      "1459 [D loss: (0.134)(R 0.230, F 0.038)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.655] [G acc: 0.062]\n",
      "1460 [D loss: (0.068)(R 0.135, F 0.000)] [D acc: (0.969)(0.938, 1.000)] [G loss: 10.291] [G acc: 0.000]\n",
      "1461 [D loss: (0.255)(R 0.100, F 0.409)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.086] [G acc: 0.000]\n",
      "1462 [D loss: (0.487)(R 0.683, F 0.291)] [D acc: (0.875)(0.812, 0.938)] [G loss: 5.814] [G acc: 0.125]\n",
      "1463 [D loss: (0.248)(R 0.081, F 0.415)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.938] [G acc: 0.000]\n",
      "1464 [D loss: (0.133)(R 0.109, F 0.157)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.014] [G acc: 0.062]\n",
      "1465 [D loss: (0.172)(R 0.070, F 0.273)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.991] [G acc: 0.188]\n",
      "1466 [D loss: (0.260)(R 0.156, F 0.364)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.298] [G acc: 0.000]\n",
      "1467 [D loss: (0.174)(R 0.286, F 0.062)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.604] [G acc: 0.000]\n",
      "1468 [D loss: (0.128)(R 0.163, F 0.092)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.075] [G acc: 0.062]\n",
      "1469 [D loss: (0.258)(R 0.492, F 0.023)] [D acc: (0.906)(0.812, 1.000)] [G loss: 6.545] [G acc: 0.000]\n",
      "1470 [D loss: (0.102)(R 0.050, F 0.154)] [D acc: (0.938)(1.000, 0.875)] [G loss: 10.670] [G acc: 0.000]\n",
      "1471 [D loss: (0.261)(R 0.080, F 0.441)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.104] [G acc: 0.062]\n",
      "1472 [D loss: (0.104)(R 0.083, F 0.125)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.482] [G acc: 0.062]\n",
      "1473 [D loss: (0.048)(R 0.058, F 0.038)] [D acc: (1.000)(1.000, 1.000)] [G loss: 11.581] [G acc: 0.000]\n",
      "1474 [D loss: (0.053)(R 0.097, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.841] [G acc: 0.062]\n",
      "1475 [D loss: (0.054)(R 0.094, F 0.014)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.187] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476 [D loss: (0.127)(R 0.095, F 0.158)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.962] [G acc: 0.062]\n",
      "1477 [D loss: (0.236)(R 0.093, F 0.379)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.094] [G acc: 0.062]\n",
      "1478 [D loss: (0.433)(R 0.279, F 0.588)] [D acc: (0.812)(0.875, 0.750)] [G loss: 8.149] [G acc: 0.062]\n",
      "1479 [D loss: (0.247)(R 0.267, F 0.228)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.518] [G acc: 0.000]\n",
      "1480 [D loss: (0.293)(R 0.109, F 0.477)] [D acc: (0.875)(1.000, 0.750)] [G loss: 9.376] [G acc: 0.062]\n",
      "1481 [D loss: (0.127)(R 0.095, F 0.160)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.443] [G acc: 0.000]\n",
      "1482 [D loss: (0.478)(R 0.573, F 0.384)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.770] [G acc: 0.000]\n",
      "1483 [D loss: (0.150)(R 0.098, F 0.202)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.751] [G acc: 0.188]\n",
      "1484 [D loss: (0.173)(R 0.139, F 0.208)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.006] [G acc: 0.125]\n",
      "1485 [D loss: (0.187)(R 0.125, F 0.249)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.676] [G acc: 0.062]\n",
      "1486 [D loss: (0.234)(R 0.414, F 0.055)] [D acc: (0.906)(0.812, 1.000)] [G loss: 8.792] [G acc: 0.000]\n",
      "1487 [D loss: (0.296)(R 0.570, F 0.023)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.391] [G acc: 0.125]\n",
      "1488 [D loss: (0.234)(R 0.081, F 0.388)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.914] [G acc: 0.062]\n",
      "1489 [D loss: (0.233)(R 0.375, F 0.091)] [D acc: (0.906)(0.875, 0.938)] [G loss: 9.642] [G acc: 0.062]\n",
      "1490 [D loss: (0.078)(R 0.070, F 0.085)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.996] [G acc: 0.062]\n",
      "1491 [D loss: (0.234)(R 0.086, F 0.382)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.173] [G acc: 0.062]\n",
      "1492 [D loss: (0.039)(R 0.076, F 0.001)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.662] [G acc: 0.125]\n",
      "1493 [D loss: (0.092)(R 0.122, F 0.061)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.106] [G acc: 0.000]\n",
      "1494 [D loss: (0.033)(R 0.053, F 0.013)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.354] [G acc: 0.062]\n",
      "1495 [D loss: (0.228)(R 0.434, F 0.021)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.625] [G acc: 0.062]\n",
      "1496 [D loss: (0.321)(R 0.147, F 0.495)] [D acc: (0.812)(0.938, 0.688)] [G loss: 9.594] [G acc: 0.062]\n",
      "1497 [D loss: (0.128)(R 0.125, F 0.132)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.858] [G acc: 0.125]\n",
      "1498 [D loss: (0.968)(R 1.231, F 0.705)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.168] [G acc: 0.062]\n",
      "1499 [D loss: (0.049)(R 0.079, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.711] [G acc: 0.000]\n",
      "1500 [D loss: (0.247)(R 0.101, F 0.394)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.722] [G acc: 0.062]\n",
      "1501 [D loss: (0.194)(R 0.097, F 0.291)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.219] [G acc: 0.000]\n",
      "1502 [D loss: (0.052)(R 0.081, F 0.023)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.753] [G acc: 0.000]\n",
      "1503 [D loss: (0.045)(R 0.064, F 0.026)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.505] [G acc: 0.062]\n",
      "1504 [D loss: (0.074)(R 0.107, F 0.041)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.447] [G acc: 0.062]\n",
      "1505 [D loss: (0.366)(R 0.658, F 0.074)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.568] [G acc: 0.125]\n",
      "1506 [D loss: (0.186)(R 0.076, F 0.296)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.012] [G acc: 0.000]\n",
      "1507 [D loss: (0.050)(R 0.086, F 0.014)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.736] [G acc: 0.062]\n",
      "1508 [D loss: (0.549)(R 1.013, F 0.085)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.098] [G acc: 0.062]\n",
      "1509 [D loss: (0.401)(R 0.067, F 0.734)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.133] [G acc: 0.062]\n",
      "1510 [D loss: (0.058)(R 0.055, F 0.061)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.502] [G acc: 0.062]\n",
      "1511 [D loss: (0.174)(R 0.128, F 0.219)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.697] [G acc: 0.000]\n",
      "1512 [D loss: (0.497)(R 0.226, F 0.767)] [D acc: (0.781)(0.938, 0.625)] [G loss: 6.564] [G acc: 0.062]\n",
      "1513 [D loss: (0.497)(R 0.740, F 0.255)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.261] [G acc: 0.188]\n",
      "1514 [D loss: (0.180)(R 0.350, F 0.011)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.437] [G acc: 0.125]\n",
      "1515 [D loss: (0.075)(R 0.080, F 0.070)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.397] [G acc: 0.125]\n",
      "1516 [D loss: (0.449)(R 0.465, F 0.432)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.693] [G acc: 0.062]\n",
      "1517 [D loss: (0.300)(R 0.434, F 0.166)] [D acc: (0.906)(0.938, 0.875)] [G loss: 9.777] [G acc: 0.000]\n",
      "1518 [D loss: (0.409)(R 0.230, F 0.588)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.684] [G acc: 0.188]\n",
      "1519 [D loss: (0.491)(R 0.617, F 0.364)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.868] [G acc: 0.250]\n",
      "1520 [D loss: (0.189)(R 0.144, F 0.233)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.836] [G acc: 0.188]\n",
      "1521 [D loss: (0.164)(R 0.258, F 0.070)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.187] [G acc: 0.125]\n",
      "1522 [D loss: (0.151)(R 0.100, F 0.202)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.031] [G acc: 0.062]\n",
      "1523 [D loss: (0.379)(R 0.224, F 0.535)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.720] [G acc: 0.188]\n",
      "1524 [D loss: (0.398)(R 0.416, F 0.380)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.450] [G acc: 0.125]\n",
      "1525 [D loss: (0.256)(R 0.198, F 0.314)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.481] [G acc: 0.000]\n",
      "1526 [D loss: (0.271)(R 0.201, F 0.341)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.806] [G acc: 0.000]\n",
      "1527 [D loss: (0.274)(R 0.286, F 0.262)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.543] [G acc: 0.000]\n",
      "1528 [D loss: (0.126)(R 0.216, F 0.036)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.216] [G acc: 0.062]\n",
      "1529 [D loss: (0.105)(R 0.095, F 0.115)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.048] [G acc: 0.062]\n",
      "1530 [D loss: (0.133)(R 0.137, F 0.130)] [D acc: (0.938)(0.938, 0.938)] [G loss: 10.243] [G acc: 0.000]\n",
      "1531 [D loss: (0.197)(R 0.217, F 0.177)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.467] [G acc: 0.188]\n",
      "1532 [D loss: (0.100)(R 0.164, F 0.035)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.741] [G acc: 0.125]\n",
      "1533 [D loss: (0.137)(R 0.208, F 0.065)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.791] [G acc: 0.125]\n",
      "1534 [D loss: (0.049)(R 0.091, F 0.006)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.475] [G acc: 0.000]\n",
      "1535 [D loss: (0.673)(R 0.639, F 0.706)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.765] [G acc: 0.062]\n",
      "1536 [D loss: (0.567)(R 0.705, F 0.430)] [D acc: (0.781)(0.812, 0.750)] [G loss: 8.783] [G acc: 0.000]\n",
      "1537 [D loss: (0.200)(R 0.188, F 0.211)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.117] [G acc: 0.188]\n",
      "1538 [D loss: (0.328)(R 0.378, F 0.278)] [D acc: (0.875)(0.812, 0.938)] [G loss: 6.780] [G acc: 0.125]\n",
      "1539 [D loss: (0.175)(R 0.104, F 0.246)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.703] [G acc: 0.125]\n",
      "1540 [D loss: (0.118)(R 0.113, F 0.122)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.950] [G acc: 0.062]\n",
      "1541 [D loss: (0.092)(R 0.135, F 0.049)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.733] [G acc: 0.062]\n",
      "1542 [D loss: (0.146)(R 0.093, F 0.200)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.754] [G acc: 0.125]\n",
      "1543 [D loss: (0.573)(R 0.776, F 0.370)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.351] [G acc: 0.000]\n",
      "1544 [D loss: (0.104)(R 0.047, F 0.160)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.647] [G acc: 0.062]\n",
      "1545 [D loss: (0.192)(R 0.093, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.658] [G acc: 0.062]\n",
      "1546 [D loss: (1.195)(R 1.545, F 0.845)] [D acc: (0.750)(0.812, 0.688)] [G loss: 8.287] [G acc: 0.125]\n",
      "1547 [D loss: (0.312)(R 0.595, F 0.028)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.264] [G acc: 0.125]\n",
      "1548 [D loss: (0.250)(R 0.193, F 0.308)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.944] [G acc: 0.062]\n",
      "1549 [D loss: (0.069)(R 0.090, F 0.048)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.239] [G acc: 0.000]\n",
      "1550 [D loss: (0.463)(R 0.448, F 0.479)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.828] [G acc: 0.125]\n",
      "1551 [D loss: (0.117)(R 0.194, F 0.039)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.088] [G acc: 0.062]\n",
      "1552 [D loss: (0.180)(R 0.138, F 0.223)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.770] [G acc: 0.000]\n",
      "1553 [D loss: (0.404)(R 0.210, F 0.599)] [D acc: (0.781)(0.875, 0.688)] [G loss: 7.279] [G acc: 0.125]\n",
      "1554 [D loss: (0.266)(R 0.234, F 0.297)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.339] [G acc: 0.000]\n",
      "1555 [D loss: (0.109)(R 0.189, F 0.029)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.749] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1556 [D loss: (0.052)(R 0.059, F 0.046)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.698] [G acc: 0.250]\n",
      "1557 [D loss: (0.337)(R 0.491, F 0.183)] [D acc: (0.875)(0.812, 0.938)] [G loss: 8.183] [G acc: 0.000]\n",
      "1558 [D loss: (0.272)(R 0.416, F 0.127)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.393] [G acc: 0.062]\n",
      "1559 [D loss: (0.205)(R 0.091, F 0.320)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.779] [G acc: 0.062]\n",
      "1560 [D loss: (0.256)(R 0.089, F 0.424)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.871] [G acc: 0.062]\n",
      "1561 [D loss: (0.078)(R 0.091, F 0.065)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.354] [G acc: 0.188]\n",
      "1562 [D loss: (0.517)(R 0.646, F 0.387)] [D acc: (0.781)(0.812, 0.750)] [G loss: 7.119] [G acc: 0.125]\n",
      "1563 [D loss: (0.377)(R 0.737, F 0.016)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.126] [G acc: 0.188]\n",
      "1564 [D loss: (0.166)(R 0.085, F 0.247)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.912] [G acc: 0.000]\n",
      "1565 [D loss: (0.103)(R 0.100, F 0.106)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.389] [G acc: 0.062]\n",
      "1566 [D loss: (0.316)(R 0.233, F 0.398)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.841] [G acc: 0.062]\n",
      "1567 [D loss: (0.241)(R 0.334, F 0.148)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.033] [G acc: 0.188]\n",
      "1568 [D loss: (0.082)(R 0.103, F 0.060)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.414] [G acc: 0.062]\n",
      "1569 [D loss: (0.146)(R 0.072, F 0.220)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.218] [G acc: 0.000]\n",
      "1570 [D loss: (0.085)(R 0.067, F 0.103)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.739] [G acc: 0.062]\n",
      "1571 [D loss: (0.222)(R 0.070, F 0.374)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.842] [G acc: 0.125]\n",
      "1572 [D loss: (0.362)(R 0.410, F 0.315)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.346] [G acc: 0.000]\n",
      "1573 [D loss: (0.177)(R 0.133, F 0.221)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.571] [G acc: 0.188]\n",
      "1574 [D loss: (0.146)(R 0.094, F 0.197)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.073] [G acc: 0.000]\n",
      "1575 [D loss: (0.211)(R 0.099, F 0.323)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.842] [G acc: 0.188]\n",
      "1576 [D loss: (0.209)(R 0.369, F 0.050)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.822] [G acc: 0.062]\n",
      "1577 [D loss: (0.180)(R 0.139, F 0.221)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.142] [G acc: 0.062]\n",
      "1578 [D loss: (0.245)(R 0.249, F 0.241)] [D acc: (0.875)(0.875, 0.875)] [G loss: 9.762] [G acc: 0.000]\n",
      "1579 [D loss: (0.071)(R 0.081, F 0.061)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.472] [G acc: 0.062]\n",
      "1580 [D loss: (0.215)(R 0.100, F 0.329)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.806] [G acc: 0.125]\n",
      "1581 [D loss: (0.272)(R 0.216, F 0.328)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.129] [G acc: 0.062]\n",
      "1582 [D loss: (0.117)(R 0.072, F 0.162)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.082] [G acc: 0.062]\n",
      "1583 [D loss: (0.267)(R 0.236, F 0.298)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.335] [G acc: 0.062]\n",
      "1584 [D loss: (0.040)(R 0.062, F 0.018)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.884] [G acc: 0.000]\n",
      "1585 [D loss: (0.191)(R 0.136, F 0.246)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.435] [G acc: 0.062]\n",
      "1586 [D loss: (0.105)(R 0.188, F 0.022)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.284] [G acc: 0.125]\n",
      "1587 [D loss: (0.145)(R 0.084, F 0.206)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.745] [G acc: 0.125]\n",
      "1588 [D loss: (0.108)(R 0.072, F 0.144)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.227] [G acc: 0.125]\n",
      "1589 [D loss: (0.335)(R 0.240, F 0.431)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.138] [G acc: 0.062]\n",
      "1590 [D loss: (0.337)(R 0.067, F 0.608)] [D acc: (0.875)(1.000, 0.750)] [G loss: 10.372] [G acc: 0.000]\n",
      "1591 [D loss: (0.073)(R 0.139, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 10.323] [G acc: 0.000]\n",
      "1592 [D loss: (0.342)(R 0.457, F 0.227)] [D acc: (0.844)(0.750, 0.938)] [G loss: 8.388] [G acc: 0.062]\n",
      "1593 [D loss: (0.361)(R 0.428, F 0.295)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.788] [G acc: 0.312]\n",
      "1594 [D loss: (0.151)(R 0.098, F 0.204)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.615] [G acc: 0.000]\n",
      "1595 [D loss: (0.269)(R 0.178, F 0.361)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.837] [G acc: 0.062]\n",
      "1596 [D loss: (0.266)(R 0.296, F 0.237)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.199] [G acc: 0.125]\n",
      "1597 [D loss: (0.151)(R 0.097, F 0.205)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.009] [G acc: 0.000]\n",
      "1598 [D loss: (0.162)(R 0.085, F 0.239)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.436] [G acc: 0.062]\n",
      "1599 [D loss: (0.517)(R 0.865, F 0.168)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.748] [G acc: 0.062]\n",
      "1600 [D loss: (0.170)(R 0.113, F 0.228)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.086] [G acc: 0.062]\n",
      "1601 [D loss: (0.278)(R 0.365, F 0.191)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.752] [G acc: 0.125]\n",
      "1602 [D loss: (0.088)(R 0.080, F 0.096)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.821] [G acc: 0.000]\n",
      "1603 [D loss: (0.374)(R 0.443, F 0.305)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.100] [G acc: 0.062]\n",
      "1604 [D loss: (0.178)(R 0.193, F 0.164)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.113] [G acc: 0.125]\n",
      "1605 [D loss: (0.055)(R 0.098, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.334] [G acc: 0.062]\n",
      "1606 [D loss: (0.356)(R 0.088, F 0.623)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.729] [G acc: 0.125]\n",
      "1607 [D loss: (0.284)(R 0.083, F 0.485)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.401] [G acc: 0.000]\n",
      "1608 [D loss: (0.412)(R 0.709, F 0.116)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.867] [G acc: 0.000]\n",
      "1609 [D loss: (0.123)(R 0.186, F 0.060)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.905] [G acc: 0.125]\n",
      "1610 [D loss: (0.268)(R 0.297, F 0.239)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.712] [G acc: 0.000]\n",
      "1611 [D loss: (0.096)(R 0.108, F 0.084)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.434] [G acc: 0.125]\n",
      "1612 [D loss: (0.137)(R 0.094, F 0.181)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.243] [G acc: 0.062]\n",
      "1613 [D loss: (0.041)(R 0.064, F 0.017)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.193] [G acc: 0.062]\n",
      "1614 [D loss: (0.198)(R 0.096, F 0.300)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.043] [G acc: 0.250]\n",
      "1615 [D loss: (0.189)(R 0.108, F 0.271)] [D acc: (0.906)(1.000, 0.812)] [G loss: 9.188] [G acc: 0.062]\n",
      "1616 [D loss: (0.253)(R 0.475, F 0.031)] [D acc: (0.938)(0.875, 1.000)] [G loss: 9.125] [G acc: 0.000]\n",
      "1617 [D loss: (1.221)(R 1.935, F 0.507)] [D acc: (0.781)(0.750, 0.812)] [G loss: 8.008] [G acc: 0.125]\n",
      "1618 [D loss: (0.151)(R 0.209, F 0.093)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.792] [G acc: 0.188]\n",
      "1619 [D loss: (0.118)(R 0.139, F 0.097)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.394] [G acc: 0.125]\n",
      "1620 [D loss: (0.246)(R 0.116, F 0.377)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.524] [G acc: 0.062]\n",
      "1621 [D loss: (0.193)(R 0.208, F 0.177)] [D acc: (0.906)(0.875, 0.938)] [G loss: 9.031] [G acc: 0.000]\n",
      "1622 [D loss: (0.500)(R 0.210, F 0.790)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.265] [G acc: 0.125]\n",
      "1623 [D loss: (0.287)(R 0.122, F 0.452)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.064] [G acc: 0.062]\n",
      "1624 [D loss: (0.307)(R 0.190, F 0.424)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.428] [G acc: 0.125]\n",
      "1625 [D loss: (0.094)(R 0.143, F 0.044)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.912] [G acc: 0.000]\n",
      "1626 [D loss: (0.288)(R 0.117, F 0.460)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.993] [G acc: 0.000]\n",
      "1627 [D loss: (0.150)(R 0.238, F 0.062)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.038] [G acc: 0.062]\n",
      "1628 [D loss: (0.125)(R 0.099, F 0.151)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.745] [G acc: 0.062]\n",
      "1629 [D loss: (0.134)(R 0.098, F 0.171)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.908] [G acc: 0.188]\n",
      "1630 [D loss: (0.113)(R 0.100, F 0.127)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.849] [G acc: 0.062]\n",
      "1631 [D loss: (0.097)(R 0.129, F 0.066)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.219] [G acc: 0.125]\n",
      "1632 [D loss: (0.197)(R 0.159, F 0.235)] [D acc: (0.938)(0.938, 0.938)] [G loss: 10.280] [G acc: 0.000]\n",
      "1633 [D loss: (0.526)(R 0.604, F 0.447)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.529] [G acc: 0.062]\n",
      "1634 [D loss: (0.077)(R 0.067, F 0.088)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.400] [G acc: 0.062]\n",
      "1635 [D loss: (0.176)(R 0.227, F 0.125)] [D acc: (0.906)(0.875, 0.938)] [G loss: 10.078] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636 [D loss: (0.267)(R 0.078, F 0.456)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.891] [G acc: 0.125]\n",
      "1637 [D loss: (0.109)(R 0.090, F 0.128)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.219] [G acc: 0.000]\n",
      "1638 [D loss: (0.444)(R 0.584, F 0.304)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.457] [G acc: 0.062]\n",
      "1639 [D loss: (0.390)(R 0.586, F 0.194)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.831] [G acc: 0.062]\n",
      "1640 [D loss: (0.249)(R 0.296, F 0.202)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.293] [G acc: 0.062]\n",
      "1641 [D loss: (0.245)(R 0.069, F 0.421)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.241] [G acc: 0.125]\n",
      "1642 [D loss: (0.122)(R 0.174, F 0.070)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.821] [G acc: 0.125]\n",
      "1643 [D loss: (0.112)(R 0.106, F 0.118)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.700] [G acc: 0.125]\n",
      "1644 [D loss: (0.259)(R 0.117, F 0.401)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.561] [G acc: 0.000]\n",
      "1645 [D loss: (0.083)(R 0.131, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.724] [G acc: 0.062]\n",
      "1646 [D loss: (0.083)(R 0.084, F 0.082)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.963] [G acc: 0.062]\n",
      "1647 [D loss: (0.244)(R 0.364, F 0.124)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.276] [G acc: 0.062]\n",
      "1648 [D loss: (0.355)(R 0.105, F 0.606)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.935] [G acc: 0.062]\n",
      "1649 [D loss: (0.195)(R 0.224, F 0.166)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.027] [G acc: 0.188]\n",
      "1650 [D loss: (0.159)(R 0.179, F 0.140)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.595] [G acc: 0.188]\n",
      "1651 [D loss: (0.074)(R 0.110, F 0.039)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.121] [G acc: 0.062]\n",
      "1652 [D loss: (0.168)(R 0.109, F 0.227)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.509] [G acc: 0.000]\n",
      "1653 [D loss: (0.096)(R 0.186, F 0.006)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.631] [G acc: 0.188]\n",
      "1654 [D loss: (0.204)(R 0.114, F 0.294)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.442] [G acc: 0.000]\n",
      "1655 [D loss: (0.064)(R 0.086, F 0.042)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.833] [G acc: 0.000]\n",
      "1656 [D loss: (0.239)(R 0.068, F 0.410)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.313] [G acc: 0.250]\n",
      "1657 [D loss: (0.080)(R 0.094, F 0.066)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.085] [G acc: 0.062]\n",
      "1658 [D loss: (0.054)(R 0.097, F 0.011)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.676] [G acc: 0.000]\n",
      "1659 [D loss: (0.187)(R 0.173, F 0.201)] [D acc: (0.906)(0.938, 0.875)] [G loss: 9.235] [G acc: 0.062]\n",
      "1660 [D loss: (0.089)(R 0.075, F 0.104)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.572] [G acc: 0.062]\n",
      "1661 [D loss: (0.083)(R 0.155, F 0.010)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.124] [G acc: 0.000]\n",
      "1662 [D loss: (0.390)(R 0.304, F 0.476)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.053] [G acc: 0.062]\n",
      "1663 [D loss: (0.060)(R 0.056, F 0.063)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.990] [G acc: 0.000]\n",
      "1664 [D loss: (0.062)(R 0.065, F 0.059)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.420] [G acc: 0.062]\n",
      "1665 [D loss: (0.353)(R 0.241, F 0.465)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.577] [G acc: 0.125]\n",
      "1666 [D loss: (0.381)(R 0.274, F 0.489)] [D acc: (0.844)(0.938, 0.750)] [G loss: 9.404] [G acc: 0.062]\n",
      "1667 [D loss: (0.061)(R 0.105, F 0.018)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.626] [G acc: 0.062]\n",
      "1668 [D loss: (0.296)(R 0.115, F 0.477)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.782] [G acc: 0.062]\n",
      "1669 [D loss: (0.165)(R 0.092, F 0.237)] [D acc: (0.969)(1.000, 0.938)] [G loss: 11.201] [G acc: 0.062]\n",
      "1670 [D loss: (0.167)(R 0.194, F 0.140)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.824] [G acc: 0.125]\n",
      "1671 [D loss: (0.238)(R 0.095, F 0.381)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.674] [G acc: 0.125]\n",
      "1672 [D loss: (0.087)(R 0.130, F 0.044)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.705] [G acc: 0.188]\n",
      "1673 [D loss: (0.128)(R 0.167, F 0.089)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.915] [G acc: 0.062]\n",
      "1674 [D loss: (0.188)(R 0.200, F 0.175)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.867] [G acc: 0.062]\n",
      "1675 [D loss: (0.192)(R 0.341, F 0.044)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.634] [G acc: 0.062]\n",
      "1676 [D loss: (0.335)(R 0.668, F 0.003)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.309] [G acc: 0.188]\n",
      "1677 [D loss: (0.242)(R 0.164, F 0.320)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.215] [G acc: 0.062]\n",
      "1678 [D loss: (0.141)(R 0.073, F 0.209)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.737] [G acc: 0.125]\n",
      "1679 [D loss: (0.088)(R 0.098, F 0.079)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.089] [G acc: 0.125]\n",
      "1680 [D loss: (0.293)(R 0.078, F 0.508)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.215] [G acc: 0.062]\n",
      "1681 [D loss: (0.185)(R 0.330, F 0.040)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.784] [G acc: 0.062]\n",
      "1682 [D loss: (0.081)(R 0.117, F 0.046)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.386] [G acc: 0.125]\n",
      "1683 [D loss: (0.165)(R 0.292, F 0.038)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.035] [G acc: 0.000]\n",
      "1684 [D loss: (0.208)(R 0.162, F 0.255)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.691] [G acc: 0.125]\n",
      "1685 [D loss: (0.075)(R 0.061, F 0.089)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.515] [G acc: 0.062]\n",
      "1686 [D loss: (0.485)(R 0.089, F 0.880)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.459] [G acc: 0.062]\n",
      "1687 [D loss: (0.061)(R 0.095, F 0.027)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.008] [G acc: 0.125]\n",
      "1688 [D loss: (0.161)(R 0.285, F 0.037)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.317] [G acc: 0.125]\n",
      "1689 [D loss: (0.182)(R 0.143, F 0.222)] [D acc: (0.906)(0.938, 0.875)] [G loss: 9.295] [G acc: 0.062]\n",
      "1690 [D loss: (0.082)(R 0.107, F 0.057)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.963] [G acc: 0.188]\n",
      "1691 [D loss: (0.069)(R 0.126, F 0.013)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.500] [G acc: 0.188]\n",
      "1692 [D loss: (0.142)(R 0.074, F 0.210)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.685] [G acc: 0.000]\n",
      "1693 [D loss: (0.051)(R 0.087, F 0.015)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.336] [G acc: 0.125]\n",
      "1694 [D loss: (0.393)(R 0.453, F 0.333)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.015] [G acc: 0.125]\n",
      "1695 [D loss: (0.569)(R 0.786, F 0.352)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.906] [G acc: 0.188]\n",
      "1696 [D loss: (0.161)(R 0.063, F 0.259)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.827] [G acc: 0.125]\n",
      "1697 [D loss: (0.664)(R 0.836, F 0.492)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.552] [G acc: 0.062]\n",
      "1698 [D loss: (0.153)(R 0.066, F 0.239)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.334] [G acc: 0.000]\n",
      "1699 [D loss: (0.147)(R 0.110, F 0.185)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.135] [G acc: 0.000]\n",
      "1700 [D loss: (0.091)(R 0.093, F 0.089)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.420] [G acc: 0.125]\n",
      "1701 [D loss: (0.059)(R 0.106, F 0.012)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.166] [G acc: 0.000]\n",
      "1702 [D loss: (0.119)(R 0.079, F 0.160)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.407] [G acc: 0.188]\n",
      "1703 [D loss: (0.085)(R 0.115, F 0.055)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.046] [G acc: 0.125]\n",
      "1704 [D loss: (0.114)(R 0.067, F 0.160)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.890] [G acc: 0.062]\n",
      "1705 [D loss: (0.093)(R 0.117, F 0.069)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.371] [G acc: 0.000]\n",
      "1706 [D loss: (0.245)(R 0.198, F 0.292)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.755] [G acc: 0.125]\n",
      "1707 [D loss: (0.273)(R 0.342, F 0.204)] [D acc: (0.938)(0.938, 0.938)] [G loss: 9.333] [G acc: 0.062]\n",
      "1708 [D loss: (0.105)(R 0.198, F 0.013)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.998] [G acc: 0.062]\n",
      "1709 [D loss: (0.093)(R 0.146, F 0.040)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.380] [G acc: 0.125]\n",
      "1710 [D loss: (0.151)(R 0.292, F 0.010)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.699] [G acc: 0.000]\n",
      "1711 [D loss: (0.322)(R 0.092, F 0.552)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.204] [G acc: 0.125]\n",
      "1712 [D loss: (0.057)(R 0.077, F 0.037)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.420] [G acc: 0.125]\n",
      "1713 [D loss: (0.252)(R 0.210, F 0.295)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.102] [G acc: 0.000]\n",
      "1714 [D loss: (0.188)(R 0.126, F 0.251)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.667] [G acc: 0.062]\n",
      "1715 [D loss: (0.304)(R 0.140, F 0.468)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.389] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1716 [D loss: (0.204)(R 0.123, F 0.285)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.509] [G acc: 0.125]\n",
      "1717 [D loss: (0.328)(R 0.242, F 0.413)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.989] [G acc: 0.188]\n",
      "1718 [D loss: (0.085)(R 0.156, F 0.014)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.804] [G acc: 0.062]\n",
      "1719 [D loss: (0.133)(R 0.090, F 0.177)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.842] [G acc: 0.125]\n",
      "1720 [D loss: (0.284)(R 0.198, F 0.370)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.828] [G acc: 0.125]\n",
      "1721 [D loss: (0.111)(R 0.084, F 0.139)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.114] [G acc: 0.188]\n",
      "1722 [D loss: (0.152)(R 0.128, F 0.175)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.818] [G acc: 0.000]\n",
      "1723 [D loss: (0.157)(R 0.169, F 0.146)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.659] [G acc: 0.125]\n",
      "1724 [D loss: (0.171)(R 0.136, F 0.206)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.482] [G acc: 0.125]\n",
      "1725 [D loss: (0.208)(R 0.089, F 0.328)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.456] [G acc: 0.062]\n",
      "1726 [D loss: (0.202)(R 0.374, F 0.029)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.453] [G acc: 0.125]\n",
      "1727 [D loss: (0.099)(R 0.159, F 0.039)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.301] [G acc: 0.000]\n",
      "1728 [D loss: (0.711)(R 1.181, F 0.241)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.522] [G acc: 0.188]\n",
      "1729 [D loss: (0.096)(R 0.091, F 0.101)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.730] [G acc: 0.000]\n",
      "1730 [D loss: (0.161)(R 0.146, F 0.176)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.017] [G acc: 0.062]\n",
      "1731 [D loss: (0.050)(R 0.071, F 0.028)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.516] [G acc: 0.000]\n",
      "1732 [D loss: (0.378)(R 0.544, F 0.213)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.568] [G acc: 0.125]\n",
      "1733 [D loss: (0.302)(R 0.225, F 0.378)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.533] [G acc: 0.062]\n",
      "1734 [D loss: (0.301)(R 0.537, F 0.066)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.002] [G acc: 0.125]\n",
      "1735 [D loss: (0.124)(R 0.071, F 0.177)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.563] [G acc: 0.000]\n",
      "1736 [D loss: (0.353)(R 0.202, F 0.505)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.663] [G acc: 0.062]\n",
      "1737 [D loss: (0.231)(R 0.129, F 0.333)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.206] [G acc: 0.000]\n",
      "1738 [D loss: (0.311)(R 0.091, F 0.531)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.624] [G acc: 0.125]\n",
      "1739 [D loss: (0.177)(R 0.098, F 0.256)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.077] [G acc: 0.188]\n",
      "1740 [D loss: (0.168)(R 0.136, F 0.200)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.210] [G acc: 0.000]\n",
      "1741 [D loss: (0.048)(R 0.075, F 0.022)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.399] [G acc: 0.062]\n",
      "1742 [D loss: (0.198)(R 0.133, F 0.262)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.794] [G acc: 0.062]\n",
      "1743 [D loss: (0.415)(R 0.215, F 0.615)] [D acc: (0.844)(0.875, 0.812)] [G loss: 10.187] [G acc: 0.062]\n",
      "1744 [D loss: (0.339)(R 0.561, F 0.116)] [D acc: (0.875)(0.812, 0.938)] [G loss: 8.366] [G acc: 0.062]\n",
      "1745 [D loss: (0.156)(R 0.129, F 0.182)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.859] [G acc: 0.000]\n",
      "1746 [D loss: (0.350)(R 0.531, F 0.168)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.277] [G acc: 0.000]\n",
      "1747 [D loss: (0.198)(R 0.350, F 0.046)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.567] [G acc: 0.062]\n",
      "1748 [D loss: (0.201)(R 0.180, F 0.223)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.586] [G acc: 0.125]\n",
      "1749 [D loss: (0.438)(R 0.402, F 0.474)] [D acc: (0.844)(0.812, 0.875)] [G loss: 5.673] [G acc: 0.062]\n",
      "1750 [D loss: (0.422)(R 0.481, F 0.363)] [D acc: (0.812)(0.875, 0.750)] [G loss: 8.819] [G acc: 0.000]\n",
      "1751 [D loss: (0.240)(R 0.359, F 0.122)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.631] [G acc: 0.062]\n",
      "1752 [D loss: (0.119)(R 0.093, F 0.145)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.468] [G acc: 0.000]\n",
      "1753 [D loss: (0.105)(R 0.110, F 0.100)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.736] [G acc: 0.062]\n",
      "1754 [D loss: (0.322)(R 0.396, F 0.248)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.367] [G acc: 0.000]\n",
      "1755 [D loss: (0.504)(R 0.216, F 0.793)] [D acc: (0.750)(0.875, 0.625)] [G loss: 8.288] [G acc: 0.000]\n",
      "1756 [D loss: (0.470)(R 0.133, F 0.807)] [D acc: (0.812)(0.938, 0.688)] [G loss: 7.016] [G acc: 0.062]\n",
      "1757 [D loss: (0.306)(R 0.384, F 0.228)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.620] [G acc: 0.062]\n",
      "1758 [D loss: (0.258)(R 0.341, F 0.174)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.247] [G acc: 0.000]\n",
      "1759 [D loss: (0.180)(R 0.101, F 0.260)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.194] [G acc: 0.188]\n",
      "1760 [D loss: (0.190)(R 0.193, F 0.187)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.483] [G acc: 0.188]\n",
      "1761 [D loss: (0.087)(R 0.112, F 0.063)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.422] [G acc: 0.125]\n",
      "1762 [D loss: (0.453)(R 0.303, F 0.603)] [D acc: (0.875)(0.938, 0.812)] [G loss: 9.071] [G acc: 0.062]\n",
      "1763 [D loss: (0.123)(R 0.175, F 0.071)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.954] [G acc: 0.062]\n",
      "1764 [D loss: (0.334)(R 0.104, F 0.563)] [D acc: (0.875)(1.000, 0.750)] [G loss: 9.166] [G acc: 0.125]\n",
      "1765 [D loss: (0.064)(R 0.093, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.076] [G acc: 0.125]\n",
      "1766 [D loss: (0.215)(R 0.243, F 0.187)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.509] [G acc: 0.250]\n",
      "1767 [D loss: (0.220)(R 0.334, F 0.105)] [D acc: (0.938)(0.875, 1.000)] [G loss: 8.354] [G acc: 0.062]\n",
      "1768 [D loss: (0.249)(R 0.096, F 0.403)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.698] [G acc: 0.062]\n",
      "1769 [D loss: (0.366)(R 0.220, F 0.512)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.568] [G acc: 0.125]\n",
      "1770 [D loss: (0.300)(R 0.263, F 0.337)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.104] [G acc: 0.125]\n",
      "1771 [D loss: (0.212)(R 0.181, F 0.243)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.253] [G acc: 0.000]\n",
      "1772 [D loss: (0.157)(R 0.108, F 0.206)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.203] [G acc: 0.062]\n",
      "1773 [D loss: (0.255)(R 0.143, F 0.367)] [D acc: (0.875)(1.000, 0.750)] [G loss: 7.582] [G acc: 0.062]\n",
      "1774 [D loss: (0.160)(R 0.104, F 0.217)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.577] [G acc: 0.062]\n",
      "1775 [D loss: (0.138)(R 0.118, F 0.158)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.382] [G acc: 0.062]\n",
      "1776 [D loss: (0.360)(R 0.448, F 0.271)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.942] [G acc: 0.000]\n",
      "1777 [D loss: (0.177)(R 0.216, F 0.138)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.823] [G acc: 0.062]\n",
      "1778 [D loss: (0.351)(R 0.342, F 0.361)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.104] [G acc: 0.125]\n",
      "1779 [D loss: (0.165)(R 0.094, F 0.237)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.269] [G acc: 0.125]\n",
      "1780 [D loss: (0.659)(R 0.926, F 0.392)] [D acc: (0.781)(0.812, 0.750)] [G loss: 6.886] [G acc: 0.062]\n",
      "1781 [D loss: (0.261)(R 0.337, F 0.186)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.414] [G acc: 0.188]\n",
      "1782 [D loss: (0.253)(R 0.229, F 0.277)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.013] [G acc: 0.062]\n",
      "1783 [D loss: (0.344)(R 0.211, F 0.477)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.217] [G acc: 0.125]\n",
      "1784 [D loss: (0.098)(R 0.141, F 0.056)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.275] [G acc: 0.000]\n",
      "1785 [D loss: (0.241)(R 0.436, F 0.046)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.858] [G acc: 0.062]\n",
      "1786 [D loss: (0.337)(R 0.645, F 0.030)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.558] [G acc: 0.125]\n",
      "1787 [D loss: (0.371)(R 0.358, F 0.384)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.833] [G acc: 0.125]\n",
      "1788 [D loss: (0.227)(R 0.162, F 0.293)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.510] [G acc: 0.000]\n",
      "1789 [D loss: (0.390)(R 0.417, F 0.363)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.811] [G acc: 0.125]\n",
      "1790 [D loss: (0.202)(R 0.112, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.638] [G acc: 0.188]\n",
      "1791 [D loss: (0.225)(R 0.116, F 0.335)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.502] [G acc: 0.062]\n",
      "1792 [D loss: (0.573)(R 0.567, F 0.580)] [D acc: (0.781)(0.875, 0.688)] [G loss: 5.604] [G acc: 0.125]\n",
      "1793 [D loss: (0.358)(R 0.532, F 0.185)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.408] [G acc: 0.000]\n",
      "1794 [D loss: (0.241)(R 0.390, F 0.092)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.503] [G acc: 0.000]\n",
      "1795 [D loss: (0.217)(R 0.202, F 0.231)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.024] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796 [D loss: (0.229)(R 0.174, F 0.284)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.115] [G acc: 0.188]\n",
      "1797 [D loss: (0.381)(R 0.353, F 0.408)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.952] [G acc: 0.125]\n",
      "1798 [D loss: (0.235)(R 0.184, F 0.286)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.772] [G acc: 0.188]\n",
      "1799 [D loss: (0.268)(R 0.140, F 0.395)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.908] [G acc: 0.062]\n",
      "1800 [D loss: (0.191)(R 0.212, F 0.170)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.209] [G acc: 0.062]\n",
      "1801 [D loss: (0.359)(R 0.366, F 0.353)] [D acc: (0.781)(0.812, 0.750)] [G loss: 7.199] [G acc: 0.125]\n",
      "1802 [D loss: (0.356)(R 0.621, F 0.090)] [D acc: (0.906)(0.812, 1.000)] [G loss: 8.253] [G acc: 0.000]\n",
      "1803 [D loss: (0.316)(R 0.178, F 0.454)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.292] [G acc: 0.062]\n",
      "1804 [D loss: (0.588)(R 0.380, F 0.796)] [D acc: (0.750)(0.875, 0.625)] [G loss: 5.965] [G acc: 0.062]\n",
      "1805 [D loss: (0.412)(R 0.691, F 0.133)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.842] [G acc: 0.125]\n",
      "1806 [D loss: (0.313)(R 0.101, F 0.526)] [D acc: (0.875)(1.000, 0.750)] [G loss: 8.244] [G acc: 0.000]\n",
      "1807 [D loss: (0.449)(R 0.790, F 0.109)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.633] [G acc: 0.125]\n",
      "1808 [D loss: (0.396)(R 0.132, F 0.659)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.266] [G acc: 0.188]\n",
      "1809 [D loss: (0.274)(R 0.146, F 0.403)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.556] [G acc: 0.125]\n",
      "1810 [D loss: (0.177)(R 0.167, F 0.187)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.573] [G acc: 0.062]\n",
      "1811 [D loss: (0.395)(R 0.370, F 0.420)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.282] [G acc: 0.062]\n",
      "1812 [D loss: (0.200)(R 0.109, F 0.291)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.567] [G acc: 0.062]\n",
      "1813 [D loss: (0.263)(R 0.239, F 0.287)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.972] [G acc: 0.000]\n",
      "1814 [D loss: (0.346)(R 0.229, F 0.462)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.061] [G acc: 0.125]\n",
      "1815 [D loss: (0.286)(R 0.215, F 0.357)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.494] [G acc: 0.125]\n",
      "1816 [D loss: (0.241)(R 0.318, F 0.165)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.764] [G acc: 0.188]\n",
      "1817 [D loss: (0.293)(R 0.193, F 0.393)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.340] [G acc: 0.125]\n",
      "1818 [D loss: (0.237)(R 0.239, F 0.234)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.397] [G acc: 0.250]\n",
      "1819 [D loss: (0.283)(R 0.210, F 0.357)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.562] [G acc: 0.000]\n",
      "1820 [D loss: (0.835)(R 0.944, F 0.726)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.387] [G acc: 0.062]\n",
      "1821 [D loss: (0.277)(R 0.163, F 0.391)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.608] [G acc: 0.125]\n",
      "1822 [D loss: (0.326)(R 0.140, F 0.512)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.595] [G acc: 0.125]\n",
      "1823 [D loss: (0.416)(R 0.316, F 0.517)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.685] [G acc: 0.125]\n",
      "1824 [D loss: (0.198)(R 0.289, F 0.106)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.451] [G acc: 0.062]\n",
      "1825 [D loss: (0.218)(R 0.411, F 0.025)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.034] [G acc: 0.062]\n",
      "1826 [D loss: (0.200)(R 0.173, F 0.228)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.612] [G acc: 0.125]\n",
      "1827 [D loss: (0.101)(R 0.154, F 0.048)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.619] [G acc: 0.250]\n",
      "1828 [D loss: (0.276)(R 0.434, F 0.118)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.848] [G acc: 0.188]\n",
      "1829 [D loss: (0.277)(R 0.224, F 0.329)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.763] [G acc: 0.062]\n",
      "1830 [D loss: (0.165)(R 0.144, F 0.186)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.847] [G acc: 0.125]\n",
      "1831 [D loss: (0.394)(R 0.490, F 0.298)] [D acc: (0.844)(0.875, 0.812)] [G loss: 7.535] [G acc: 0.062]\n",
      "1832 [D loss: (0.087)(R 0.110, F 0.064)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.103] [G acc: 0.250]\n",
      "1833 [D loss: (0.309)(R 0.222, F 0.396)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.892] [G acc: 0.062]\n",
      "1834 [D loss: (0.135)(R 0.178, F 0.091)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.431] [G acc: 0.125]\n",
      "1835 [D loss: (0.267)(R 0.198, F 0.336)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.257] [G acc: 0.062]\n",
      "1836 [D loss: (0.204)(R 0.108, F 0.300)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.901] [G acc: 0.125]\n",
      "1837 [D loss: (0.270)(R 0.526, F 0.014)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.537] [G acc: 0.250]\n",
      "1838 [D loss: (0.237)(R 0.171, F 0.303)] [D acc: (0.906)(0.938, 0.875)] [G loss: 9.137] [G acc: 0.062]\n",
      "1839 [D loss: (0.374)(R 0.242, F 0.505)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.236] [G acc: 0.125]\n",
      "1840 [D loss: (0.280)(R 0.102, F 0.459)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.617] [G acc: 0.188]\n",
      "1841 [D loss: (0.130)(R 0.085, F 0.175)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.688] [G acc: 0.125]\n",
      "1842 [D loss: (0.175)(R 0.119, F 0.231)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.583] [G acc: 0.000]\n",
      "1843 [D loss: (0.317)(R 0.240, F 0.395)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.960] [G acc: 0.062]\n",
      "1844 [D loss: (0.115)(R 0.081, F 0.149)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.174] [G acc: 0.000]\n",
      "1845 [D loss: (0.069)(R 0.128, F 0.010)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.468] [G acc: 0.062]\n",
      "1846 [D loss: (0.545)(R 0.826, F 0.264)] [D acc: (0.844)(0.812, 0.875)] [G loss: 7.688] [G acc: 0.188]\n",
      "1847 [D loss: (0.141)(R 0.089, F 0.194)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.987] [G acc: 0.062]\n",
      "1848 [D loss: (0.189)(R 0.165, F 0.214)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.825] [G acc: 0.125]\n",
      "1849 [D loss: (0.274)(R 0.094, F 0.454)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.607] [G acc: 0.125]\n",
      "1850 [D loss: (0.169)(R 0.120, F 0.218)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.981] [G acc: 0.125]\n",
      "1851 [D loss: (0.141)(R 0.101, F 0.181)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.319] [G acc: 0.125]\n",
      "1852 [D loss: (0.211)(R 0.155, F 0.267)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.544] [G acc: 0.125]\n",
      "1853 [D loss: (0.260)(R 0.089, F 0.430)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.405] [G acc: 0.062]\n",
      "1854 [D loss: (0.337)(R 0.342, F 0.331)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.273] [G acc: 0.125]\n",
      "1855 [D loss: (0.100)(R 0.141, F 0.059)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.943] [G acc: 0.125]\n",
      "1856 [D loss: (0.297)(R 0.341, F 0.252)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.253] [G acc: 0.125]\n",
      "1857 [D loss: (0.315)(R 0.107, F 0.523)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.949] [G acc: 0.062]\n",
      "1858 [D loss: (0.133)(R 0.125, F 0.142)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.624] [G acc: 0.000]\n",
      "1859 [D loss: (0.300)(R 0.293, F 0.306)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.670] [G acc: 0.062]\n",
      "1860 [D loss: (0.122)(R 0.189, F 0.055)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.144] [G acc: 0.062]\n",
      "1861 [D loss: (0.387)(R 0.456, F 0.319)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.898] [G acc: 0.000]\n",
      "1862 [D loss: (0.065)(R 0.109, F 0.022)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.813] [G acc: 0.125]\n",
      "1863 [D loss: (0.299)(R 0.219, F 0.379)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.115] [G acc: 0.062]\n",
      "1864 [D loss: (0.168)(R 0.269, F 0.067)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.077] [G acc: 0.188]\n",
      "1865 [D loss: (0.199)(R 0.093, F 0.305)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.517] [G acc: 0.188]\n",
      "1866 [D loss: (0.409)(R 0.626, F 0.192)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.838] [G acc: 0.062]\n",
      "1867 [D loss: (0.188)(R 0.109, F 0.267)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.880] [G acc: 0.125]\n",
      "1868 [D loss: (0.215)(R 0.094, F 0.336)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.966] [G acc: 0.125]\n",
      "1869 [D loss: (0.320)(R 0.417, F 0.224)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.361] [G acc: 0.062]\n",
      "1870 [D loss: (0.096)(R 0.105, F 0.087)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.068] [G acc: 0.062]\n",
      "1871 [D loss: (0.455)(R 0.443, F 0.466)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.025] [G acc: 0.188]\n",
      "1872 [D loss: (0.142)(R 0.160, F 0.123)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.924] [G acc: 0.062]\n",
      "1873 [D loss: (0.138)(R 0.257, F 0.019)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.057] [G acc: 0.250]\n",
      "1874 [D loss: (0.242)(R 0.233, F 0.251)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.226] [G acc: 0.062]\n",
      "1875 [D loss: (0.066)(R 0.102, F 0.031)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.834] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1876 [D loss: (0.131)(R 0.090, F 0.172)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.926] [G acc: 0.188]\n",
      "1877 [D loss: (0.216)(R 0.085, F 0.346)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.793] [G acc: 0.062]\n",
      "1878 [D loss: (0.941)(R 1.576, F 0.306)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.960] [G acc: 0.062]\n",
      "1879 [D loss: (0.323)(R 0.169, F 0.478)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.692] [G acc: 0.062]\n",
      "1880 [D loss: (0.100)(R 0.103, F 0.096)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.611] [G acc: 0.062]\n",
      "1881 [D loss: (0.217)(R 0.214, F 0.220)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.702] [G acc: 0.000]\n",
      "1882 [D loss: (0.066)(R 0.124, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.025] [G acc: 0.000]\n",
      "1883 [D loss: (0.174)(R 0.287, F 0.060)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.241] [G acc: 0.062]\n",
      "1884 [D loss: (0.088)(R 0.115, F 0.061)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.875] [G acc: 0.062]\n",
      "1885 [D loss: (0.245)(R 0.135, F 0.355)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.739] [G acc: 0.000]\n",
      "1886 [D loss: (0.045)(R 0.087, F 0.004)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.128] [G acc: 0.125]\n",
      "1887 [D loss: (0.171)(R 0.070, F 0.272)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.803] [G acc: 0.188]\n",
      "1888 [D loss: (0.210)(R 0.101, F 0.319)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.656] [G acc: 0.125]\n",
      "1889 [D loss: (0.437)(R 0.555, F 0.320)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.966] [G acc: 0.062]\n",
      "1890 [D loss: (0.070)(R 0.088, F 0.052)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.781] [G acc: 0.125]\n",
      "1891 [D loss: (0.294)(R 0.163, F 0.425)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.271] [G acc: 0.062]\n",
      "1892 [D loss: (0.119)(R 0.196, F 0.042)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.293] [G acc: 0.062]\n",
      "1893 [D loss: (0.093)(R 0.110, F 0.076)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.255] [G acc: 0.125]\n",
      "1894 [D loss: (0.244)(R 0.388, F 0.101)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.537] [G acc: 0.000]\n",
      "1895 [D loss: (0.101)(R 0.071, F 0.132)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.673] [G acc: 0.125]\n",
      "1896 [D loss: (0.251)(R 0.088, F 0.414)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.388] [G acc: 0.188]\n",
      "1897 [D loss: (0.046)(R 0.082, F 0.010)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.798] [G acc: 0.062]\n",
      "1898 [D loss: (0.069)(R 0.097, F 0.041)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.771] [G acc: 0.000]\n",
      "1899 [D loss: (0.234)(R 0.318, F 0.150)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.478] [G acc: 0.062]\n",
      "1900 [D loss: (0.117)(R 0.125, F 0.108)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.086] [G acc: 0.125]\n",
      "1901 [D loss: (0.087)(R 0.164, F 0.011)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.105] [G acc: 0.000]\n",
      "1902 [D loss: (0.188)(R 0.193, F 0.183)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.582] [G acc: 0.062]\n",
      "1903 [D loss: (0.335)(R 0.258, F 0.413)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.993] [G acc: 0.125]\n",
      "1904 [D loss: (0.396)(R 0.701, F 0.091)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.363] [G acc: 0.000]\n",
      "1905 [D loss: (0.216)(R 0.101, F 0.331)] [D acc: (0.938)(1.000, 0.875)] [G loss: 10.249] [G acc: 0.000]\n",
      "1906 [D loss: (0.049)(R 0.080, F 0.019)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.882] [G acc: 0.000]\n",
      "1907 [D loss: (0.188)(R 0.116, F 0.260)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.652] [G acc: 0.062]\n",
      "1908 [D loss: (0.047)(R 0.065, F 0.030)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.899] [G acc: 0.062]\n",
      "1909 [D loss: (0.173)(R 0.098, F 0.249)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.847] [G acc: 0.062]\n",
      "1910 [D loss: (0.164)(R 0.192, F 0.135)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.479] [G acc: 0.125]\n",
      "1911 [D loss: (0.331)(R 0.494, F 0.167)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.441] [G acc: 0.125]\n",
      "1912 [D loss: (0.125)(R 0.241, F 0.009)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.583] [G acc: 0.188]\n",
      "1913 [D loss: (0.100)(R 0.090, F 0.110)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.985] [G acc: 0.062]\n",
      "1914 [D loss: (0.212)(R 0.109, F 0.315)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.503] [G acc: 0.125]\n",
      "1915 [D loss: (0.146)(R 0.061, F 0.231)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.948] [G acc: 0.000]\n",
      "1916 [D loss: (0.305)(R 0.246, F 0.363)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.126] [G acc: 0.062]\n",
      "1917 [D loss: (0.172)(R 0.171, F 0.172)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.967] [G acc: 0.125]\n",
      "1918 [D loss: (0.159)(R 0.057, F 0.262)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.933] [G acc: 0.000]\n",
      "1919 [D loss: (0.146)(R 0.063, F 0.230)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.299] [G acc: 0.000]\n",
      "1920 [D loss: (0.178)(R 0.066, F 0.291)] [D acc: (0.938)(1.000, 0.875)] [G loss: 8.077] [G acc: 0.000]\n",
      "1921 [D loss: (0.154)(R 0.087, F 0.220)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.066] [G acc: 0.000]\n",
      "1922 [D loss: (0.151)(R 0.159, F 0.143)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.334] [G acc: 0.062]\n",
      "1923 [D loss: (0.368)(R 0.330, F 0.405)] [D acc: (0.812)(0.812, 0.812)] [G loss: 7.666] [G acc: 0.188]\n",
      "1924 [D loss: (0.099)(R 0.133, F 0.066)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.887] [G acc: 0.125]\n",
      "1925 [D loss: (0.129)(R 0.099, F 0.160)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.899] [G acc: 0.125]\n",
      "1926 [D loss: (0.163)(R 0.284, F 0.041)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.539] [G acc: 0.125]\n",
      "1927 [D loss: (0.112)(R 0.076, F 0.149)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.272] [G acc: 0.125]\n",
      "1928 [D loss: (0.151)(R 0.285, F 0.016)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.476] [G acc: 0.125]\n",
      "1929 [D loss: (0.154)(R 0.047, F 0.260)] [D acc: (0.969)(1.000, 0.938)] [G loss: 10.057] [G acc: 0.000]\n",
      "1930 [D loss: (0.190)(R 0.108, F 0.271)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.055] [G acc: 0.000]\n",
      "1931 [D loss: (0.066)(R 0.085, F 0.046)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.564] [G acc: 0.000]\n",
      "1932 [D loss: (0.156)(R 0.200, F 0.111)] [D acc: (0.906)(0.875, 0.938)] [G loss: 8.756] [G acc: 0.125]\n",
      "1933 [D loss: (0.069)(R 0.095, F 0.044)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.864] [G acc: 0.000]\n",
      "1934 [D loss: (0.128)(R 0.113, F 0.143)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.155] [G acc: 0.062]\n",
      "1935 [D loss: (0.187)(R 0.180, F 0.194)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.838] [G acc: 0.125]\n",
      "1936 [D loss: (0.201)(R 0.397, F 0.005)] [D acc: (0.969)(0.938, 1.000)] [G loss: 7.008] [G acc: 0.125]\n",
      "1937 [D loss: (0.155)(R 0.066, F 0.244)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.351] [G acc: 0.062]\n",
      "1938 [D loss: (0.198)(R 0.086, F 0.309)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.717] [G acc: 0.188]\n",
      "1939 [D loss: (0.189)(R 0.075, F 0.302)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.090] [G acc: 0.000]\n",
      "1940 [D loss: (0.123)(R 0.092, F 0.155)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.196] [G acc: 0.125]\n",
      "1941 [D loss: (0.090)(R 0.081, F 0.100)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.067] [G acc: 0.000]\n",
      "1942 [D loss: (0.071)(R 0.131, F 0.011)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.214] [G acc: 0.000]\n",
      "1943 [D loss: (0.270)(R 0.529, F 0.011)] [D acc: (0.938)(0.875, 1.000)] [G loss: 6.485] [G acc: 0.188]\n",
      "1944 [D loss: (0.755)(R 1.430, F 0.079)] [D acc: (0.875)(0.750, 1.000)] [G loss: 6.652] [G acc: 0.062]\n",
      "1945 [D loss: (0.043)(R 0.082, F 0.005)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.221] [G acc: 0.000]\n",
      "1946 [D loss: (0.287)(R 0.217, F 0.356)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.127] [G acc: 0.000]\n",
      "1947 [D loss: (0.222)(R 0.421, F 0.023)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.927] [G acc: 0.062]\n",
      "1948 [D loss: (0.153)(R 0.072, F 0.234)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.853] [G acc: 0.125]\n",
      "1949 [D loss: (0.245)(R 0.065, F 0.425)] [D acc: (0.906)(1.000, 0.812)] [G loss: 10.778] [G acc: 0.000]\n",
      "1950 [D loss: (0.134)(R 0.075, F 0.193)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.567] [G acc: 0.000]\n",
      "1951 [D loss: (0.145)(R 0.105, F 0.185)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.367] [G acc: 0.062]\n",
      "1952 [D loss: (0.212)(R 0.196, F 0.228)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.044] [G acc: 0.062]\n",
      "1953 [D loss: (0.108)(R 0.066, F 0.150)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.453] [G acc: 0.125]\n",
      "1954 [D loss: (0.102)(R 0.070, F 0.134)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.374] [G acc: 0.062]\n",
      "1955 [D loss: (0.105)(R 0.081, F 0.128)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.483] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1956 [D loss: (0.379)(R 0.183, F 0.575)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.259] [G acc: 0.062]\n",
      "1957 [D loss: (0.884)(R 1.602, F 0.167)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.190] [G acc: 0.000]\n",
      "1958 [D loss: (0.356)(R 0.295, F 0.416)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.138] [G acc: 0.188]\n",
      "1959 [D loss: (0.237)(R 0.356, F 0.119)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.424] [G acc: 0.062]\n",
      "1960 [D loss: (0.129)(R 0.102, F 0.157)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.400] [G acc: 0.062]\n",
      "1961 [D loss: (0.088)(R 0.112, F 0.064)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.189] [G acc: 0.125]\n",
      "1962 [D loss: (0.078)(R 0.071, F 0.085)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.396] [G acc: 0.125]\n",
      "1963 [D loss: (0.125)(R 0.116, F 0.134)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.916] [G acc: 0.062]\n",
      "1964 [D loss: (0.036)(R 0.063, F 0.008)] [D acc: (1.000)(1.000, 1.000)] [G loss: 8.782] [G acc: 0.000]\n",
      "1965 [D loss: (0.447)(R 0.729, F 0.165)] [D acc: (0.906)(0.875, 0.938)] [G loss: 6.641] [G acc: 0.188]\n",
      "1966 [D loss: (0.133)(R 0.116, F 0.150)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.407] [G acc: 0.188]\n",
      "1967 [D loss: (0.252)(R 0.341, F 0.164)] [D acc: (0.875)(0.812, 0.938)] [G loss: 7.603] [G acc: 0.000]\n",
      "1968 [D loss: (0.059)(R 0.082, F 0.036)] [D acc: (1.000)(1.000, 1.000)] [G loss: 9.126] [G acc: 0.000]\n",
      "1969 [D loss: (0.123)(R 0.048, F 0.197)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.065] [G acc: 0.188]\n",
      "1970 [D loss: (0.150)(R 0.099, F 0.202)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.217] [G acc: 0.062]\n",
      "1971 [D loss: (0.614)(R 0.806, F 0.421)] [D acc: (0.812)(0.812, 0.812)] [G loss: 6.771] [G acc: 0.188]\n",
      "1972 [D loss: (0.236)(R 0.101, F 0.371)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.238] [G acc: 0.062]\n",
      "1973 [D loss: (0.050)(R 0.078, F 0.022)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.414] [G acc: 0.125]\n",
      "1974 [D loss: (0.382)(R 0.101, F 0.663)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.969] [G acc: 0.125]\n",
      "1975 [D loss: (0.161)(R 0.105, F 0.218)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.304] [G acc: 0.250]\n",
      "1976 [D loss: (0.088)(R 0.088, F 0.088)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.736] [G acc: 0.000]\n",
      "1977 [D loss: (0.113)(R 0.081, F 0.145)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.086] [G acc: 0.062]\n",
      "1978 [D loss: (0.285)(R 0.184, F 0.387)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.411] [G acc: 0.062]\n",
      "1979 [D loss: (0.049)(R 0.046, F 0.053)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.325] [G acc: 0.188]\n",
      "1980 [D loss: (0.600)(R 1.004, F 0.197)] [D acc: (0.812)(0.812, 0.812)] [G loss: 6.638] [G acc: 0.125]\n",
      "1981 [D loss: (0.217)(R 0.187, F 0.246)] [D acc: (0.906)(0.938, 0.875)] [G loss: 8.151] [G acc: 0.000]\n",
      "1982 [D loss: (0.120)(R 0.067, F 0.174)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.524] [G acc: 0.062]\n",
      "1983 [D loss: (0.115)(R 0.156, F 0.073)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.253] [G acc: 0.062]\n",
      "1984 [D loss: (0.220)(R 0.136, F 0.304)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.444] [G acc: 0.125]\n",
      "1985 [D loss: (0.093)(R 0.121, F 0.066)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.198] [G acc: 0.125]\n",
      "1986 [D loss: (0.230)(R 0.416, F 0.043)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.423] [G acc: 0.125]\n",
      "1987 [D loss: (0.399)(R 0.433, F 0.365)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.657] [G acc: 0.125]\n",
      "1988 [D loss: (0.241)(R 0.075, F 0.406)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.669] [G acc: 0.062]\n",
      "1989 [D loss: (0.130)(R 0.240, F 0.019)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.932] [G acc: 0.188]\n",
      "1990 [D loss: (0.105)(R 0.068, F 0.142)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.525] [G acc: 0.062]\n",
      "1991 [D loss: (0.181)(R 0.090, F 0.273)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.809] [G acc: 0.000]\n",
      "1992 [D loss: (0.237)(R 0.283, F 0.192)] [D acc: (0.938)(0.938, 0.938)] [G loss: 7.343] [G acc: 0.125]\n",
      "1993 [D loss: (0.190)(R 0.126, F 0.254)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.025] [G acc: 0.062]\n",
      "1994 [D loss: (0.156)(R 0.078, F 0.235)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.430] [G acc: 0.125]\n",
      "1995 [D loss: (0.041)(R 0.064, F 0.017)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.152] [G acc: 0.125]\n",
      "1996 [D loss: (0.171)(R 0.056, F 0.287)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.256] [G acc: 0.000]\n",
      "1997 [D loss: (0.528)(R 0.827, F 0.230)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.349] [G acc: 0.125]\n",
      "1998 [D loss: (0.127)(R 0.089, F 0.166)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.749] [G acc: 0.125]\n",
      "1999 [D loss: (0.257)(R 0.067, F 0.446)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.778] [G acc: 0.125]\n",
      "2000 [D loss: (0.110)(R 0.141, F 0.079)] [D acc: (0.969)(0.938, 1.000)] [G loss: 8.666] [G acc: 0.125]\n",
      "2001 [D loss: (0.575)(R 0.950, F 0.200)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.983] [G acc: 0.125]\n",
      "2002 [D loss: (0.143)(R 0.071, F 0.215)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.828] [G acc: 0.062]\n",
      "2003 [D loss: (0.221)(R 0.099, F 0.343)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.913] [G acc: 0.125]\n",
      "2004 [D loss: (0.148)(R 0.056, F 0.240)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.237] [G acc: 0.062]\n",
      "2005 [D loss: (0.176)(R 0.080, F 0.272)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.503] [G acc: 0.062]\n",
      "2006 [D loss: (0.051)(R 0.077, F 0.024)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.742] [G acc: 0.062]\n",
      "2007 [D loss: (0.199)(R 0.074, F 0.324)] [D acc: (0.938)(1.000, 0.875)] [G loss: 9.017] [G acc: 0.000]\n",
      "2008 [D loss: (0.632)(R 0.730, F 0.534)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.858] [G acc: 0.062]\n",
      "2009 [D loss: (0.150)(R 0.093, F 0.207)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.924] [G acc: 0.125]\n",
      "2010 [D loss: (0.278)(R 0.513, F 0.043)] [D acc: (0.938)(0.875, 1.000)] [G loss: 11.414] [G acc: 0.062]\n",
      "2011 [D loss: (0.260)(R 0.173, F 0.348)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.845] [G acc: 0.125]\n",
      "2012 [D loss: (0.889)(R 1.519, F 0.258)] [D acc: (0.750)(0.688, 0.812)] [G loss: 8.502] [G acc: 0.062]\n",
      "2013 [D loss: (0.142)(R 0.078, F 0.205)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.335] [G acc: 0.000]\n",
      "2014 [D loss: (0.128)(R 0.069, F 0.187)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.442] [G acc: 0.000]\n",
      "2015 [D loss: (0.348)(R 0.097, F 0.600)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.952] [G acc: 0.062]\n",
      "2016 [D loss: (0.726)(R 0.510, F 0.942)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.562] [G acc: 0.062]\n",
      "2017 [D loss: (0.199)(R 0.086, F 0.312)] [D acc: (0.875)(1.000, 0.750)] [G loss: 10.190] [G acc: 0.000]\n",
      "2018 [D loss: (0.094)(R 0.083, F 0.104)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.591] [G acc: 0.125]\n",
      "2019 [D loss: (0.155)(R 0.130, F 0.179)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.684] [G acc: 0.062]\n",
      "2020 [D loss: (0.192)(R 0.366, F 0.018)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.093] [G acc: 0.125]\n",
      "2021 [D loss: (0.481)(R 0.332, F 0.630)] [D acc: (0.750)(0.875, 0.625)] [G loss: 7.256] [G acc: 0.062]\n",
      "2022 [D loss: (0.152)(R 0.183, F 0.122)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.140] [G acc: 0.125]\n",
      "2023 [D loss: (0.046)(R 0.074, F 0.017)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.282] [G acc: 0.062]\n",
      "2024 [D loss: (0.168)(R 0.267, F 0.068)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.557] [G acc: 0.125]\n",
      "2025 [D loss: (0.498)(R 0.636, F 0.360)] [D acc: (0.812)(0.750, 0.875)] [G loss: 3.420] [G acc: 0.188]\n",
      "2026 [D loss: (0.364)(R 0.370, F 0.358)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.220] [G acc: 0.188]\n",
      "2027 [D loss: (0.124)(R 0.108, F 0.140)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.312] [G acc: 0.125]\n",
      "2028 [D loss: (0.118)(R 0.078, F 0.158)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.210] [G acc: 0.062]\n",
      "2029 [D loss: (0.358)(R 0.157, F 0.560)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.911] [G acc: 0.125]\n",
      "2030 [D loss: (0.232)(R 0.101, F 0.363)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.441] [G acc: 0.188]\n",
      "2031 [D loss: (0.203)(R 0.290, F 0.117)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.503] [G acc: 0.250]\n",
      "2032 [D loss: (0.435)(R 0.506, F 0.364)] [D acc: (0.844)(0.812, 0.875)] [G loss: 7.940] [G acc: 0.000]\n",
      "2033 [D loss: (0.086)(R 0.083, F 0.089)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.153] [G acc: 0.062]\n",
      "2034 [D loss: (0.435)(R 0.716, F 0.154)] [D acc: (0.906)(0.875, 0.938)] [G loss: 7.081] [G acc: 0.062]\n",
      "2035 [D loss: (0.199)(R 0.134, F 0.264)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.862] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2036 [D loss: (0.318)(R 0.152, F 0.483)] [D acc: (0.844)(0.938, 0.750)] [G loss: 7.336] [G acc: 0.000]\n",
      "2037 [D loss: (0.293)(R 0.204, F 0.382)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.836] [G acc: 0.188]\n",
      "2038 [D loss: (0.264)(R 0.101, F 0.426)] [D acc: (0.906)(1.000, 0.812)] [G loss: 8.324] [G acc: 0.062]\n",
      "2039 [D loss: (0.438)(R 0.199, F 0.678)] [D acc: (0.875)(0.938, 0.812)] [G loss: 8.520] [G acc: 0.000]\n",
      "2040 [D loss: (0.212)(R 0.086, F 0.339)] [D acc: (0.938)(1.000, 0.875)] [G loss: 7.939] [G acc: 0.062]\n",
      "2041 [D loss: (0.266)(R 0.448, F 0.084)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.963] [G acc: 0.188]\n",
      "2042 [D loss: (0.051)(R 0.080, F 0.023)] [D acc: (1.000)(1.000, 1.000)] [G loss: 7.396] [G acc: 0.125]\n",
      "2043 [D loss: (0.229)(R 0.355, F 0.103)] [D acc: (0.938)(0.938, 0.938)] [G loss: 8.514] [G acc: 0.062]\n",
      "2044 [D loss: (0.466)(R 0.376, F 0.556)] [D acc: (0.812)(0.812, 0.812)] [G loss: 9.482] [G acc: 0.000]\n",
      "2045 [D loss: (0.222)(R 0.217, F 0.227)] [D acc: (0.875)(0.875, 0.875)] [G loss: 8.763] [G acc: 0.000]\n",
      "2046 [D loss: (0.279)(R 0.307, F 0.250)] [D acc: (0.844)(0.812, 0.875)] [G loss: 7.048] [G acc: 0.188]\n",
      "2047 [D loss: (0.091)(R 0.170, F 0.012)] [D acc: (0.969)(0.938, 1.000)] [G loss: 9.519] [G acc: 0.000]\n",
      "2048 [D loss: (0.066)(R 0.078, F 0.055)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.814] [G acc: 0.250]\n",
      "2049 [D loss: (0.241)(R 0.313, F 0.170)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.652] [G acc: 0.062]\n",
      "2050 [D loss: (0.089)(R 0.114, F 0.065)] [D acc: (0.969)(1.000, 0.938)] [G loss: 7.107] [G acc: 0.125]\n",
      "2051 [D loss: (0.305)(R 0.169, F 0.441)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.039] [G acc: 0.125]\n",
      "2052 [D loss: (0.566)(R 0.677, F 0.454)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.809] [G acc: 0.062]\n",
      "2053 [D loss: (0.392)(R 0.637, F 0.147)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.785] [G acc: 0.250]\n",
      "2054 [D loss: (0.228)(R 0.144, F 0.311)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.625] [G acc: 0.188]\n",
      "2055 [D loss: (0.140)(R 0.109, F 0.170)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.258] [G acc: 0.062]\n",
      "2056 [D loss: (0.074)(R 0.111, F 0.038)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.919] [G acc: 0.125]\n",
      "2057 [D loss: (0.199)(R 0.183, F 0.214)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.662] [G acc: 0.125]\n",
      "2058 [D loss: (0.544)(R 0.826, F 0.262)] [D acc: (0.844)(0.875, 0.812)] [G loss: 8.345] [G acc: 0.000]\n",
      "2059 [D loss: (0.333)(R 0.264, F 0.402)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.462] [G acc: 0.188]\n",
      "2060 [D loss: (0.450)(R 0.093, F 0.807)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.709] [G acc: 0.125]\n",
      "2061 [D loss: (0.112)(R 0.132, F 0.093)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.545] [G acc: 0.062]\n",
      "2062 [D loss: (0.088)(R 0.139, F 0.037)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.933] [G acc: 0.250]\n",
      "2063 [D loss: (0.812)(R 1.116, F 0.507)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.362] [G acc: 0.125]\n",
      "2064 [D loss: (0.175)(R 0.077, F 0.273)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.337] [G acc: 0.188]\n",
      "2065 [D loss: (0.166)(R 0.092, F 0.241)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.737] [G acc: 0.188]\n",
      "2066 [D loss: (0.262)(R 0.138, F 0.386)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.707] [G acc: 0.125]\n",
      "2067 [D loss: (0.233)(R 0.263, F 0.204)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.471] [G acc: 0.000]\n",
      "2068 [D loss: (0.130)(R 0.140, F 0.120)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.692] [G acc: 0.125]\n",
      "2069 [D loss: (0.390)(R 0.338, F 0.441)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.530] [G acc: 0.125]\n",
      "2070 [D loss: (0.264)(R 0.098, F 0.430)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.398] [G acc: 0.062]\n",
      "2071 [D loss: (0.171)(R 0.224, F 0.119)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.700] [G acc: 0.125]\n",
      "2072 [D loss: (0.480)(R 0.438, F 0.522)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.818] [G acc: 0.125]\n",
      "2073 [D loss: (0.221)(R 0.412, F 0.030)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.156] [G acc: 0.250]\n",
      "2074 [D loss: (0.821)(R 1.391, F 0.251)] [D acc: (0.844)(0.812, 0.875)] [G loss: 5.348] [G acc: 0.188]\n",
      "2075 [D loss: (0.092)(R 0.115, F 0.068)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.949] [G acc: 0.125]\n",
      "2076 [D loss: (0.059)(R 0.069, F 0.049)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.934] [G acc: 0.312]\n",
      "2077 [D loss: (0.245)(R 0.211, F 0.279)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.847] [G acc: 0.188]\n",
      "2078 [D loss: (0.360)(R 0.086, F 0.634)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.090] [G acc: 0.125]\n",
      "2079 [D loss: (0.329)(R 0.105, F 0.553)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.269] [G acc: 0.000]\n",
      "2080 [D loss: (0.648)(R 0.650, F 0.645)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.790] [G acc: 0.062]\n",
      "2081 [D loss: (0.246)(R 0.158, F 0.334)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.962] [G acc: 0.312]\n",
      "2082 [D loss: (0.346)(R 0.485, F 0.207)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.466] [G acc: 0.125]\n",
      "2083 [D loss: (0.506)(R 0.778, F 0.235)] [D acc: (0.875)(0.875, 0.875)] [G loss: 7.909] [G acc: 0.062]\n",
      "2084 [D loss: (0.527)(R 0.866, F 0.189)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.115] [G acc: 0.000]\n",
      "2085 [D loss: (0.543)(R 0.782, F 0.304)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.338] [G acc: 0.125]\n",
      "2086 [D loss: (0.314)(R 0.209, F 0.419)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.537] [G acc: 0.062]\n",
      "2087 [D loss: (0.081)(R 0.110, F 0.052)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.105] [G acc: 0.062]\n",
      "2088 [D loss: (0.226)(R 0.157, F 0.294)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.302] [G acc: 0.125]\n",
      "2089 [D loss: (0.126)(R 0.089, F 0.163)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.407] [G acc: 0.250]\n",
      "2090 [D loss: (0.471)(R 0.474, F 0.468)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.638] [G acc: 0.125]\n",
      "2091 [D loss: (0.518)(R 0.730, F 0.307)] [D acc: (0.812)(0.750, 0.875)] [G loss: 7.059] [G acc: 0.062]\n",
      "2092 [D loss: (0.253)(R 0.115, F 0.391)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.526] [G acc: 0.250]\n",
      "2093 [D loss: (0.192)(R 0.348, F 0.036)] [D acc: (0.969)(0.938, 1.000)] [G loss: 6.165] [G acc: 0.188]\n",
      "2094 [D loss: (0.283)(R 0.174, F 0.393)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.721] [G acc: 0.125]\n",
      "2095 [D loss: (0.143)(R 0.078, F 0.209)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.635] [G acc: 0.062]\n",
      "2096 [D loss: (0.501)(R 0.544, F 0.457)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.322] [G acc: 0.250]\n",
      "2097 [D loss: (0.334)(R 0.484, F 0.183)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.430] [G acc: 0.000]\n",
      "2098 [D loss: (0.403)(R 0.097, F 0.708)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.066] [G acc: 0.250]\n",
      "2099 [D loss: (0.970)(R 1.709, F 0.232)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.802] [G acc: 0.062]\n",
      "2100 [D loss: (0.225)(R 0.230, F 0.219)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.408] [G acc: 0.188]\n",
      "2101 [D loss: (0.399)(R 0.270, F 0.528)] [D acc: (0.844)(0.938, 0.750)] [G loss: 8.176] [G acc: 0.125]\n",
      "2102 [D loss: (0.226)(R 0.106, F 0.347)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.999] [G acc: 0.062]\n",
      "2103 [D loss: (0.682)(R 0.565, F 0.800)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.864] [G acc: 0.062]\n",
      "2104 [D loss: (0.529)(R 0.570, F 0.487)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.216] [G acc: 0.062]\n",
      "2105 [D loss: (0.515)(R 0.101, F 0.928)] [D acc: (0.812)(1.000, 0.625)] [G loss: 6.069] [G acc: 0.000]\n",
      "2106 [D loss: (0.629)(R 0.702, F 0.557)] [D acc: (0.781)(0.875, 0.688)] [G loss: 6.525] [G acc: 0.188]\n",
      "2107 [D loss: (0.296)(R 0.114, F 0.479)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.384] [G acc: 0.125]\n",
      "2108 [D loss: (0.477)(R 0.445, F 0.509)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.369] [G acc: 0.188]\n",
      "2109 [D loss: (0.258)(R 0.187, F 0.329)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.155] [G acc: 0.062]\n",
      "2110 [D loss: (0.324)(R 0.107, F 0.541)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.968] [G acc: 0.188]\n",
      "2111 [D loss: (0.221)(R 0.108, F 0.334)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.603] [G acc: 0.188]\n",
      "2112 [D loss: (0.163)(R 0.286, F 0.041)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.324] [G acc: 0.250]\n",
      "2113 [D loss: (0.413)(R 0.494, F 0.332)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.646] [G acc: 0.125]\n",
      "2114 [D loss: (0.636)(R 1.065, F 0.207)] [D acc: (0.812)(0.750, 0.875)] [G loss: 7.091] [G acc: 0.062]\n",
      "2115 [D loss: (0.226)(R 0.107, F 0.345)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.896] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2116 [D loss: (0.422)(R 0.172, F 0.672)] [D acc: (0.844)(1.000, 0.688)] [G loss: 5.400] [G acc: 0.125]\n",
      "2117 [D loss: (0.152)(R 0.125, F 0.179)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.133] [G acc: 0.188]\n",
      "2118 [D loss: (0.109)(R 0.105, F 0.113)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.358] [G acc: 0.125]\n",
      "2119 [D loss: (0.152)(R 0.133, F 0.172)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.150] [G acc: 0.062]\n",
      "2120 [D loss: (0.566)(R 0.736, F 0.395)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.777] [G acc: 0.125]\n",
      "2121 [D loss: (0.256)(R 0.302, F 0.209)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.566] [G acc: 0.312]\n",
      "2122 [D loss: (0.384)(R 0.434, F 0.334)] [D acc: (0.844)(0.812, 0.875)] [G loss: 6.512] [G acc: 0.188]\n",
      "2123 [D loss: (0.300)(R 0.235, F 0.364)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.816] [G acc: 0.125]\n",
      "2124 [D loss: (0.179)(R 0.236, F 0.122)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.121] [G acc: 0.250]\n",
      "2125 [D loss: (0.246)(R 0.163, F 0.330)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.556] [G acc: 0.125]\n",
      "2126 [D loss: (0.518)(R 0.833, F 0.202)] [D acc: (0.875)(0.812, 0.938)] [G loss: 5.776] [G acc: 0.062]\n",
      "2127 [D loss: (0.720)(R 0.714, F 0.725)] [D acc: (0.719)(0.812, 0.625)] [G loss: 5.413] [G acc: 0.062]\n",
      "2128 [D loss: (0.522)(R 0.678, F 0.365)] [D acc: (0.812)(0.750, 0.875)] [G loss: 4.173] [G acc: 0.250]\n",
      "2129 [D loss: (0.330)(R 0.297, F 0.362)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.054] [G acc: 0.188]\n",
      "2130 [D loss: (0.195)(R 0.159, F 0.230)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.110] [G acc: 0.188]\n",
      "2131 [D loss: (0.233)(R 0.111, F 0.354)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.029] [G acc: 0.125]\n",
      "2132 [D loss: (0.337)(R 0.392, F 0.283)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.986] [G acc: 0.125]\n",
      "2133 [D loss: (0.206)(R 0.262, F 0.151)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.324] [G acc: 0.188]\n",
      "2134 [D loss: (0.101)(R 0.173, F 0.028)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.713] [G acc: 0.188]\n",
      "2135 [D loss: (0.502)(R 0.448, F 0.557)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.287] [G acc: 0.312]\n",
      "2136 [D loss: (0.409)(R 0.257, F 0.560)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.122] [G acc: 0.000]\n",
      "2137 [D loss: (0.416)(R 0.137, F 0.694)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.570] [G acc: 0.000]\n",
      "2138 [D loss: (0.430)(R 0.515, F 0.345)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.570] [G acc: 0.188]\n",
      "2139 [D loss: (0.469)(R 0.600, F 0.338)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.331] [G acc: 0.125]\n",
      "2140 [D loss: (0.290)(R 0.143, F 0.436)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.783] [G acc: 0.188]\n",
      "2141 [D loss: (0.403)(R 0.526, F 0.280)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.002] [G acc: 0.438]\n",
      "2142 [D loss: (0.319)(R 0.206, F 0.431)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.678] [G acc: 0.062]\n",
      "2143 [D loss: (0.399)(R 0.165, F 0.634)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.766] [G acc: 0.250]\n",
      "2144 [D loss: (0.265)(R 0.298, F 0.232)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.292] [G acc: 0.250]\n",
      "2145 [D loss: (0.160)(R 0.144, F 0.177)] [D acc: (0.969)(1.000, 0.938)] [G loss: 8.189] [G acc: 0.000]\n",
      "2146 [D loss: (0.626)(R 0.730, F 0.523)] [D acc: (0.781)(0.812, 0.750)] [G loss: 5.123] [G acc: 0.188]\n",
      "2147 [D loss: (0.217)(R 0.140, F 0.295)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.908] [G acc: 0.250]\n",
      "2148 [D loss: (0.623)(R 1.050, F 0.197)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.329] [G acc: 0.125]\n",
      "2149 [D loss: (0.647)(R 0.641, F 0.654)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.537] [G acc: 0.250]\n",
      "2150 [D loss: (0.319)(R 0.129, F 0.508)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.169] [G acc: 0.125]\n",
      "2151 [D loss: (0.318)(R 0.136, F 0.500)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.214] [G acc: 0.188]\n",
      "2152 [D loss: (0.325)(R 0.232, F 0.418)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.459] [G acc: 0.188]\n",
      "2153 [D loss: (0.374)(R 0.477, F 0.271)] [D acc: (0.812)(0.812, 0.812)] [G loss: 5.061] [G acc: 0.188]\n",
      "2154 [D loss: (0.208)(R 0.106, F 0.311)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.525] [G acc: 0.125]\n",
      "2155 [D loss: (0.334)(R 0.463, F 0.206)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.718] [G acc: 0.188]\n",
      "2156 [D loss: (0.351)(R 0.123, F 0.578)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.093] [G acc: 0.062]\n",
      "2157 [D loss: (0.169)(R 0.132, F 0.205)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.518] [G acc: 0.125]\n",
      "2158 [D loss: (0.228)(R 0.185, F 0.271)] [D acc: (0.875)(0.938, 0.812)] [G loss: 7.167] [G acc: 0.062]\n",
      "2159 [D loss: (0.328)(R 0.252, F 0.403)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.218] [G acc: 0.125]\n",
      "2160 [D loss: (0.244)(R 0.158, F 0.330)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.370] [G acc: 0.125]\n",
      "2161 [D loss: (0.343)(R 0.501, F 0.185)] [D acc: (0.844)(0.750, 0.938)] [G loss: 6.507] [G acc: 0.125]\n",
      "2162 [D loss: (0.228)(R 0.146, F 0.309)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.033] [G acc: 0.188]\n",
      "2163 [D loss: (0.143)(R 0.119, F 0.167)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.557] [G acc: 0.188]\n",
      "2164 [D loss: (0.323)(R 0.150, F 0.497)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.268] [G acc: 0.125]\n",
      "2165 [D loss: (0.318)(R 0.314, F 0.321)] [D acc: (0.875)(0.875, 0.875)] [G loss: 6.701] [G acc: 0.062]\n",
      "2166 [D loss: (0.301)(R 0.347, F 0.254)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.802] [G acc: 0.250]\n",
      "2167 [D loss: (0.564)(R 0.830, F 0.299)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.593] [G acc: 0.125]\n",
      "2168 [D loss: (0.255)(R 0.228, F 0.283)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.077] [G acc: 0.188]\n",
      "2169 [D loss: (0.363)(R 0.644, F 0.083)] [D acc: (0.875)(0.750, 1.000)] [G loss: 3.715] [G acc: 0.188]\n",
      "2170 [D loss: (0.257)(R 0.262, F 0.252)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.020] [G acc: 0.188]\n",
      "2171 [D loss: (0.283)(R 0.236, F 0.329)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.121] [G acc: 0.125]\n",
      "2172 [D loss: (0.239)(R 0.322, F 0.155)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.684] [G acc: 0.062]\n",
      "2173 [D loss: (0.228)(R 0.283, F 0.174)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.379] [G acc: 0.188]\n",
      "2174 [D loss: (0.176)(R 0.153, F 0.199)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.080] [G acc: 0.188]\n",
      "2175 [D loss: (0.644)(R 0.974, F 0.315)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.759] [G acc: 0.062]\n",
      "2176 [D loss: (0.134)(R 0.138, F 0.129)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.281] [G acc: 0.125]\n",
      "2177 [D loss: (0.578)(R 0.939, F 0.217)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.386] [G acc: 0.062]\n",
      "2178 [D loss: (0.238)(R 0.153, F 0.322)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.948] [G acc: 0.250]\n",
      "2179 [D loss: (0.283)(R 0.190, F 0.375)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.685] [G acc: 0.250]\n",
      "2180 [D loss: (0.343)(R 0.139, F 0.547)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.796] [G acc: 0.125]\n",
      "2181 [D loss: (0.402)(R 0.456, F 0.349)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.419] [G acc: 0.125]\n",
      "2182 [D loss: (0.510)(R 0.426, F 0.595)] [D acc: (0.781)(0.875, 0.688)] [G loss: 5.263] [G acc: 0.062]\n",
      "2183 [D loss: (0.139)(R 0.142, F 0.135)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.978] [G acc: 0.188]\n",
      "2184 [D loss: (0.238)(R 0.298, F 0.178)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.660] [G acc: 0.312]\n",
      "2185 [D loss: (0.453)(R 0.144, F 0.762)] [D acc: (0.844)(1.000, 0.688)] [G loss: 5.516] [G acc: 0.000]\n",
      "2186 [D loss: (0.324)(R 0.218, F 0.429)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.560] [G acc: 0.188]\n",
      "2187 [D loss: (0.593)(R 0.199, F 0.987)] [D acc: (0.719)(0.938, 0.500)] [G loss: 5.885] [G acc: 0.062]\n",
      "2188 [D loss: (0.496)(R 0.777, F 0.215)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.517] [G acc: 0.125]\n",
      "2189 [D loss: (0.219)(R 0.230, F 0.208)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.653] [G acc: 0.188]\n",
      "2190 [D loss: (0.126)(R 0.161, F 0.090)] [D acc: (0.969)(1.000, 0.938)] [G loss: 9.070] [G acc: 0.000]\n",
      "2191 [D loss: (0.281)(R 0.469, F 0.093)] [D acc: (0.938)(0.875, 1.000)] [G loss: 7.033] [G acc: 0.125]\n",
      "2192 [D loss: (0.373)(R 0.224, F 0.522)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.158] [G acc: 0.125]\n",
      "2193 [D loss: (0.473)(R 0.406, F 0.539)] [D acc: (0.812)(0.875, 0.750)] [G loss: 6.054] [G acc: 0.062]\n",
      "2194 [D loss: (0.499)(R 0.146, F 0.852)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.371] [G acc: 0.188]\n",
      "2195 [D loss: (0.301)(R 0.392, F 0.210)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.082] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196 [D loss: (0.556)(R 0.288, F 0.825)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.933] [G acc: 0.312]\n",
      "2197 [D loss: (0.201)(R 0.227, F 0.174)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.112] [G acc: 0.125]\n",
      "2198 [D loss: (0.120)(R 0.120, F 0.119)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.058] [G acc: 0.250]\n",
      "2199 [D loss: (0.645)(R 0.778, F 0.513)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.910] [G acc: 0.000]\n",
      "2200 [D loss: (0.359)(R 0.399, F 0.318)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.768] [G acc: 0.188]\n",
      "2201 [D loss: (0.167)(R 0.280, F 0.055)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.090] [G acc: 0.188]\n",
      "2202 [D loss: (0.141)(R 0.132, F 0.150)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.603] [G acc: 0.125]\n",
      "2203 [D loss: (0.524)(R 0.697, F 0.351)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.752] [G acc: 0.312]\n",
      "2204 [D loss: (0.430)(R 0.190, F 0.671)] [D acc: (0.781)(0.938, 0.625)] [G loss: 5.482] [G acc: 0.125]\n",
      "2205 [D loss: (0.396)(R 0.447, F 0.346)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.264] [G acc: 0.188]\n",
      "2206 [D loss: (0.173)(R 0.118, F 0.229)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.471] [G acc: 0.375]\n",
      "2207 [D loss: (0.236)(R 0.311, F 0.161)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.062] [G acc: 0.125]\n",
      "2208 [D loss: (0.450)(R 0.219, F 0.682)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.598] [G acc: 0.188]\n",
      "2209 [D loss: (0.301)(R 0.402, F 0.200)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.666] [G acc: 0.188]\n",
      "2210 [D loss: (0.438)(R 0.270, F 0.606)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.039] [G acc: 0.125]\n",
      "2211 [D loss: (0.182)(R 0.138, F 0.226)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.518] [G acc: 0.062]\n",
      "2212 [D loss: (0.184)(R 0.336, F 0.032)] [D acc: (0.938)(0.875, 1.000)] [G loss: 4.750] [G acc: 0.000]\n",
      "2213 [D loss: (0.212)(R 0.147, F 0.277)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.344] [G acc: 0.250]\n",
      "2214 [D loss: (0.467)(R 0.485, F 0.448)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.452] [G acc: 0.125]\n",
      "2215 [D loss: (0.116)(R 0.171, F 0.060)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.588] [G acc: 0.188]\n",
      "2216 [D loss: (0.243)(R 0.127, F 0.358)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.551] [G acc: 0.188]\n",
      "2217 [D loss: (0.411)(R 0.396, F 0.426)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.127] [G acc: 0.312]\n",
      "2218 [D loss: (0.353)(R 0.158, F 0.548)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.631] [G acc: 0.125]\n",
      "2219 [D loss: (1.115)(R 1.840, F 0.389)] [D acc: (0.781)(0.750, 0.812)] [G loss: 2.602] [G acc: 0.375]\n",
      "2220 [D loss: (0.567)(R 0.332, F 0.801)] [D acc: (0.781)(0.938, 0.625)] [G loss: 5.300] [G acc: 0.125]\n",
      "2221 [D loss: (0.591)(R 0.649, F 0.532)] [D acc: (0.812)(0.812, 0.812)] [G loss: 6.561] [G acc: 0.125]\n",
      "2222 [D loss: (0.294)(R 0.154, F 0.435)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.979] [G acc: 0.125]\n",
      "2223 [D loss: (0.347)(R 0.143, F 0.551)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.636] [G acc: 0.125]\n",
      "2224 [D loss: (0.187)(R 0.166, F 0.208)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.506] [G acc: 0.312]\n",
      "2225 [D loss: (0.374)(R 0.136, F 0.613)] [D acc: (0.844)(1.000, 0.688)] [G loss: 7.097] [G acc: 0.062]\n",
      "2226 [D loss: (0.225)(R 0.160, F 0.290)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.572] [G acc: 0.250]\n",
      "2227 [D loss: (0.327)(R 0.166, F 0.488)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.308] [G acc: 0.125]\n",
      "2228 [D loss: (0.408)(R 0.343, F 0.474)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.836] [G acc: 0.250]\n",
      "2229 [D loss: (0.199)(R 0.158, F 0.239)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.189] [G acc: 0.125]\n",
      "2230 [D loss: (0.443)(R 0.552, F 0.334)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.894] [G acc: 0.250]\n",
      "2231 [D loss: (0.111)(R 0.124, F 0.098)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.523] [G acc: 0.188]\n",
      "2232 [D loss: (0.405)(R 0.413, F 0.397)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.928] [G acc: 0.188]\n",
      "2233 [D loss: (0.414)(R 0.445, F 0.383)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.705] [G acc: 0.000]\n",
      "2234 [D loss: (0.467)(R 0.606, F 0.328)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.666] [G acc: 0.188]\n",
      "2235 [D loss: (0.285)(R 0.156, F 0.413)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.599] [G acc: 0.125]\n",
      "2236 [D loss: (0.317)(R 0.210, F 0.424)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.054] [G acc: 0.188]\n",
      "2237 [D loss: (0.547)(R 0.519, F 0.576)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.912] [G acc: 0.188]\n",
      "2238 [D loss: (0.287)(R 0.157, F 0.417)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.318] [G acc: 0.125]\n",
      "2239 [D loss: (0.210)(R 0.143, F 0.278)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.347] [G acc: 0.188]\n",
      "2240 [D loss: (0.342)(R 0.595, F 0.089)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.281] [G acc: 0.125]\n",
      "2241 [D loss: (0.267)(R 0.205, F 0.329)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.037] [G acc: 0.062]\n",
      "2242 [D loss: (0.202)(R 0.185, F 0.219)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.085] [G acc: 0.250]\n",
      "2243 [D loss: (0.299)(R 0.200, F 0.398)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.667] [G acc: 0.188]\n",
      "2244 [D loss: (0.603)(R 1.012, F 0.195)] [D acc: (0.844)(0.750, 0.938)] [G loss: 4.786] [G acc: 0.188]\n",
      "2245 [D loss: (0.381)(R 0.471, F 0.292)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.309] [G acc: 0.000]\n",
      "2246 [D loss: (0.523)(R 0.354, F 0.692)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.870] [G acc: 0.125]\n",
      "2247 [D loss: (0.434)(R 0.668, F 0.199)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.752] [G acc: 0.125]\n",
      "2248 [D loss: (0.351)(R 0.135, F 0.566)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.482] [G acc: 0.125]\n",
      "2249 [D loss: (0.405)(R 0.222, F 0.589)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.631] [G acc: 0.062]\n",
      "2250 [D loss: (0.430)(R 0.655, F 0.206)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.894] [G acc: 0.188]\n",
      "2251 [D loss: (0.525)(R 0.504, F 0.545)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.615] [G acc: 0.125]\n",
      "2252 [D loss: (0.366)(R 0.210, F 0.521)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.726] [G acc: 0.062]\n",
      "2253 [D loss: (0.559)(R 0.407, F 0.711)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.240] [G acc: 0.062]\n",
      "2254 [D loss: (0.384)(R 0.124, F 0.643)] [D acc: (0.781)(1.000, 0.562)] [G loss: 5.038] [G acc: 0.062]\n",
      "2255 [D loss: (0.136)(R 0.143, F 0.130)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.914] [G acc: 0.125]\n",
      "2256 [D loss: (0.204)(R 0.345, F 0.062)] [D acc: (0.938)(0.875, 1.000)] [G loss: 3.996] [G acc: 0.062]\n",
      "2257 [D loss: (0.214)(R 0.172, F 0.256)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.879] [G acc: 0.188]\n",
      "2258 [D loss: (0.213)(R 0.148, F 0.277)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.297] [G acc: 0.188]\n",
      "2259 [D loss: (0.354)(R 0.536, F 0.171)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.631] [G acc: 0.188]\n",
      "2260 [D loss: (0.583)(R 0.598, F 0.569)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.925] [G acc: 0.125]\n",
      "2261 [D loss: (0.341)(R 0.452, F 0.231)] [D acc: (0.812)(0.750, 0.875)] [G loss: 6.931] [G acc: 0.125]\n",
      "2262 [D loss: (0.526)(R 0.749, F 0.303)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.497] [G acc: 0.125]\n",
      "2263 [D loss: (0.455)(R 0.492, F 0.418)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.778] [G acc: 0.000]\n",
      "2264 [D loss: (0.337)(R 0.346, F 0.328)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.196] [G acc: 0.188]\n",
      "2265 [D loss: (0.430)(R 0.480, F 0.380)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.721] [G acc: 0.188]\n",
      "2266 [D loss: (0.354)(R 0.204, F 0.504)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.570] [G acc: 0.062]\n",
      "2267 [D loss: (0.253)(R 0.171, F 0.334)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.915] [G acc: 0.125]\n",
      "2268 [D loss: (0.644)(R 1.014, F 0.274)] [D acc: (0.781)(0.688, 0.875)] [G loss: 2.541] [G acc: 0.188]\n",
      "2269 [D loss: (0.343)(R 0.404, F 0.282)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.860] [G acc: 0.062]\n",
      "2270 [D loss: (0.266)(R 0.216, F 0.316)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.908] [G acc: 0.188]\n",
      "2271 [D loss: (0.247)(R 0.178, F 0.317)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.317] [G acc: 0.250]\n",
      "2272 [D loss: (0.737)(R 0.644, F 0.831)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.272] [G acc: 0.125]\n",
      "2273 [D loss: (0.373)(R 0.267, F 0.480)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.940] [G acc: 0.250]\n",
      "2274 [D loss: (0.296)(R 0.170, F 0.423)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.722] [G acc: 0.188]\n",
      "2275 [D loss: (0.362)(R 0.399, F 0.325)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.735] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2276 [D loss: (0.422)(R 0.421, F 0.423)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.863] [G acc: 0.312]\n",
      "2277 [D loss: (0.316)(R 0.223, F 0.409)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.489] [G acc: 0.188]\n",
      "2278 [D loss: (0.182)(R 0.180, F 0.185)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.545] [G acc: 0.188]\n",
      "2279 [D loss: (0.125)(R 0.166, F 0.084)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.871] [G acc: 0.188]\n",
      "2280 [D loss: (0.101)(R 0.137, F 0.065)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.468] [G acc: 0.125]\n",
      "2281 [D loss: (0.244)(R 0.205, F 0.283)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.584] [G acc: 0.188]\n",
      "2282 [D loss: (0.303)(R 0.364, F 0.242)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.270] [G acc: 0.188]\n",
      "2283 [D loss: (0.443)(R 0.626, F 0.260)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.891] [G acc: 0.188]\n",
      "2284 [D loss: (0.215)(R 0.334, F 0.095)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.029] [G acc: 0.250]\n",
      "2285 [D loss: (0.194)(R 0.140, F 0.248)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.199] [G acc: 0.188]\n",
      "2286 [D loss: (0.094)(R 0.149, F 0.039)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.980] [G acc: 0.188]\n",
      "2287 [D loss: (0.286)(R 0.204, F 0.367)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.712] [G acc: 0.062]\n",
      "2288 [D loss: (0.073)(R 0.126, F 0.020)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.413] [G acc: 0.125]\n",
      "2289 [D loss: (0.305)(R 0.400, F 0.210)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.979] [G acc: 0.188]\n",
      "2290 [D loss: (0.457)(R 0.525, F 0.388)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.904] [G acc: 0.188]\n",
      "2291 [D loss: (0.288)(R 0.452, F 0.125)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.935] [G acc: 0.250]\n",
      "2292 [D loss: (0.161)(R 0.118, F 0.205)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.976] [G acc: 0.125]\n",
      "2293 [D loss: (0.231)(R 0.138, F 0.323)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.457] [G acc: 0.062]\n",
      "2294 [D loss: (0.154)(R 0.187, F 0.120)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.341] [G acc: 0.188]\n",
      "2295 [D loss: (0.222)(R 0.234, F 0.210)] [D acc: (0.906)(0.938, 0.875)] [G loss: 7.338] [G acc: 0.062]\n",
      "2296 [D loss: (0.500)(R 0.470, F 0.530)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.221] [G acc: 0.250]\n",
      "2297 [D loss: (0.237)(R 0.184, F 0.290)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.723] [G acc: 0.312]\n",
      "2298 [D loss: (0.130)(R 0.238, F 0.021)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.751] [G acc: 0.188]\n",
      "2299 [D loss: (0.226)(R 0.164, F 0.288)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.003] [G acc: 0.250]\n",
      "2300 [D loss: (0.135)(R 0.113, F 0.158)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.582] [G acc: 0.250]\n",
      "2301 [D loss: (0.334)(R 0.171, F 0.496)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.002] [G acc: 0.250]\n",
      "2302 [D loss: (0.622)(R 0.639, F 0.606)] [D acc: (0.812)(0.938, 0.688)] [G loss: 6.033] [G acc: 0.125]\n",
      "2303 [D loss: (0.304)(R 0.223, F 0.386)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.334] [G acc: 0.188]\n",
      "2304 [D loss: (0.190)(R 0.366, F 0.013)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.550] [G acc: 0.250]\n",
      "2305 [D loss: (0.607)(R 0.346, F 0.869)] [D acc: (0.719)(0.875, 0.562)] [G loss: 4.941] [G acc: 0.250]\n",
      "2306 [D loss: (0.771)(R 1.019, F 0.523)] [D acc: (0.719)(0.688, 0.750)] [G loss: 3.777] [G acc: 0.188]\n",
      "2307 [D loss: (0.231)(R 0.168, F 0.295)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.421] [G acc: 0.188]\n",
      "2308 [D loss: (0.757)(R 1.211, F 0.302)] [D acc: (0.844)(0.812, 0.875)] [G loss: 5.579] [G acc: 0.125]\n",
      "2309 [D loss: (0.190)(R 0.134, F 0.245)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.839] [G acc: 0.062]\n",
      "2310 [D loss: (0.734)(R 0.679, F 0.790)] [D acc: (0.719)(0.875, 0.562)] [G loss: 5.439] [G acc: 0.188]\n",
      "2311 [D loss: (0.340)(R 0.423, F 0.258)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.304] [G acc: 0.250]\n",
      "2312 [D loss: (0.264)(R 0.105, F 0.423)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.211] [G acc: 0.062]\n",
      "2313 [D loss: (0.251)(R 0.198, F 0.305)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.019] [G acc: 0.062]\n",
      "2314 [D loss: (0.730)(R 0.839, F 0.620)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.708] [G acc: 0.125]\n",
      "2315 [D loss: (0.376)(R 0.445, F 0.308)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.787] [G acc: 0.000]\n",
      "2316 [D loss: (0.290)(R 0.418, F 0.163)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.752] [G acc: 0.062]\n",
      "2317 [D loss: (0.298)(R 0.120, F 0.476)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.815] [G acc: 0.062]\n",
      "2318 [D loss: (0.490)(R 0.639, F 0.341)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.689] [G acc: 0.250]\n",
      "2319 [D loss: (0.399)(R 0.198, F 0.599)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.693] [G acc: 0.250]\n",
      "2320 [D loss: (0.321)(R 0.314, F 0.328)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.472] [G acc: 0.125]\n",
      "2321 [D loss: (0.424)(R 0.178, F 0.671)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.880] [G acc: 0.312]\n",
      "2322 [D loss: (0.479)(R 0.767, F 0.191)] [D acc: (0.875)(0.812, 0.938)] [G loss: 7.028] [G acc: 0.125]\n",
      "2323 [D loss: (0.558)(R 0.686, F 0.431)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.339] [G acc: 0.312]\n",
      "2324 [D loss: (0.240)(R 0.358, F 0.122)] [D acc: (0.938)(0.875, 1.000)] [G loss: 5.208] [G acc: 0.188]\n",
      "2325 [D loss: (0.255)(R 0.156, F 0.354)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.356] [G acc: 0.125]\n",
      "2326 [D loss: (0.272)(R 0.116, F 0.429)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.794] [G acc: 0.250]\n",
      "2327 [D loss: (0.228)(R 0.251, F 0.204)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.726] [G acc: 0.125]\n",
      "2328 [D loss: (0.315)(R 0.250, F 0.379)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.975] [G acc: 0.062]\n",
      "2329 [D loss: (0.282)(R 0.212, F 0.351)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.732] [G acc: 0.125]\n",
      "2330 [D loss: (0.539)(R 0.219, F 0.859)] [D acc: (0.719)(0.875, 0.562)] [G loss: 4.234] [G acc: 0.188]\n",
      "2331 [D loss: (0.205)(R 0.215, F 0.194)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.980] [G acc: 0.000]\n",
      "2332 [D loss: (0.414)(R 0.155, F 0.672)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.270] [G acc: 0.312]\n",
      "2333 [D loss: (0.365)(R 0.202, F 0.528)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.722] [G acc: 0.062]\n",
      "2334 [D loss: (0.532)(R 0.882, F 0.183)] [D acc: (0.812)(0.688, 0.938)] [G loss: 5.826] [G acc: 0.000]\n",
      "2335 [D loss: (0.330)(R 0.116, F 0.543)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.071] [G acc: 0.062]\n",
      "2336 [D loss: (0.213)(R 0.287, F 0.140)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.931] [G acc: 0.062]\n",
      "2337 [D loss: (0.478)(R 0.232, F 0.724)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.054] [G acc: 0.188]\n",
      "2338 [D loss: (0.344)(R 0.213, F 0.476)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.491] [G acc: 0.125]\n",
      "2339 [D loss: (0.303)(R 0.338, F 0.268)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.623] [G acc: 0.125]\n",
      "2340 [D loss: (0.222)(R 0.204, F 0.240)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.637] [G acc: 0.125]\n",
      "2341 [D loss: (0.714)(R 0.897, F 0.530)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.418] [G acc: 0.125]\n",
      "2342 [D loss: (0.317)(R 0.253, F 0.380)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.304] [G acc: 0.188]\n",
      "2343 [D loss: (0.182)(R 0.193, F 0.172)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.856] [G acc: 0.062]\n",
      "2344 [D loss: (0.333)(R 0.343, F 0.323)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.948] [G acc: 0.125]\n",
      "2345 [D loss: (0.157)(R 0.168, F 0.147)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.410] [G acc: 0.125]\n",
      "2346 [D loss: (0.450)(R 0.387, F 0.513)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.718] [G acc: 0.125]\n",
      "2347 [D loss: (0.231)(R 0.264, F 0.198)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.714] [G acc: 0.250]\n",
      "2348 [D loss: (0.212)(R 0.133, F 0.290)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.586] [G acc: 0.250]\n",
      "2349 [D loss: (0.211)(R 0.182, F 0.239)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.906] [G acc: 0.188]\n",
      "2350 [D loss: (0.262)(R 0.268, F 0.255)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.398] [G acc: 0.125]\n",
      "2351 [D loss: (0.332)(R 0.332, F 0.332)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.218] [G acc: 0.188]\n",
      "2352 [D loss: (0.244)(R 0.204, F 0.284)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.684] [G acc: 0.188]\n",
      "2353 [D loss: (0.317)(R 0.204, F 0.431)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.783] [G acc: 0.250]\n",
      "2354 [D loss: (0.206)(R 0.299, F 0.113)] [D acc: (0.969)(0.938, 1.000)] [G loss: 5.185] [G acc: 0.125]\n",
      "2355 [D loss: (0.575)(R 0.399, F 0.751)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.132] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356 [D loss: (0.460)(R 0.439, F 0.481)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.637] [G acc: 0.250]\n",
      "2357 [D loss: (0.548)(R 0.571, F 0.524)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.643] [G acc: 0.125]\n",
      "2358 [D loss: (0.627)(R 0.408, F 0.846)] [D acc: (0.750)(0.938, 0.562)] [G loss: 4.432] [G acc: 0.188]\n",
      "2359 [D loss: (0.457)(R 0.672, F 0.242)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.173] [G acc: 0.125]\n",
      "2360 [D loss: (0.480)(R 0.738, F 0.222)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.057] [G acc: 0.188]\n",
      "2361 [D loss: (0.485)(R 0.925, F 0.045)] [D acc: (0.906)(0.812, 1.000)] [G loss: 4.649] [G acc: 0.125]\n",
      "2362 [D loss: (0.253)(R 0.302, F 0.205)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.746] [G acc: 0.062]\n",
      "2363 [D loss: (0.451)(R 0.508, F 0.394)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.578] [G acc: 0.125]\n",
      "2364 [D loss: (0.487)(R 0.456, F 0.518)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.873] [G acc: 0.312]\n",
      "2365 [D loss: (0.261)(R 0.161, F 0.361)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.923] [G acc: 0.188]\n",
      "2366 [D loss: (0.327)(R 0.404, F 0.249)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.049] [G acc: 0.188]\n",
      "2367 [D loss: (0.220)(R 0.194, F 0.245)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.251] [G acc: 0.250]\n",
      "2368 [D loss: (0.425)(R 0.698, F 0.151)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.484] [G acc: 0.188]\n",
      "2369 [D loss: (0.203)(R 0.149, F 0.256)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.133] [G acc: 0.250]\n",
      "2370 [D loss: (0.216)(R 0.157, F 0.275)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.901] [G acc: 0.250]\n",
      "2371 [D loss: (0.381)(R 0.409, F 0.353)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.895] [G acc: 0.125]\n",
      "2372 [D loss: (0.982)(R 1.770, F 0.195)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.711] [G acc: 0.188]\n",
      "2373 [D loss: (0.301)(R 0.267, F 0.335)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.823] [G acc: 0.312]\n",
      "2374 [D loss: (0.388)(R 0.404, F 0.372)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.002] [G acc: 0.250]\n",
      "2375 [D loss: (0.221)(R 0.151, F 0.291)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.724] [G acc: 0.375]\n",
      "2376 [D loss: (0.389)(R 0.330, F 0.448)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.710] [G acc: 0.438]\n",
      "2377 [D loss: (0.398)(R 0.301, F 0.494)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.143] [G acc: 0.125]\n",
      "2378 [D loss: (0.131)(R 0.191, F 0.070)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.045] [G acc: 0.062]\n",
      "2379 [D loss: (0.270)(R 0.333, F 0.207)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.497] [G acc: 0.125]\n",
      "2380 [D loss: (0.315)(R 0.225, F 0.406)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.274] [G acc: 0.250]\n",
      "2381 [D loss: (0.424)(R 0.182, F 0.667)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.193] [G acc: 0.125]\n",
      "2382 [D loss: (0.149)(R 0.192, F 0.106)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.983] [G acc: 0.188]\n",
      "2383 [D loss: (0.139)(R 0.156, F 0.121)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.030] [G acc: 0.188]\n",
      "2384 [D loss: (0.217)(R 0.150, F 0.285)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.411] [G acc: 0.062]\n",
      "2385 [D loss: (0.428)(R 0.365, F 0.491)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.391] [G acc: 0.125]\n",
      "2386 [D loss: (0.157)(R 0.145, F 0.169)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.949] [G acc: 0.188]\n",
      "2387 [D loss: (0.147)(R 0.168, F 0.127)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.753] [G acc: 0.188]\n",
      "2388 [D loss: (0.325)(R 0.248, F 0.401)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.905] [G acc: 0.188]\n",
      "2389 [D loss: (0.302)(R 0.401, F 0.202)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.356] [G acc: 0.250]\n",
      "2390 [D loss: (0.505)(R 0.411, F 0.598)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.449] [G acc: 0.250]\n",
      "2391 [D loss: (0.509)(R 0.466, F 0.552)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.291] [G acc: 0.250]\n",
      "2392 [D loss: (0.651)(R 0.695, F 0.608)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.120] [G acc: 0.188]\n",
      "2393 [D loss: (0.298)(R 0.210, F 0.387)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.572] [G acc: 0.188]\n",
      "2394 [D loss: (0.391)(R 0.278, F 0.504)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.801] [G acc: 0.250]\n",
      "2395 [D loss: (0.191)(R 0.199, F 0.183)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.699] [G acc: 0.188]\n",
      "2396 [D loss: (0.255)(R 0.126, F 0.384)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.451] [G acc: 0.125]\n",
      "2397 [D loss: (0.255)(R 0.344, F 0.167)] [D acc: (0.844)(0.750, 0.938)] [G loss: 6.302] [G acc: 0.000]\n",
      "2398 [D loss: (0.273)(R 0.321, F 0.226)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.442] [G acc: 0.125]\n",
      "2399 [D loss: (0.454)(R 0.516, F 0.393)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.625] [G acc: 0.250]\n",
      "2400 [D loss: (0.474)(R 0.372, F 0.576)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.688] [G acc: 0.125]\n",
      "2401 [D loss: (0.499)(R 0.502, F 0.496)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.267] [G acc: 0.188]\n",
      "2402 [D loss: (0.270)(R 0.137, F 0.403)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.296] [G acc: 0.188]\n",
      "2403 [D loss: (0.235)(R 0.150, F 0.320)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.615] [G acc: 0.250]\n",
      "2404 [D loss: (0.739)(R 0.990, F 0.487)] [D acc: (0.719)(0.688, 0.750)] [G loss: 3.661] [G acc: 0.250]\n",
      "2405 [D loss: (0.367)(R 0.310, F 0.425)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.841] [G acc: 0.125]\n",
      "2406 [D loss: (0.267)(R 0.314, F 0.220)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.533] [G acc: 0.188]\n",
      "2407 [D loss: (0.784)(R 1.065, F 0.502)] [D acc: (0.750)(0.750, 0.750)] [G loss: 4.304] [G acc: 0.188]\n",
      "2408 [D loss: (0.682)(R 0.566, F 0.798)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.041] [G acc: 0.188]\n",
      "2409 [D loss: (0.416)(R 0.264, F 0.568)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.411] [G acc: 0.188]\n",
      "2410 [D loss: (0.389)(R 0.466, F 0.312)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.102] [G acc: 0.188]\n",
      "2411 [D loss: (0.193)(R 0.195, F 0.190)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.844] [G acc: 0.188]\n",
      "2412 [D loss: (0.249)(R 0.323, F 0.176)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.976] [G acc: 0.312]\n",
      "2413 [D loss: (0.304)(R 0.373, F 0.235)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.525] [G acc: 0.062]\n",
      "2414 [D loss: (0.337)(R 0.326, F 0.348)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.797] [G acc: 0.188]\n",
      "2415 [D loss: (0.479)(R 0.168, F 0.790)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.056] [G acc: 0.250]\n",
      "2416 [D loss: (0.375)(R 0.204, F 0.547)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.759] [G acc: 0.375]\n",
      "2417 [D loss: (0.235)(R 0.146, F 0.324)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.349] [G acc: 0.125]\n",
      "2418 [D loss: (0.182)(R 0.288, F 0.076)] [D acc: (0.938)(0.875, 1.000)] [G loss: 4.535] [G acc: 0.188]\n",
      "2419 [D loss: (0.249)(R 0.229, F 0.269)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.323] [G acc: 0.188]\n",
      "2420 [D loss: (0.483)(R 0.362, F 0.605)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.462] [G acc: 0.125]\n",
      "2421 [D loss: (0.549)(R 0.265, F 0.834)] [D acc: (0.750)(0.938, 0.562)] [G loss: 4.704] [G acc: 0.188]\n",
      "2422 [D loss: (0.652)(R 0.634, F 0.670)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.807] [G acc: 0.125]\n",
      "2423 [D loss: (0.219)(R 0.247, F 0.192)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.902] [G acc: 0.125]\n",
      "2424 [D loss: (0.491)(R 0.609, F 0.372)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.243] [G acc: 0.188]\n",
      "2425 [D loss: (0.708)(R 1.150, F 0.266)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.709] [G acc: 0.062]\n",
      "2426 [D loss: (0.243)(R 0.380, F 0.107)] [D acc: (0.938)(0.875, 1.000)] [G loss: 3.672] [G acc: 0.188]\n",
      "2427 [D loss: (0.466)(R 0.572, F 0.360)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.023] [G acc: 0.250]\n",
      "2428 [D loss: (0.651)(R 0.908, F 0.394)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.448] [G acc: 0.188]\n",
      "2429 [D loss: (0.347)(R 0.454, F 0.241)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.890] [G acc: 0.125]\n",
      "2430 [D loss: (0.205)(R 0.154, F 0.255)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.880] [G acc: 0.188]\n",
      "2431 [D loss: (0.400)(R 0.397, F 0.402)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.979] [G acc: 0.188]\n",
      "2432 [D loss: (0.213)(R 0.145, F 0.280)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.008] [G acc: 0.250]\n",
      "2433 [D loss: (0.298)(R 0.377, F 0.219)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.961] [G acc: 0.250]\n",
      "2434 [D loss: (0.283)(R 0.190, F 0.377)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.585] [G acc: 0.312]\n",
      "2435 [D loss: (0.425)(R 0.622, F 0.227)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.260] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2436 [D loss: (0.208)(R 0.209, F 0.208)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.571] [G acc: 0.312]\n",
      "2437 [D loss: (0.168)(R 0.198, F 0.138)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.209] [G acc: 0.188]\n",
      "2438 [D loss: (0.233)(R 0.149, F 0.316)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.754] [G acc: 0.188]\n",
      "2439 [D loss: (0.445)(R 0.443, F 0.447)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.664] [G acc: 0.125]\n",
      "2440 [D loss: (0.438)(R 0.608, F 0.268)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.311] [G acc: 0.062]\n",
      "2441 [D loss: (0.497)(R 0.861, F 0.132)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.313] [G acc: 0.188]\n",
      "2442 [D loss: (0.253)(R 0.257, F 0.249)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.528] [G acc: 0.312]\n",
      "2443 [D loss: (0.441)(R 0.498, F 0.384)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.859] [G acc: 0.250]\n",
      "2444 [D loss: (0.382)(R 0.337, F 0.426)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.744] [G acc: 0.000]\n",
      "2445 [D loss: (0.280)(R 0.105, F 0.454)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.313] [G acc: 0.250]\n",
      "2446 [D loss: (0.350)(R 0.431, F 0.270)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.931] [G acc: 0.125]\n",
      "2447 [D loss: (0.290)(R 0.124, F 0.455)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.490] [G acc: 0.312]\n",
      "2448 [D loss: (0.251)(R 0.140, F 0.361)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.457] [G acc: 0.000]\n",
      "2449 [D loss: (0.487)(R 0.601, F 0.373)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.848] [G acc: 0.250]\n",
      "2450 [D loss: (0.266)(R 0.244, F 0.288)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.125] [G acc: 0.250]\n",
      "2451 [D loss: (0.307)(R 0.259, F 0.355)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.670] [G acc: 0.250]\n",
      "2452 [D loss: (0.405)(R 0.516, F 0.295)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.053] [G acc: 0.312]\n",
      "2453 [D loss: (0.712)(R 0.838, F 0.585)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.058] [G acc: 0.062]\n",
      "2454 [D loss: (0.325)(R 0.188, F 0.463)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.389] [G acc: 0.250]\n",
      "2455 [D loss: (0.302)(R 0.133, F 0.471)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.953] [G acc: 0.312]\n",
      "2456 [D loss: (0.360)(R 0.414, F 0.305)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.634] [G acc: 0.125]\n",
      "2457 [D loss: (0.581)(R 0.866, F 0.295)] [D acc: (0.812)(0.750, 0.875)] [G loss: 3.346] [G acc: 0.312]\n",
      "2458 [D loss: (0.279)(R 0.318, F 0.240)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.522] [G acc: 0.188]\n",
      "2459 [D loss: (0.338)(R 0.392, F 0.283)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.547] [G acc: 0.188]\n",
      "2460 [D loss: (0.512)(R 0.551, F 0.472)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.046] [G acc: 0.062]\n",
      "2461 [D loss: (0.242)(R 0.129, F 0.354)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.720] [G acc: 0.312]\n",
      "2462 [D loss: (0.447)(R 0.203, F 0.692)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.457] [G acc: 0.188]\n",
      "2463 [D loss: (0.445)(R 0.200, F 0.689)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.690] [G acc: 0.250]\n",
      "2464 [D loss: (0.452)(R 0.523, F 0.381)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.390] [G acc: 0.250]\n",
      "2465 [D loss: (0.162)(R 0.240, F 0.084)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.581] [G acc: 0.188]\n",
      "2466 [D loss: (0.205)(R 0.211, F 0.199)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.385] [G acc: 0.125]\n",
      "2467 [D loss: (0.179)(R 0.230, F 0.128)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.011] [G acc: 0.125]\n",
      "2468 [D loss: (0.210)(R 0.202, F 0.218)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.462] [G acc: 0.125]\n",
      "2469 [D loss: (0.288)(R 0.125, F 0.450)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.338] [G acc: 0.125]\n",
      "2470 [D loss: (0.269)(R 0.146, F 0.391)] [D acc: (0.906)(1.000, 0.812)] [G loss: 7.141] [G acc: 0.000]\n",
      "2471 [D loss: (0.248)(R 0.249, F 0.248)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.939] [G acc: 0.188]\n",
      "2472 [D loss: (0.184)(R 0.208, F 0.160)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.513] [G acc: 0.125]\n",
      "2473 [D loss: (0.246)(R 0.184, F 0.308)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.079] [G acc: 0.125]\n",
      "2474 [D loss: (0.558)(R 0.611, F 0.506)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.925] [G acc: 0.250]\n",
      "2475 [D loss: (0.490)(R 0.451, F 0.529)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.526] [G acc: 0.188]\n",
      "2476 [D loss: (0.522)(R 0.533, F 0.512)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.982] [G acc: 0.125]\n",
      "2477 [D loss: (0.464)(R 0.396, F 0.532)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.208] [G acc: 0.062]\n",
      "2478 [D loss: (0.571)(R 0.656, F 0.486)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.044] [G acc: 0.188]\n",
      "2479 [D loss: (0.260)(R 0.196, F 0.324)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.284] [G acc: 0.250]\n",
      "2480 [D loss: (0.456)(R 0.356, F 0.556)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.500] [G acc: 0.250]\n",
      "2481 [D loss: (0.330)(R 0.257, F 0.402)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.159] [G acc: 0.188]\n",
      "2482 [D loss: (0.313)(R 0.292, F 0.335)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.143] [G acc: 0.312]\n",
      "2483 [D loss: (0.491)(R 0.407, F 0.576)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.403] [G acc: 0.250]\n",
      "2484 [D loss: (0.237)(R 0.267, F 0.207)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.956] [G acc: 0.188]\n",
      "2485 [D loss: (0.340)(R 0.439, F 0.241)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.792] [G acc: 0.312]\n",
      "2486 [D loss: (0.147)(R 0.231, F 0.064)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.935] [G acc: 0.125]\n",
      "2487 [D loss: (0.229)(R 0.167, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.426] [G acc: 0.188]\n",
      "2488 [D loss: (0.458)(R 0.452, F 0.464)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.396] [G acc: 0.062]\n",
      "2489 [D loss: (0.347)(R 0.637, F 0.056)] [D acc: (0.906)(0.812, 1.000)] [G loss: 4.192] [G acc: 0.188]\n",
      "2490 [D loss: (0.267)(R 0.287, F 0.248)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.642] [G acc: 0.125]\n",
      "2491 [D loss: (0.349)(R 0.173, F 0.525)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.707] [G acc: 0.000]\n",
      "2492 [D loss: (0.238)(R 0.174, F 0.302)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.060] [G acc: 0.250]\n",
      "2493 [D loss: (0.260)(R 0.126, F 0.395)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.032] [G acc: 0.125]\n",
      "2494 [D loss: (0.305)(R 0.215, F 0.395)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.906] [G acc: 0.188]\n",
      "2495 [D loss: (0.423)(R 0.537, F 0.308)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.762] [G acc: 0.312]\n",
      "2496 [D loss: (0.331)(R 0.393, F 0.268)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.973] [G acc: 0.312]\n",
      "2497 [D loss: (0.421)(R 0.333, F 0.508)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.001] [G acc: 0.312]\n",
      "2498 [D loss: (0.206)(R 0.191, F 0.220)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.702] [G acc: 0.188]\n",
      "2499 [D loss: (0.239)(R 0.214, F 0.264)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.411] [G acc: 0.000]\n",
      "2500 [D loss: (0.094)(R 0.135, F 0.053)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.080] [G acc: 0.250]\n",
      "2501 [D loss: (0.253)(R 0.200, F 0.306)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.667] [G acc: 0.188]\n",
      "2502 [D loss: (0.367)(R 0.210, F 0.525)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.514] [G acc: 0.125]\n",
      "2503 [D loss: (0.375)(R 0.374, F 0.376)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.233] [G acc: 0.125]\n",
      "2504 [D loss: (0.394)(R 0.158, F 0.631)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.483] [G acc: 0.188]\n",
      "2505 [D loss: (0.274)(R 0.308, F 0.241)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.985] [G acc: 0.250]\n",
      "2506 [D loss: (0.591)(R 0.983, F 0.199)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.420] [G acc: 0.250]\n",
      "2507 [D loss: (0.430)(R 0.468, F 0.393)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.452] [G acc: 0.125]\n",
      "2508 [D loss: (0.462)(R 0.521, F 0.404)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.435] [G acc: 0.375]\n",
      "2509 [D loss: (0.535)(R 0.473, F 0.597)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.622] [G acc: 0.250]\n",
      "2510 [D loss: (0.578)(R 0.720, F 0.435)] [D acc: (0.719)(0.688, 0.750)] [G loss: 3.706] [G acc: 0.250]\n",
      "2511 [D loss: (0.235)(R 0.132, F 0.338)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.940] [G acc: 0.250]\n",
      "2512 [D loss: (0.315)(R 0.155, F 0.475)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.342] [G acc: 0.312]\n",
      "2513 [D loss: (0.266)(R 0.214, F 0.318)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.165] [G acc: 0.188]\n",
      "2514 [D loss: (0.360)(R 0.138, F 0.581)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.268] [G acc: 0.250]\n",
      "2515 [D loss: (0.333)(R 0.208, F 0.457)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.554] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2516 [D loss: (0.267)(R 0.232, F 0.302)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.670] [G acc: 0.125]\n",
      "2517 [D loss: (0.222)(R 0.160, F 0.285)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.596] [G acc: 0.312]\n",
      "2518 [D loss: (0.387)(R 0.278, F 0.495)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.365] [G acc: 0.125]\n",
      "2519 [D loss: (0.408)(R 0.342, F 0.474)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.336] [G acc: 0.062]\n",
      "2520 [D loss: (0.261)(R 0.267, F 0.254)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.951] [G acc: 0.125]\n",
      "2521 [D loss: (0.529)(R 0.247, F 0.811)] [D acc: (0.750)(0.938, 0.562)] [G loss: 4.822] [G acc: 0.125]\n",
      "2522 [D loss: (0.320)(R 0.281, F 0.360)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.476] [G acc: 0.188]\n",
      "2523 [D loss: (0.487)(R 0.706, F 0.268)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.835] [G acc: 0.188]\n",
      "2524 [D loss: (0.424)(R 0.265, F 0.583)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.504] [G acc: 0.312]\n",
      "2525 [D loss: (0.298)(R 0.259, F 0.336)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.895] [G acc: 0.250]\n",
      "2526 [D loss: (0.509)(R 0.581, F 0.437)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.700] [G acc: 0.125]\n",
      "2527 [D loss: (0.398)(R 0.168, F 0.628)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.842] [G acc: 0.188]\n",
      "2528 [D loss: (0.271)(R 0.154, F 0.387)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.574] [G acc: 0.312]\n",
      "2529 [D loss: (0.363)(R 0.165, F 0.561)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.020] [G acc: 0.125]\n",
      "2530 [D loss: (0.558)(R 0.891, F 0.225)] [D acc: (0.812)(0.750, 0.875)] [G loss: 4.182] [G acc: 0.188]\n",
      "2531 [D loss: (0.418)(R 0.134, F 0.702)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.032] [G acc: 0.188]\n",
      "2532 [D loss: (0.294)(R 0.226, F 0.361)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.773] [G acc: 0.250]\n",
      "2533 [D loss: (0.210)(R 0.126, F 0.294)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.818] [G acc: 0.125]\n",
      "2534 [D loss: (0.412)(R 0.266, F 0.558)] [D acc: (0.844)(0.938, 0.750)] [G loss: 6.009] [G acc: 0.000]\n",
      "2535 [D loss: (0.337)(R 0.420, F 0.255)] [D acc: (0.812)(0.812, 0.812)] [G loss: 4.067] [G acc: 0.188]\n",
      "2536 [D loss: (0.390)(R 0.289, F 0.491)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.176] [G acc: 0.188]\n",
      "2537 [D loss: (0.416)(R 0.202, F 0.630)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.981] [G acc: 0.250]\n",
      "2538 [D loss: (0.612)(R 0.818, F 0.406)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.930] [G acc: 0.125]\n",
      "2539 [D loss: (0.366)(R 0.171, F 0.560)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.836] [G acc: 0.188]\n",
      "2540 [D loss: (0.330)(R 0.214, F 0.447)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.354] [G acc: 0.250]\n",
      "2541 [D loss: (0.381)(R 0.543, F 0.219)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.685] [G acc: 0.062]\n",
      "2542 [D loss: (0.120)(R 0.146, F 0.093)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.707] [G acc: 0.000]\n",
      "2543 [D loss: (0.317)(R 0.260, F 0.375)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.966] [G acc: 0.062]\n",
      "2544 [D loss: (0.368)(R 0.405, F 0.332)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.554] [G acc: 0.188]\n",
      "2545 [D loss: (0.184)(R 0.209, F 0.158)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.244] [G acc: 0.062]\n",
      "2546 [D loss: (0.302)(R 0.273, F 0.330)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.335] [G acc: 0.188]\n",
      "2547 [D loss: (0.136)(R 0.194, F 0.078)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.888] [G acc: 0.250]\n",
      "2548 [D loss: (0.194)(R 0.267, F 0.120)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.656] [G acc: 0.062]\n",
      "2549 [D loss: (0.222)(R 0.102, F 0.342)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.119] [G acc: 0.062]\n",
      "2550 [D loss: (0.461)(R 0.278, F 0.643)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.086] [G acc: 0.250]\n",
      "2551 [D loss: (0.268)(R 0.173, F 0.363)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.172] [G acc: 0.188]\n",
      "2552 [D loss: (0.254)(R 0.196, F 0.312)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.261] [G acc: 0.312]\n",
      "2553 [D loss: (0.286)(R 0.198, F 0.375)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.058] [G acc: 0.188]\n",
      "2554 [D loss: (0.242)(R 0.248, F 0.237)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.475] [G acc: 0.125]\n",
      "2555 [D loss: (0.365)(R 0.468, F 0.262)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.481] [G acc: 0.312]\n",
      "2556 [D loss: (0.179)(R 0.205, F 0.154)] [D acc: (0.969)(1.000, 0.938)] [G loss: 6.343] [G acc: 0.062]\n",
      "2557 [D loss: (0.457)(R 0.348, F 0.565)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.254] [G acc: 0.188]\n",
      "2558 [D loss: (0.548)(R 0.403, F 0.692)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.912] [G acc: 0.062]\n",
      "2559 [D loss: (0.293)(R 0.165, F 0.420)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.538] [G acc: 0.125]\n",
      "2560 [D loss: (0.345)(R 0.158, F 0.531)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.875] [G acc: 0.188]\n",
      "2561 [D loss: (0.631)(R 0.879, F 0.383)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.584] [G acc: 0.062]\n",
      "2562 [D loss: (0.167)(R 0.138, F 0.195)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.570] [G acc: 0.250]\n",
      "2563 [D loss: (0.455)(R 0.318, F 0.593)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.626] [G acc: 0.312]\n",
      "2564 [D loss: (0.263)(R 0.206, F 0.320)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.952] [G acc: 0.188]\n",
      "2565 [D loss: (0.236)(R 0.209, F 0.264)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.609] [G acc: 0.250]\n",
      "2566 [D loss: (0.246)(R 0.236, F 0.256)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.009] [G acc: 0.312]\n",
      "2567 [D loss: (0.359)(R 0.192, F 0.526)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.718] [G acc: 0.312]\n",
      "2568 [D loss: (0.262)(R 0.239, F 0.284)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.226] [G acc: 0.188]\n",
      "2569 [D loss: (0.327)(R 0.210, F 0.445)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.167] [G acc: 0.312]\n",
      "2570 [D loss: (0.198)(R 0.163, F 0.233)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.210] [G acc: 0.125]\n",
      "2571 [D loss: (0.300)(R 0.262, F 0.338)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.932] [G acc: 0.188]\n",
      "2572 [D loss: (0.253)(R 0.213, F 0.294)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.362] [G acc: 0.125]\n",
      "2573 [D loss: (0.406)(R 0.205, F 0.607)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.472] [G acc: 0.125]\n",
      "2574 [D loss: (0.482)(R 0.340, F 0.623)] [D acc: (0.719)(0.875, 0.562)] [G loss: 5.299] [G acc: 0.062]\n",
      "2575 [D loss: (0.598)(R 0.660, F 0.536)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.659] [G acc: 0.125]\n",
      "2576 [D loss: (0.193)(R 0.196, F 0.190)] [D acc: (0.938)(1.000, 0.875)] [G loss: 6.582] [G acc: 0.062]\n",
      "2577 [D loss: (0.362)(R 0.290, F 0.434)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.746] [G acc: 0.062]\n",
      "2578 [D loss: (0.315)(R 0.275, F 0.356)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.147] [G acc: 0.125]\n",
      "2579 [D loss: (0.295)(R 0.180, F 0.409)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.701] [G acc: 0.062]\n",
      "2580 [D loss: (0.208)(R 0.271, F 0.146)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.672] [G acc: 0.250]\n",
      "2581 [D loss: (0.455)(R 0.563, F 0.347)] [D acc: (0.781)(0.750, 0.812)] [G loss: 3.838] [G acc: 0.188]\n",
      "2582 [D loss: (0.393)(R 0.291, F 0.495)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.819] [G acc: 0.125]\n",
      "2583 [D loss: (0.336)(R 0.306, F 0.367)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.117] [G acc: 0.125]\n",
      "2584 [D loss: (0.281)(R 0.516, F 0.045)] [D acc: (0.875)(0.750, 1.000)] [G loss: 4.302] [G acc: 0.062]\n",
      "2585 [D loss: (0.889)(R 1.648, F 0.131)] [D acc: (0.844)(0.750, 0.938)] [G loss: 3.638] [G acc: 0.125]\n",
      "2586 [D loss: (0.217)(R 0.154, F 0.281)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.835] [G acc: 0.375]\n",
      "2587 [D loss: (0.372)(R 0.476, F 0.267)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.810] [G acc: 0.062]\n",
      "2588 [D loss: (0.550)(R 0.626, F 0.475)] [D acc: (0.750)(0.750, 0.750)] [G loss: 3.194] [G acc: 0.125]\n",
      "2589 [D loss: (0.305)(R 0.148, F 0.462)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.159] [G acc: 0.312]\n",
      "2590 [D loss: (0.407)(R 0.315, F 0.498)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.874] [G acc: 0.312]\n",
      "2591 [D loss: (0.182)(R 0.128, F 0.237)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.161] [G acc: 0.125]\n",
      "2592 [D loss: (0.339)(R 0.177, F 0.500)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.812] [G acc: 0.188]\n",
      "2593 [D loss: (0.152)(R 0.195, F 0.110)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.501] [G acc: 0.188]\n",
      "2594 [D loss: (0.400)(R 0.421, F 0.378)] [D acc: (0.875)(0.938, 0.812)] [G loss: 6.850] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2595 [D loss: (0.439)(R 0.619, F 0.259)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.588] [G acc: 0.125]\n",
      "2596 [D loss: (0.190)(R 0.193, F 0.187)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.946] [G acc: 0.125]\n",
      "2597 [D loss: (0.243)(R 0.142, F 0.344)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.020] [G acc: 0.375]\n",
      "2598 [D loss: (0.420)(R 0.493, F 0.347)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.886] [G acc: 0.250]\n",
      "2599 [D loss: (0.313)(R 0.332, F 0.295)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.282] [G acc: 0.125]\n",
      "2600 [D loss: (0.951)(R 1.305, F 0.596)] [D acc: (0.719)(0.688, 0.750)] [G loss: 4.459] [G acc: 0.125]\n",
      "2601 [D loss: (0.437)(R 0.441, F 0.433)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.606] [G acc: 0.188]\n",
      "2602 [D loss: (0.164)(R 0.158, F 0.170)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.568] [G acc: 0.125]\n",
      "2603 [D loss: (0.237)(R 0.192, F 0.282)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.048] [G acc: 0.188]\n",
      "2604 [D loss: (0.272)(R 0.143, F 0.400)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.358] [G acc: 0.125]\n",
      "2605 [D loss: (0.318)(R 0.204, F 0.432)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.410] [G acc: 0.125]\n",
      "2606 [D loss: (0.576)(R 0.373, F 0.779)] [D acc: (0.781)(0.938, 0.625)] [G loss: 5.102] [G acc: 0.250]\n",
      "2607 [D loss: (0.381)(R 0.242, F 0.519)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.982] [G acc: 0.000]\n",
      "2608 [D loss: (0.444)(R 0.511, F 0.377)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.674] [G acc: 0.062]\n",
      "2609 [D loss: (0.505)(R 0.767, F 0.242)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.517] [G acc: 0.188]\n",
      "2610 [D loss: (0.179)(R 0.196, F 0.162)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.832] [G acc: 0.250]\n",
      "2611 [D loss: (0.452)(R 0.453, F 0.451)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.376] [G acc: 0.188]\n",
      "2612 [D loss: (0.167)(R 0.168, F 0.165)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.788] [G acc: 0.125]\n",
      "2613 [D loss: (0.248)(R 0.224, F 0.271)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.664] [G acc: 0.125]\n",
      "2614 [D loss: (0.489)(R 0.729, F 0.249)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.248] [G acc: 0.188]\n",
      "2615 [D loss: (0.245)(R 0.163, F 0.327)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.692] [G acc: 0.188]\n",
      "2616 [D loss: (0.302)(R 0.160, F 0.445)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.209] [G acc: 0.312]\n",
      "2617 [D loss: (0.656)(R 0.791, F 0.522)] [D acc: (0.750)(0.750, 0.750)] [G loss: 4.881] [G acc: 0.125]\n",
      "2618 [D loss: (0.396)(R 0.444, F 0.347)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.708] [G acc: 0.250]\n",
      "2619 [D loss: (0.530)(R 0.454, F 0.607)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.969] [G acc: 0.188]\n",
      "2620 [D loss: (0.560)(R 0.610, F 0.510)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.231] [G acc: 0.312]\n",
      "2621 [D loss: (0.366)(R 0.119, F 0.613)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.758] [G acc: 0.250]\n",
      "2622 [D loss: (0.470)(R 0.636, F 0.304)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.707] [G acc: 0.250]\n",
      "2623 [D loss: (0.250)(R 0.219, F 0.282)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.230] [G acc: 0.188]\n",
      "2624 [D loss: (0.136)(R 0.155, F 0.116)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.617] [G acc: 0.375]\n",
      "2625 [D loss: (0.242)(R 0.131, F 0.353)] [D acc: (0.906)(1.000, 0.812)] [G loss: 6.755] [G acc: 0.188]\n",
      "2626 [D loss: (0.311)(R 0.175, F 0.448)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.817] [G acc: 0.188]\n",
      "2627 [D loss: (0.574)(R 0.594, F 0.553)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.001] [G acc: 0.062]\n",
      "2628 [D loss: (0.462)(R 0.404, F 0.520)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.858] [G acc: 0.250]\n",
      "2629 [D loss: (0.400)(R 0.380, F 0.419)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.339] [G acc: 0.125]\n",
      "2630 [D loss: (0.589)(R 0.513, F 0.665)] [D acc: (0.719)(0.812, 0.625)] [G loss: 4.985] [G acc: 0.062]\n",
      "2631 [D loss: (0.249)(R 0.286, F 0.212)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.420] [G acc: 0.125]\n",
      "2632 [D loss: (0.348)(R 0.141, F 0.554)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.554] [G acc: 0.188]\n",
      "2633 [D loss: (0.228)(R 0.195, F 0.260)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.562] [G acc: 0.188]\n",
      "2634 [D loss: (0.410)(R 0.356, F 0.463)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.013] [G acc: 0.188]\n",
      "2635 [D loss: (0.503)(R 0.702, F 0.305)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.442] [G acc: 0.125]\n",
      "2636 [D loss: (0.630)(R 0.584, F 0.675)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.605] [G acc: 0.188]\n",
      "2637 [D loss: (0.329)(R 0.492, F 0.167)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.620] [G acc: 0.188]\n",
      "2638 [D loss: (0.179)(R 0.244, F 0.114)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.355] [G acc: 0.188]\n",
      "2639 [D loss: (0.344)(R 0.444, F 0.245)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.176] [G acc: 0.188]\n",
      "2640 [D loss: (0.263)(R 0.164, F 0.363)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.386] [G acc: 0.312]\n",
      "2641 [D loss: (0.128)(R 0.233, F 0.024)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.830] [G acc: 0.250]\n",
      "2642 [D loss: (0.398)(R 0.274, F 0.522)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.450] [G acc: 0.250]\n",
      "2643 [D loss: (0.809)(R 0.991, F 0.628)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.435] [G acc: 0.188]\n",
      "2644 [D loss: (0.539)(R 0.322, F 0.756)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.295] [G acc: 0.375]\n",
      "2645 [D loss: (0.413)(R 0.482, F 0.344)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.369] [G acc: 0.250]\n",
      "2646 [D loss: (0.449)(R 0.305, F 0.593)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.608] [G acc: 0.188]\n",
      "2647 [D loss: (0.250)(R 0.150, F 0.350)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.319] [G acc: 0.312]\n",
      "2648 [D loss: (0.600)(R 0.660, F 0.540)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.879] [G acc: 0.125]\n",
      "2649 [D loss: (0.331)(R 0.268, F 0.395)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.321] [G acc: 0.312]\n",
      "2650 [D loss: (0.293)(R 0.358, F 0.228)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.893] [G acc: 0.188]\n",
      "2651 [D loss: (0.396)(R 0.399, F 0.393)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.406] [G acc: 0.312]\n",
      "2652 [D loss: (0.327)(R 0.200, F 0.453)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.652] [G acc: 0.125]\n",
      "2653 [D loss: (0.198)(R 0.235, F 0.161)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.101] [G acc: 0.250]\n",
      "2654 [D loss: (0.422)(R 0.457, F 0.387)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.303] [G acc: 0.250]\n",
      "2655 [D loss: (0.127)(R 0.220, F 0.034)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.587] [G acc: 0.125]\n",
      "2656 [D loss: (0.258)(R 0.355, F 0.162)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.154] [G acc: 0.125]\n",
      "2657 [D loss: (0.249)(R 0.174, F 0.323)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.693] [G acc: 0.312]\n",
      "2658 [D loss: (0.374)(R 0.307, F 0.442)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.748] [G acc: 0.250]\n",
      "2659 [D loss: (0.427)(R 0.308, F 0.546)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.344] [G acc: 0.250]\n",
      "2660 [D loss: (0.471)(R 0.521, F 0.422)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.160] [G acc: 0.250]\n",
      "2661 [D loss: (0.125)(R 0.166, F 0.085)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.948] [G acc: 0.125]\n",
      "2662 [D loss: (0.418)(R 0.166, F 0.670)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.054] [G acc: 0.250]\n",
      "2663 [D loss: (0.173)(R 0.239, F 0.106)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.177] [G acc: 0.000]\n",
      "2664 [D loss: (0.099)(R 0.148, F 0.051)] [D acc: (1.000)(1.000, 1.000)] [G loss: 5.084] [G acc: 0.250]\n",
      "2665 [D loss: (0.516)(R 0.565, F 0.467)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.583] [G acc: 0.125]\n",
      "2666 [D loss: (0.315)(R 0.152, F 0.478)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.610] [G acc: 0.250]\n",
      "2667 [D loss: (0.789)(R 1.345, F 0.234)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.121] [G acc: 0.250]\n",
      "2668 [D loss: (0.437)(R 0.319, F 0.554)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.345] [G acc: 0.250]\n",
      "2669 [D loss: (0.361)(R 0.516, F 0.207)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.634] [G acc: 0.250]\n",
      "2670 [D loss: (0.244)(R 0.172, F 0.316)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.303] [G acc: 0.188]\n",
      "2671 [D loss: (0.441)(R 0.564, F 0.319)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.688] [G acc: 0.250]\n",
      "2672 [D loss: (0.216)(R 0.109, F 0.322)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.667] [G acc: 0.250]\n",
      "2673 [D loss: (0.242)(R 0.226, F 0.258)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.996] [G acc: 0.125]\n",
      "2674 [D loss: (0.716)(R 0.814, F 0.618)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.434] [G acc: 0.250]\n",
      "2675 [D loss: (0.173)(R 0.299, F 0.047)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.292] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2676 [D loss: (0.396)(R 0.425, F 0.368)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.577] [G acc: 0.188]\n",
      "2677 [D loss: (0.189)(R 0.155, F 0.223)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.620] [G acc: 0.250]\n",
      "2678 [D loss: (0.876)(R 1.323, F 0.429)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.647] [G acc: 0.125]\n",
      "2679 [D loss: (0.420)(R 0.470, F 0.370)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.205] [G acc: 0.188]\n",
      "2680 [D loss: (0.238)(R 0.176, F 0.299)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.520] [G acc: 0.188]\n",
      "2681 [D loss: (0.595)(R 0.524, F 0.665)] [D acc: (0.719)(0.875, 0.562)] [G loss: 5.303] [G acc: 0.188]\n",
      "2682 [D loss: (0.251)(R 0.169, F 0.334)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.049] [G acc: 0.312]\n",
      "2683 [D loss: (0.428)(R 0.629, F 0.227)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.572] [G acc: 0.188]\n",
      "2684 [D loss: (0.306)(R 0.283, F 0.329)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.321] [G acc: 0.188]\n",
      "2685 [D loss: (0.333)(R 0.342, F 0.324)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.896] [G acc: 0.125]\n",
      "2686 [D loss: (0.298)(R 0.413, F 0.183)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.553] [G acc: 0.125]\n",
      "2687 [D loss: (0.402)(R 0.473, F 0.330)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.834] [G acc: 0.250]\n",
      "2688 [D loss: (0.210)(R 0.162, F 0.259)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.598] [G acc: 0.312]\n",
      "2689 [D loss: (0.442)(R 0.474, F 0.410)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.790] [G acc: 0.188]\n",
      "2690 [D loss: (0.383)(R 0.161, F 0.604)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.576] [G acc: 0.188]\n",
      "2691 [D loss: (0.261)(R 0.196, F 0.327)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.244] [G acc: 0.312]\n",
      "2692 [D loss: (0.139)(R 0.244, F 0.035)] [D acc: (0.938)(0.875, 1.000)] [G loss: 3.551] [G acc: 0.375]\n",
      "2693 [D loss: (0.318)(R 0.365, F 0.271)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.990] [G acc: 0.312]\n",
      "2694 [D loss: (0.274)(R 0.152, F 0.395)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.601] [G acc: 0.125]\n",
      "2695 [D loss: (0.506)(R 0.250, F 0.763)] [D acc: (0.750)(0.938, 0.562)] [G loss: 4.453] [G acc: 0.250]\n",
      "2696 [D loss: (0.169)(R 0.195, F 0.142)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.089] [G acc: 0.125]\n",
      "2697 [D loss: (0.506)(R 0.154, F 0.858)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.748] [G acc: 0.125]\n",
      "2698 [D loss: (0.222)(R 0.190, F 0.255)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.447] [G acc: 0.188]\n",
      "2699 [D loss: (0.120)(R 0.213, F 0.028)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.931] [G acc: 0.062]\n",
      "2700 [D loss: (0.161)(R 0.214, F 0.108)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.518] [G acc: 0.125]\n",
      "2701 [D loss: (0.344)(R 0.286, F 0.402)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.409] [G acc: 0.250]\n",
      "2702 [D loss: (0.370)(R 0.178, F 0.562)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.096] [G acc: 0.250]\n",
      "2703 [D loss: (0.456)(R 0.476, F 0.435)] [D acc: (0.812)(0.812, 0.812)] [G loss: 4.757] [G acc: 0.125]\n",
      "2704 [D loss: (0.469)(R 0.373, F 0.566)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.681] [G acc: 0.312]\n",
      "2705 [D loss: (0.307)(R 0.395, F 0.220)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.646] [G acc: 0.250]\n",
      "2706 [D loss: (0.535)(R 0.585, F 0.485)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.962] [G acc: 0.250]\n",
      "2707 [D loss: (0.360)(R 0.118, F 0.601)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.800] [G acc: 0.125]\n",
      "2708 [D loss: (0.453)(R 0.503, F 0.404)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.837] [G acc: 0.125]\n",
      "2709 [D loss: (0.387)(R 0.259, F 0.516)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.219] [G acc: 0.312]\n",
      "2710 [D loss: (0.256)(R 0.152, F 0.360)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.068] [G acc: 0.125]\n",
      "2711 [D loss: (0.400)(R 0.264, F 0.537)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.446] [G acc: 0.188]\n",
      "2712 [D loss: (0.286)(R 0.277, F 0.296)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.541] [G acc: 0.188]\n",
      "2713 [D loss: (0.585)(R 0.852, F 0.317)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.806] [G acc: 0.312]\n",
      "2714 [D loss: (0.552)(R 0.343, F 0.762)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.884] [G acc: 0.188]\n",
      "2715 [D loss: (0.408)(R 0.292, F 0.525)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.548] [G acc: 0.125]\n",
      "2716 [D loss: (0.448)(R 0.272, F 0.624)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.671] [G acc: 0.312]\n",
      "2717 [D loss: (0.235)(R 0.275, F 0.196)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.122] [G acc: 0.188]\n",
      "2718 [D loss: (0.329)(R 0.277, F 0.382)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.764] [G acc: 0.312]\n",
      "2719 [D loss: (0.356)(R 0.180, F 0.531)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.427] [G acc: 0.062]\n",
      "2720 [D loss: (0.337)(R 0.251, F 0.424)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.809] [G acc: 0.188]\n",
      "2721 [D loss: (0.342)(R 0.171, F 0.513)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.294] [G acc: 0.188]\n",
      "2722 [D loss: (0.471)(R 0.655, F 0.287)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.049] [G acc: 0.250]\n",
      "2723 [D loss: (0.460)(R 0.330, F 0.591)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.949] [G acc: 0.188]\n",
      "2724 [D loss: (0.253)(R 0.207, F 0.298)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.891] [G acc: 0.125]\n",
      "2725 [D loss: (0.441)(R 0.487, F 0.395)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.455] [G acc: 0.188]\n",
      "2726 [D loss: (0.357)(R 0.270, F 0.444)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.475] [G acc: 0.125]\n",
      "2727 [D loss: (0.458)(R 0.501, F 0.415)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.144] [G acc: 0.188]\n",
      "2728 [D loss: (0.282)(R 0.151, F 0.412)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.028] [G acc: 0.125]\n",
      "2729 [D loss: (0.427)(R 0.642, F 0.213)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.756] [G acc: 0.125]\n",
      "2730 [D loss: (0.171)(R 0.207, F 0.134)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.501] [G acc: 0.250]\n",
      "2731 [D loss: (0.240)(R 0.164, F 0.316)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.021] [G acc: 0.125]\n",
      "2732 [D loss: (0.526)(R 0.507, F 0.545)] [D acc: (0.719)(0.750, 0.688)] [G loss: 4.485] [G acc: 0.188]\n",
      "2733 [D loss: (0.667)(R 0.701, F 0.633)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.889] [G acc: 0.125]\n",
      "2734 [D loss: (0.374)(R 0.417, F 0.332)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.016] [G acc: 0.250]\n",
      "2735 [D loss: (0.374)(R 0.251, F 0.496)] [D acc: (0.875)(0.938, 0.812)] [G loss: 5.834] [G acc: 0.062]\n",
      "2736 [D loss: (0.320)(R 0.195, F 0.444)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.141] [G acc: 0.125]\n",
      "2737 [D loss: (0.414)(R 0.315, F 0.514)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.566] [G acc: 0.062]\n",
      "2738 [D loss: (0.423)(R 0.383, F 0.464)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.249] [G acc: 0.125]\n",
      "2739 [D loss: (0.364)(R 0.338, F 0.391)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.656] [G acc: 0.125]\n",
      "2740 [D loss: (0.487)(R 0.351, F 0.624)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.648] [G acc: 0.250]\n",
      "2741 [D loss: (0.403)(R 0.224, F 0.582)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.853] [G acc: 0.250]\n",
      "2742 [D loss: (0.673)(R 0.905, F 0.441)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.122] [G acc: 0.188]\n",
      "2743 [D loss: (0.554)(R 0.929, F 0.178)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.270] [G acc: 0.250]\n",
      "2744 [D loss: (0.215)(R 0.171, F 0.259)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.741] [G acc: 0.250]\n",
      "2745 [D loss: (0.387)(R 0.583, F 0.191)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.509] [G acc: 0.250]\n",
      "2746 [D loss: (0.688)(R 0.860, F 0.515)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.041] [G acc: 0.250]\n",
      "2747 [D loss: (0.540)(R 0.371, F 0.708)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.398] [G acc: 0.250]\n",
      "2748 [D loss: (0.478)(R 0.195, F 0.762)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.549] [G acc: 0.250]\n",
      "2749 [D loss: (0.261)(R 0.168, F 0.353)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.023] [G acc: 0.312]\n",
      "2750 [D loss: (0.258)(R 0.203, F 0.312)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.110] [G acc: 0.250]\n",
      "2751 [D loss: (0.272)(R 0.203, F 0.341)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.551] [G acc: 0.125]\n",
      "2752 [D loss: (0.292)(R 0.320, F 0.264)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.315] [G acc: 0.188]\n",
      "2753 [D loss: (0.211)(R 0.279, F 0.143)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.907] [G acc: 0.375]\n",
      "2754 [D loss: (0.550)(R 0.522, F 0.578)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.636] [G acc: 0.125]\n",
      "2755 [D loss: (0.194)(R 0.254, F 0.134)] [D acc: (0.938)(0.938, 0.938)] [G loss: 6.408] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2756 [D loss: (0.322)(R 0.402, F 0.242)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.392] [G acc: 0.125]\n",
      "2757 [D loss: (0.757)(R 0.843, F 0.671)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.118] [G acc: 0.312]\n",
      "2758 [D loss: (0.413)(R 0.174, F 0.652)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.522] [G acc: 0.188]\n",
      "2759 [D loss: (0.439)(R 0.428, F 0.450)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.149] [G acc: 0.188]\n",
      "2760 [D loss: (0.367)(R 0.310, F 0.425)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.430] [G acc: 0.125]\n",
      "2761 [D loss: (0.267)(R 0.223, F 0.311)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.086] [G acc: 0.125]\n",
      "2762 [D loss: (0.334)(R 0.226, F 0.441)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.669] [G acc: 0.250]\n",
      "2763 [D loss: (0.414)(R 0.255, F 0.573)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.894] [G acc: 0.312]\n",
      "2764 [D loss: (0.278)(R 0.282, F 0.274)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.449] [G acc: 0.250]\n",
      "2765 [D loss: (0.450)(R 0.372, F 0.529)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.617] [G acc: 0.250]\n",
      "2766 [D loss: (0.245)(R 0.199, F 0.292)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.504] [G acc: 0.250]\n",
      "2767 [D loss: (0.297)(R 0.210, F 0.384)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.413] [G acc: 0.188]\n",
      "2768 [D loss: (0.383)(R 0.291, F 0.474)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.645] [G acc: 0.188]\n",
      "2769 [D loss: (0.556)(R 0.656, F 0.455)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.637] [G acc: 0.062]\n",
      "2770 [D loss: (0.380)(R 0.177, F 0.584)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.259] [G acc: 0.188]\n",
      "2771 [D loss: (0.503)(R 0.562, F 0.445)] [D acc: (0.750)(0.750, 0.750)] [G loss: 4.219] [G acc: 0.188]\n",
      "2772 [D loss: (0.468)(R 0.218, F 0.719)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.671] [G acc: 0.188]\n",
      "2773 [D loss: (0.596)(R 0.960, F 0.232)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.903] [G acc: 0.250]\n",
      "2774 [D loss: (0.398)(R 0.241, F 0.555)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.204] [G acc: 0.125]\n",
      "2775 [D loss: (0.552)(R 0.568, F 0.536)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.071] [G acc: 0.125]\n",
      "2776 [D loss: (0.340)(R 0.202, F 0.479)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.783] [G acc: 0.250]\n",
      "2777 [D loss: (0.422)(R 0.265, F 0.579)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.036] [G acc: 0.250]\n",
      "2778 [D loss: (0.405)(R 0.256, F 0.554)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.078] [G acc: 0.125]\n",
      "2779 [D loss: (0.522)(R 0.650, F 0.395)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.690] [G acc: 0.125]\n",
      "2780 [D loss: (0.532)(R 0.723, F 0.342)] [D acc: (0.812)(0.750, 0.875)] [G loss: 3.634] [G acc: 0.188]\n",
      "2781 [D loss: (0.368)(R 0.314, F 0.421)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.285] [G acc: 0.062]\n",
      "2782 [D loss: (0.558)(R 0.812, F 0.304)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.465] [G acc: 0.125]\n",
      "2783 [D loss: (0.391)(R 0.199, F 0.583)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.756] [G acc: 0.250]\n",
      "2784 [D loss: (0.439)(R 0.338, F 0.541)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.274] [G acc: 0.188]\n",
      "2785 [D loss: (0.322)(R 0.250, F 0.394)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.057] [G acc: 0.250]\n",
      "2786 [D loss: (0.456)(R 0.380, F 0.532)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.451] [G acc: 0.250]\n",
      "2787 [D loss: (0.407)(R 0.461, F 0.352)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.491] [G acc: 0.312]\n",
      "2788 [D loss: (0.284)(R 0.194, F 0.373)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.831] [G acc: 0.312]\n",
      "2789 [D loss: (0.268)(R 0.409, F 0.127)] [D acc: (0.906)(0.812, 1.000)] [G loss: 4.065] [G acc: 0.312]\n",
      "2790 [D loss: (0.110)(R 0.185, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.122] [G acc: 0.062]\n",
      "2791 [D loss: (0.446)(R 0.519, F 0.374)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.998] [G acc: 0.250]\n",
      "2792 [D loss: (0.319)(R 0.228, F 0.411)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.381] [G acc: 0.250]\n",
      "2793 [D loss: (0.157)(R 0.165, F 0.148)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.654] [G acc: 0.312]\n",
      "2794 [D loss: (0.281)(R 0.232, F 0.330)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.874] [G acc: 0.062]\n",
      "2795 [D loss: (0.562)(R 0.646, F 0.478)] [D acc: (0.750)(0.750, 0.750)] [G loss: 3.115] [G acc: 0.125]\n",
      "2796 [D loss: (0.338)(R 0.216, F 0.459)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.502] [G acc: 0.125]\n",
      "2797 [D loss: (0.493)(R 0.476, F 0.509)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.386] [G acc: 0.125]\n",
      "2798 [D loss: (0.723)(R 0.911, F 0.535)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.596] [G acc: 0.375]\n",
      "2799 [D loss: (0.131)(R 0.170, F 0.092)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.510] [G acc: 0.188]\n",
      "2800 [D loss: (0.633)(R 0.670, F 0.596)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.167] [G acc: 0.312]\n",
      "2801 [D loss: (0.303)(R 0.263, F 0.344)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.738] [G acc: 0.312]\n",
      "2802 [D loss: (0.485)(R 0.645, F 0.325)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.808] [G acc: 0.250]\n",
      "2803 [D loss: (0.282)(R 0.245, F 0.318)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.010] [G acc: 0.062]\n",
      "2804 [D loss: (0.271)(R 0.342, F 0.200)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.708] [G acc: 0.188]\n",
      "2805 [D loss: (0.349)(R 0.344, F 0.354)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.877] [G acc: 0.125]\n",
      "2806 [D loss: (0.535)(R 0.769, F 0.301)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.939] [G acc: 0.125]\n",
      "2807 [D loss: (0.442)(R 0.225, F 0.659)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.997] [G acc: 0.188]\n",
      "2808 [D loss: (0.431)(R 0.303, F 0.560)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.053] [G acc: 0.312]\n",
      "2809 [D loss: (0.255)(R 0.225, F 0.284)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.515] [G acc: 0.250]\n",
      "2810 [D loss: (0.500)(R 0.351, F 0.649)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.552] [G acc: 0.250]\n",
      "2811 [D loss: (0.368)(R 0.217, F 0.519)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.103] [G acc: 0.312]\n",
      "2812 [D loss: (0.228)(R 0.267, F 0.189)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.709] [G acc: 0.188]\n",
      "2813 [D loss: (0.373)(R 0.291, F 0.455)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.323] [G acc: 0.188]\n",
      "2814 [D loss: (0.322)(R 0.346, F 0.297)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.867] [G acc: 0.125]\n",
      "2815 [D loss: (0.375)(R 0.254, F 0.496)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.349] [G acc: 0.250]\n",
      "2816 [D loss: (0.308)(R 0.252, F 0.364)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.487] [G acc: 0.188]\n",
      "2817 [D loss: (0.258)(R 0.278, F 0.237)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.978] [G acc: 0.250]\n",
      "2818 [D loss: (0.525)(R 0.456, F 0.595)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.166] [G acc: 0.375]\n",
      "2819 [D loss: (0.390)(R 0.460, F 0.320)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.590] [G acc: 0.250]\n",
      "2820 [D loss: (0.342)(R 0.317, F 0.367)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.797] [G acc: 0.312]\n",
      "2821 [D loss: (0.251)(R 0.226, F 0.276)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.848] [G acc: 0.062]\n",
      "2822 [D loss: (0.453)(R 0.314, F 0.591)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.222] [G acc: 0.125]\n",
      "2823 [D loss: (0.138)(R 0.216, F 0.060)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.483] [G acc: 0.312]\n",
      "2824 [D loss: (0.368)(R 0.457, F 0.279)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.859] [G acc: 0.250]\n",
      "2825 [D loss: (0.137)(R 0.197, F 0.076)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.365] [G acc: 0.188]\n",
      "2826 [D loss: (0.529)(R 0.393, F 0.665)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.962] [G acc: 0.312]\n",
      "2827 [D loss: (0.373)(R 0.448, F 0.297)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.311] [G acc: 0.250]\n",
      "2828 [D loss: (0.413)(R 0.378, F 0.447)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.034] [G acc: 0.250]\n",
      "2829 [D loss: (0.525)(R 0.356, F 0.695)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.261] [G acc: 0.188]\n",
      "2830 [D loss: (0.413)(R 0.222, F 0.604)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.032] [G acc: 0.250]\n",
      "2831 [D loss: (0.385)(R 0.340, F 0.431)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.466] [G acc: 0.250]\n",
      "2832 [D loss: (0.612)(R 0.630, F 0.593)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.337] [G acc: 0.188]\n",
      "2833 [D loss: (0.215)(R 0.166, F 0.265)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.269] [G acc: 0.062]\n",
      "2834 [D loss: (0.317)(R 0.371, F 0.262)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.721] [G acc: 0.188]\n",
      "2835 [D loss: (0.184)(R 0.200, F 0.167)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.330] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2836 [D loss: (0.440)(R 0.546, F 0.335)] [D acc: (0.812)(0.812, 0.812)] [G loss: 5.418] [G acc: 0.250]\n",
      "2837 [D loss: (0.407)(R 0.219, F 0.595)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.371] [G acc: 0.188]\n",
      "2838 [D loss: (0.340)(R 0.405, F 0.275)] [D acc: (0.875)(0.812, 0.938)] [G loss: 4.943] [G acc: 0.250]\n",
      "2839 [D loss: (0.603)(R 0.412, F 0.795)] [D acc: (0.750)(0.938, 0.562)] [G loss: 6.293] [G acc: 0.125]\n",
      "2840 [D loss: (0.342)(R 0.230, F 0.455)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.500] [G acc: 0.125]\n",
      "2841 [D loss: (0.336)(R 0.217, F 0.455)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.388] [G acc: 0.188]\n",
      "2842 [D loss: (0.476)(R 0.415, F 0.538)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.149] [G acc: 0.188]\n",
      "2843 [D loss: (0.387)(R 0.420, F 0.354)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.870] [G acc: 0.188]\n",
      "2844 [D loss: (0.276)(R 0.252, F 0.300)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.870] [G acc: 0.250]\n",
      "2845 [D loss: (0.367)(R 0.195, F 0.539)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.769] [G acc: 0.312]\n",
      "2846 [D loss: (0.311)(R 0.413, F 0.209)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.349] [G acc: 0.312]\n",
      "2847 [D loss: (0.452)(R 0.526, F 0.377)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.844] [G acc: 0.312]\n",
      "2848 [D loss: (0.141)(R 0.193, F 0.089)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.829] [G acc: 0.375]\n",
      "2849 [D loss: (0.258)(R 0.254, F 0.261)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.200] [G acc: 0.375]\n",
      "2850 [D loss: (0.434)(R 0.554, F 0.314)] [D acc: (0.812)(0.812, 0.812)] [G loss: 4.132] [G acc: 0.250]\n",
      "2851 [D loss: (0.203)(R 0.322, F 0.084)] [D acc: (0.938)(0.875, 1.000)] [G loss: 3.801] [G acc: 0.250]\n",
      "2852 [D loss: (0.660)(R 0.628, F 0.691)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.920] [G acc: 0.062]\n",
      "2853 [D loss: (0.411)(R 0.443, F 0.378)] [D acc: (0.750)(0.812, 0.688)] [G loss: 5.808] [G acc: 0.250]\n",
      "2854 [D loss: (0.288)(R 0.313, F 0.262)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.868] [G acc: 0.250]\n",
      "2855 [D loss: (0.364)(R 0.393, F 0.335)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.834] [G acc: 0.125]\n",
      "2856 [D loss: (0.611)(R 0.410, F 0.813)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.889] [G acc: 0.188]\n",
      "2857 [D loss: (0.378)(R 0.308, F 0.449)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.102] [G acc: 0.188]\n",
      "2858 [D loss: (0.293)(R 0.209, F 0.377)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.036] [G acc: 0.188]\n",
      "2859 [D loss: (0.267)(R 0.295, F 0.240)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.295] [G acc: 0.250]\n",
      "2860 [D loss: (0.447)(R 0.398, F 0.495)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.977] [G acc: 0.375]\n",
      "2861 [D loss: (0.384)(R 0.325, F 0.444)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.822] [G acc: 0.312]\n",
      "2862 [D loss: (0.452)(R 0.277, F 0.627)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.885] [G acc: 0.188]\n",
      "2863 [D loss: (0.334)(R 0.243, F 0.425)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.015] [G acc: 0.312]\n",
      "2864 [D loss: (0.370)(R 0.202, F 0.538)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.482] [G acc: 0.250]\n",
      "2865 [D loss: (0.429)(R 0.338, F 0.520)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.798] [G acc: 0.312]\n",
      "2866 [D loss: (0.280)(R 0.207, F 0.353)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.811] [G acc: 0.312]\n",
      "2867 [D loss: (0.514)(R 0.976, F 0.052)] [D acc: (0.875)(0.750, 1.000)] [G loss: 3.558] [G acc: 0.188]\n",
      "2868 [D loss: (0.250)(R 0.212, F 0.287)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.616] [G acc: 0.312]\n",
      "2869 [D loss: (0.251)(R 0.207, F 0.295)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.276] [G acc: 0.188]\n",
      "2870 [D loss: (0.464)(R 0.378, F 0.550)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.083] [G acc: 0.188]\n",
      "2871 [D loss: (0.162)(R 0.279, F 0.046)] [D acc: (0.938)(0.875, 1.000)] [G loss: 3.211] [G acc: 0.188]\n",
      "2872 [D loss: (0.238)(R 0.188, F 0.288)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.173] [G acc: 0.188]\n",
      "2873 [D loss: (0.362)(R 0.197, F 0.527)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.397] [G acc: 0.438]\n",
      "2874 [D loss: (0.185)(R 0.246, F 0.123)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.031] [G acc: 0.312]\n",
      "2875 [D loss: (0.184)(R 0.236, F 0.131)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.647] [G acc: 0.188]\n",
      "2876 [D loss: (0.247)(R 0.203, F 0.291)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.598] [G acc: 0.250]\n",
      "2877 [D loss: (0.505)(R 0.486, F 0.525)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.892] [G acc: 0.312]\n",
      "2878 [D loss: (0.314)(R 0.239, F 0.390)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.441] [G acc: 0.312]\n",
      "2879 [D loss: (0.284)(R 0.293, F 0.275)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.364] [G acc: 0.188]\n",
      "2880 [D loss: (0.395)(R 0.296, F 0.493)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.010] [G acc: 0.250]\n",
      "2881 [D loss: (0.465)(R 0.364, F 0.566)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.168] [G acc: 0.188]\n",
      "2882 [D loss: (0.308)(R 0.206, F 0.410)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.221] [G acc: 0.250]\n",
      "2883 [D loss: (0.454)(R 0.524, F 0.383)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.867] [G acc: 0.188]\n",
      "2884 [D loss: (0.306)(R 0.336, F 0.276)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.442] [G acc: 0.188]\n",
      "2885 [D loss: (0.343)(R 0.218, F 0.468)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.739] [G acc: 0.188]\n",
      "2886 [D loss: (0.319)(R 0.193, F 0.444)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.113] [G acc: 0.250]\n",
      "2887 [D loss: (0.161)(R 0.221, F 0.101)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.989] [G acc: 0.188]\n",
      "2888 [D loss: (0.340)(R 0.198, F 0.482)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.399] [G acc: 0.125]\n",
      "2889 [D loss: (0.325)(R 0.356, F 0.294)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.406] [G acc: 0.188]\n",
      "2890 [D loss: (0.377)(R 0.434, F 0.321)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.490] [G acc: 0.250]\n",
      "2891 [D loss: (0.356)(R 0.384, F 0.328)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.013] [G acc: 0.188]\n",
      "2892 [D loss: (0.451)(R 0.300, F 0.603)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.023] [G acc: 0.312]\n",
      "2893 [D loss: (0.389)(R 0.315, F 0.462)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.430] [G acc: 0.125]\n",
      "2894 [D loss: (0.172)(R 0.246, F 0.099)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.706] [G acc: 0.188]\n",
      "2895 [D loss: (0.407)(R 0.371, F 0.443)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.746] [G acc: 0.125]\n",
      "2896 [D loss: (0.391)(R 0.361, F 0.421)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.559] [G acc: 0.188]\n",
      "2897 [D loss: (0.344)(R 0.258, F 0.430)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.849] [G acc: 0.125]\n",
      "2898 [D loss: (0.460)(R 0.416, F 0.503)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.841] [G acc: 0.312]\n",
      "2899 [D loss: (0.456)(R 0.349, F 0.564)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.507] [G acc: 0.188]\n",
      "2900 [D loss: (0.429)(R 0.461, F 0.396)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.494] [G acc: 0.250]\n",
      "2901 [D loss: (0.293)(R 0.270, F 0.315)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.425] [G acc: 0.188]\n",
      "2902 [D loss: (0.354)(R 0.358, F 0.351)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.132] [G acc: 0.312]\n",
      "2903 [D loss: (0.517)(R 0.638, F 0.396)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.001] [G acc: 0.312]\n",
      "2904 [D loss: (0.445)(R 0.594, F 0.296)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.870] [G acc: 0.188]\n",
      "2905 [D loss: (0.610)(R 0.615, F 0.605)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.309] [G acc: 0.188]\n",
      "2906 [D loss: (0.316)(R 0.350, F 0.283)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.678] [G acc: 0.000]\n",
      "2907 [D loss: (0.337)(R 0.193, F 0.481)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.574] [G acc: 0.188]\n",
      "2908 [D loss: (0.332)(R 0.290, F 0.374)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.628] [G acc: 0.188]\n",
      "2909 [D loss: (0.714)(R 1.063, F 0.365)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.772] [G acc: 0.062]\n",
      "2910 [D loss: (0.421)(R 0.385, F 0.456)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.927] [G acc: 0.188]\n",
      "2911 [D loss: (0.353)(R 0.418, F 0.288)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.945] [G acc: 0.250]\n",
      "2912 [D loss: (0.415)(R 0.498, F 0.332)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.543] [G acc: 0.062]\n",
      "2913 [D loss: (0.542)(R 0.617, F 0.467)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.702] [G acc: 0.312]\n",
      "2914 [D loss: (0.452)(R 0.300, F 0.605)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.216] [G acc: 0.312]\n",
      "2915 [D loss: (0.396)(R 0.241, F 0.552)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.551] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2916 [D loss: (0.388)(R 0.389, F 0.387)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.688] [G acc: 0.188]\n",
      "2917 [D loss: (0.365)(R 0.268, F 0.463)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.217] [G acc: 0.188]\n",
      "2918 [D loss: (0.182)(R 0.207, F 0.157)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.582] [G acc: 0.250]\n",
      "2919 [D loss: (0.317)(R 0.222, F 0.412)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.961] [G acc: 0.250]\n",
      "2920 [D loss: (0.221)(R 0.303, F 0.139)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.356] [G acc: 0.125]\n",
      "2921 [D loss: (0.216)(R 0.271, F 0.161)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.605] [G acc: 0.125]\n",
      "2922 [D loss: (0.257)(R 0.208, F 0.306)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.549] [G acc: 0.125]\n",
      "2923 [D loss: (0.412)(R 0.522, F 0.302)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.804] [G acc: 0.375]\n",
      "2924 [D loss: (0.500)(R 0.672, F 0.328)] [D acc: (0.812)(0.812, 0.812)] [G loss: 4.117] [G acc: 0.188]\n",
      "2925 [D loss: (0.344)(R 0.198, F 0.490)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.489] [G acc: 0.312]\n",
      "2926 [D loss: (0.180)(R 0.203, F 0.157)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.122] [G acc: 0.250]\n",
      "2927 [D loss: (0.416)(R 0.378, F 0.454)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.435] [G acc: 0.312]\n",
      "2928 [D loss: (0.353)(R 0.281, F 0.425)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.128] [G acc: 0.250]\n",
      "2929 [D loss: (0.627)(R 0.488, F 0.767)] [D acc: (0.688)(0.812, 0.562)] [G loss: 3.304] [G acc: 0.250]\n",
      "2930 [D loss: (0.856)(R 1.141, F 0.572)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.720] [G acc: 0.312]\n",
      "2931 [D loss: (0.310)(R 0.235, F 0.384)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.860] [G acc: 0.312]\n",
      "2932 [D loss: (0.379)(R 0.301, F 0.457)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.832] [G acc: 0.250]\n",
      "2933 [D loss: (0.298)(R 0.221, F 0.376)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.586] [G acc: 0.250]\n",
      "2934 [D loss: (0.178)(R 0.229, F 0.127)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.113] [G acc: 0.188]\n",
      "2935 [D loss: (0.202)(R 0.247, F 0.157)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.450] [G acc: 0.250]\n",
      "2936 [D loss: (0.229)(R 0.237, F 0.220)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.679] [G acc: 0.312]\n",
      "2937 [D loss: (0.263)(R 0.281, F 0.244)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.356] [G acc: 0.188]\n",
      "2938 [D loss: (0.314)(R 0.297, F 0.330)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.560] [G acc: 0.250]\n",
      "2939 [D loss: (0.805)(R 0.638, F 0.972)] [D acc: (0.688)(0.938, 0.438)] [G loss: 3.731] [G acc: 0.375]\n",
      "2940 [D loss: (0.309)(R 0.202, F 0.415)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.326] [G acc: 0.312]\n",
      "2941 [D loss: (0.375)(R 0.347, F 0.404)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.262] [G acc: 0.438]\n",
      "2942 [D loss: (0.379)(R 0.466, F 0.292)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.117] [G acc: 0.188]\n",
      "2943 [D loss: (0.265)(R 0.277, F 0.254)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.986] [G acc: 0.375]\n",
      "2944 [D loss: (0.417)(R 0.224, F 0.610)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.171] [G acc: 0.250]\n",
      "2945 [D loss: (0.600)(R 1.020, F 0.179)] [D acc: (0.938)(0.938, 0.938)] [G loss: 5.592] [G acc: 0.188]\n",
      "2946 [D loss: (0.243)(R 0.238, F 0.249)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.851] [G acc: 0.188]\n",
      "2947 [D loss: (0.470)(R 0.413, F 0.526)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.850] [G acc: 0.188]\n",
      "2948 [D loss: (0.262)(R 0.296, F 0.229)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.043] [G acc: 0.250]\n",
      "2949 [D loss: (0.628)(R 0.946, F 0.309)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.533] [G acc: 0.188]\n",
      "2950 [D loss: (0.435)(R 0.231, F 0.639)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.548] [G acc: 0.250]\n",
      "2951 [D loss: (0.602)(R 0.920, F 0.284)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.251] [G acc: 0.375]\n",
      "2952 [D loss: (0.415)(R 0.280, F 0.550)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.987] [G acc: 0.188]\n",
      "2953 [D loss: (0.449)(R 0.456, F 0.442)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.416] [G acc: 0.250]\n",
      "2954 [D loss: (0.514)(R 0.543, F 0.485)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.351] [G acc: 0.125]\n",
      "2955 [D loss: (0.348)(R 0.214, F 0.483)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.785] [G acc: 0.125]\n",
      "2956 [D loss: (0.501)(R 0.443, F 0.559)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.711] [G acc: 0.250]\n",
      "2957 [D loss: (1.135)(R 1.495, F 0.775)] [D acc: (0.688)(0.812, 0.562)] [G loss: 3.460] [G acc: 0.250]\n",
      "2958 [D loss: (0.474)(R 0.348, F 0.601)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.790] [G acc: 0.250]\n",
      "2959 [D loss: (0.371)(R 0.328, F 0.414)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.859] [G acc: 0.312]\n",
      "2960 [D loss: (0.241)(R 0.229, F 0.252)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.525] [G acc: 0.188]\n",
      "2961 [D loss: (0.356)(R 0.285, F 0.427)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.012] [G acc: 0.250]\n",
      "2962 [D loss: (0.339)(R 0.258, F 0.420)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.467] [G acc: 0.188]\n",
      "2963 [D loss: (0.175)(R 0.198, F 0.152)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.010] [G acc: 0.312]\n",
      "2964 [D loss: (0.451)(R 0.235, F 0.667)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.606] [G acc: 0.188]\n",
      "2965 [D loss: (0.341)(R 0.197, F 0.486)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.175] [G acc: 0.062]\n",
      "2966 [D loss: (0.202)(R 0.231, F 0.174)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.465] [G acc: 0.250]\n",
      "2967 [D loss: (0.643)(R 0.889, F 0.396)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.340] [G acc: 0.188]\n",
      "2968 [D loss: (0.326)(R 0.267, F 0.386)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.599] [G acc: 0.250]\n",
      "2969 [D loss: (0.441)(R 0.415, F 0.466)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.353] [G acc: 0.312]\n",
      "2970 [D loss: (0.548)(R 0.347, F 0.749)] [D acc: (0.719)(0.938, 0.500)] [G loss: 4.152] [G acc: 0.312]\n",
      "2971 [D loss: (0.377)(R 0.253, F 0.501)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.300] [G acc: 0.375]\n",
      "2972 [D loss: (0.398)(R 0.359, F 0.437)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.886] [G acc: 0.250]\n",
      "2973 [D loss: (0.237)(R 0.208, F 0.266)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.061] [G acc: 0.250]\n",
      "2974 [D loss: (0.327)(R 0.252, F 0.402)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.742] [G acc: 0.188]\n",
      "2975 [D loss: (0.335)(R 0.351, F 0.319)] [D acc: (0.844)(0.875, 0.812)] [G loss: 6.344] [G acc: 0.000]\n",
      "2976 [D loss: (0.211)(R 0.252, F 0.170)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.484] [G acc: 0.062]\n",
      "2977 [D loss: (0.482)(R 0.673, F 0.291)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.986] [G acc: 0.188]\n",
      "2978 [D loss: (0.285)(R 0.232, F 0.337)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.695] [G acc: 0.062]\n",
      "2979 [D loss: (0.234)(R 0.244, F 0.225)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.896] [G acc: 0.125]\n",
      "2980 [D loss: (0.266)(R 0.219, F 0.313)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.290] [G acc: 0.250]\n",
      "2981 [D loss: (0.357)(R 0.251, F 0.462)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.634] [G acc: 0.125]\n",
      "2982 [D loss: (0.256)(R 0.241, F 0.272)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.233] [G acc: 0.250]\n",
      "2983 [D loss: (0.406)(R 0.429, F 0.383)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.788] [G acc: 0.125]\n",
      "2984 [D loss: (0.570)(R 0.623, F 0.517)] [D acc: (0.750)(0.812, 0.688)] [G loss: 4.531] [G acc: 0.125]\n",
      "2985 [D loss: (0.568)(R 0.641, F 0.494)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.133] [G acc: 0.125]\n",
      "2986 [D loss: (0.533)(R 0.617, F 0.449)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.470] [G acc: 0.188]\n",
      "2987 [D loss: (0.455)(R 0.269, F 0.641)] [D acc: (0.781)(1.000, 0.562)] [G loss: 3.530] [G acc: 0.188]\n",
      "2988 [D loss: (0.597)(R 0.641, F 0.553)] [D acc: (0.625)(0.688, 0.562)] [G loss: 3.711] [G acc: 0.375]\n",
      "2989 [D loss: (0.283)(R 0.318, F 0.249)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.900] [G acc: 0.188]\n",
      "2990 [D loss: (0.330)(R 0.448, F 0.211)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.055] [G acc: 0.312]\n",
      "2991 [D loss: (0.300)(R 0.231, F 0.368)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.996] [G acc: 0.188]\n",
      "2992 [D loss: (0.485)(R 0.627, F 0.344)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.222] [G acc: 0.250]\n",
      "2993 [D loss: (0.728)(R 0.862, F 0.594)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.458] [G acc: 0.250]\n",
      "2994 [D loss: (0.563)(R 0.658, F 0.468)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.247] [G acc: 0.125]\n",
      "2995 [D loss: (0.274)(R 0.249, F 0.299)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.895] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996 [D loss: (0.304)(R 0.341, F 0.266)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.871] [G acc: 0.062]\n",
      "2997 [D loss: (0.201)(R 0.204, F 0.199)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.213] [G acc: 0.062]\n",
      "2998 [D loss: (0.276)(R 0.300, F 0.252)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.625] [G acc: 0.125]\n",
      "2999 [D loss: (0.777)(R 0.805, F 0.749)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.036] [G acc: 0.312]\n",
      "3000 [D loss: (0.387)(R 0.263, F 0.511)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.904] [G acc: 0.062]\n",
      "3001 [D loss: (0.473)(R 0.398, F 0.547)] [D acc: (0.781)(0.938, 0.625)] [G loss: 5.641] [G acc: 0.062]\n",
      "3002 [D loss: (0.270)(R 0.193, F 0.346)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.849] [G acc: 0.125]\n",
      "3003 [D loss: (0.495)(R 0.652, F 0.337)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.157] [G acc: 0.125]\n",
      "3004 [D loss: (0.529)(R 0.741, F 0.318)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.237] [G acc: 0.375]\n",
      "3005 [D loss: (0.390)(R 0.561, F 0.219)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.437] [G acc: 0.188]\n",
      "3006 [D loss: (0.182)(R 0.265, F 0.099)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.337] [G acc: 0.062]\n",
      "3007 [D loss: (0.308)(R 0.211, F 0.406)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.950] [G acc: 0.188]\n",
      "3008 [D loss: (0.525)(R 0.673, F 0.377)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.200] [G acc: 0.250]\n",
      "3009 [D loss: (0.355)(R 0.258, F 0.452)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.290] [G acc: 0.312]\n",
      "3010 [D loss: (0.315)(R 0.549, F 0.080)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.726] [G acc: 0.188]\n",
      "3011 [D loss: (0.297)(R 0.369, F 0.225)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.854] [G acc: 0.312]\n",
      "3012 [D loss: (0.449)(R 0.465, F 0.433)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.020] [G acc: 0.250]\n",
      "3013 [D loss: (0.416)(R 0.506, F 0.326)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.257] [G acc: 0.188]\n",
      "3014 [D loss: (0.285)(R 0.263, F 0.306)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.222] [G acc: 0.312]\n",
      "3015 [D loss: (0.803)(R 1.228, F 0.377)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.553] [G acc: 0.250]\n",
      "3016 [D loss: (0.426)(R 0.607, F 0.244)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.904] [G acc: 0.188]\n",
      "3017 [D loss: (0.368)(R 0.176, F 0.559)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.298] [G acc: 0.250]\n",
      "3018 [D loss: (0.493)(R 0.297, F 0.689)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.072] [G acc: 0.500]\n",
      "3019 [D loss: (0.442)(R 0.212, F 0.673)] [D acc: (0.812)(1.000, 0.625)] [G loss: 5.086] [G acc: 0.188]\n",
      "3020 [D loss: (0.411)(R 0.185, F 0.638)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.858] [G acc: 0.375]\n",
      "3021 [D loss: (0.392)(R 0.249, F 0.535)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.127] [G acc: 0.188]\n",
      "3022 [D loss: (0.431)(R 0.389, F 0.473)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.439] [G acc: 0.250]\n",
      "3023 [D loss: (0.269)(R 0.381, F 0.157)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.839] [G acc: 0.188]\n",
      "3024 [D loss: (0.999)(R 1.219, F 0.780)] [D acc: (0.656)(0.750, 0.562)] [G loss: 3.318] [G acc: 0.188]\n",
      "3025 [D loss: (0.342)(R 0.184, F 0.500)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.778] [G acc: 0.375]\n",
      "3026 [D loss: (0.578)(R 0.609, F 0.546)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.600] [G acc: 0.312]\n",
      "3027 [D loss: (0.337)(R 0.265, F 0.409)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.868] [G acc: 0.188]\n",
      "3028 [D loss: (0.373)(R 0.456, F 0.290)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.519] [G acc: 0.250]\n",
      "3029 [D loss: (0.445)(R 0.658, F 0.232)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.768] [G acc: 0.312]\n",
      "3030 [D loss: (0.340)(R 0.238, F 0.443)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.142] [G acc: 0.312]\n",
      "3031 [D loss: (0.282)(R 0.169, F 0.394)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.719] [G acc: 0.250]\n",
      "3032 [D loss: (0.521)(R 0.574, F 0.469)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.214] [G acc: 0.312]\n",
      "3033 [D loss: (0.259)(R 0.361, F 0.156)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.792] [G acc: 0.188]\n",
      "3034 [D loss: (0.405)(R 0.212, F 0.598)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.955] [G acc: 0.312]\n",
      "3035 [D loss: (0.945)(R 1.629, F 0.260)] [D acc: (0.812)(0.750, 0.875)] [G loss: 4.533] [G acc: 0.188]\n",
      "3036 [D loss: (0.453)(R 0.511, F 0.396)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.954] [G acc: 0.188]\n",
      "3037 [D loss: (0.354)(R 0.288, F 0.420)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.346] [G acc: 0.312]\n",
      "3038 [D loss: (0.289)(R 0.237, F 0.341)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.538] [G acc: 0.188]\n",
      "3039 [D loss: (0.240)(R 0.242, F 0.238)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.707] [G acc: 0.188]\n",
      "3040 [D loss: (0.455)(R 0.468, F 0.443)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.222] [G acc: 0.375]\n",
      "3041 [D loss: (0.269)(R 0.231, F 0.308)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.395] [G acc: 0.312]\n",
      "3042 [D loss: (0.579)(R 0.675, F 0.484)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.212] [G acc: 0.312]\n",
      "3043 [D loss: (0.434)(R 0.250, F 0.617)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.298] [G acc: 0.250]\n",
      "3044 [D loss: (0.260)(R 0.166, F 0.354)] [D acc: (0.938)(1.000, 0.875)] [G loss: 1.817] [G acc: 0.438]\n",
      "3045 [D loss: (0.395)(R 0.319, F 0.472)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.919] [G acc: 0.312]\n",
      "3046 [D loss: (0.288)(R 0.226, F 0.349)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.474] [G acc: 0.250]\n",
      "3047 [D loss: (0.243)(R 0.230, F 0.257)] [D acc: (0.938)(1.000, 0.875)] [G loss: 1.958] [G acc: 0.188]\n",
      "3048 [D loss: (0.369)(R 0.440, F 0.298)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.957] [G acc: 0.188]\n",
      "3049 [D loss: (0.498)(R 0.744, F 0.253)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.012] [G acc: 0.312]\n",
      "3050 [D loss: (0.192)(R 0.257, F 0.126)] [D acc: (0.969)(0.938, 1.000)] [G loss: 2.835] [G acc: 0.250]\n",
      "3051 [D loss: (0.420)(R 0.298, F 0.541)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.419] [G acc: 0.250]\n",
      "3052 [D loss: (0.525)(R 0.206, F 0.843)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.520] [G acc: 0.250]\n",
      "3053 [D loss: (0.363)(R 0.273, F 0.453)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.105] [G acc: 0.188]\n",
      "3054 [D loss: (0.332)(R 0.453, F 0.211)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.280] [G acc: 0.188]\n",
      "3055 [D loss: (0.461)(R 0.666, F 0.257)] [D acc: (0.781)(0.688, 0.875)] [G loss: 2.017] [G acc: 0.312]\n",
      "3056 [D loss: (0.495)(R 0.231, F 0.760)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.747] [G acc: 0.188]\n",
      "3057 [D loss: (0.441)(R 0.190, F 0.692)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.556] [G acc: 0.312]\n",
      "3058 [D loss: (0.471)(R 0.297, F 0.645)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.851] [G acc: 0.188]\n",
      "3059 [D loss: (0.310)(R 0.240, F 0.379)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.081] [G acc: 0.250]\n",
      "3060 [D loss: (0.512)(R 0.440, F 0.583)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.305] [G acc: 0.188]\n",
      "3061 [D loss: (0.223)(R 0.203, F 0.242)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.215] [G acc: 0.062]\n",
      "3062 [D loss: (0.457)(R 0.624, F 0.290)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.135] [G acc: 0.250]\n",
      "3063 [D loss: (0.427)(R 0.449, F 0.405)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.961] [G acc: 0.250]\n",
      "3064 [D loss: (0.199)(R 0.245, F 0.152)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.764] [G acc: 0.125]\n",
      "3065 [D loss: (0.215)(R 0.180, F 0.249)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.081] [G acc: 0.188]\n",
      "3066 [D loss: (0.318)(R 0.172, F 0.464)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.097] [G acc: 0.188]\n",
      "3067 [D loss: (0.288)(R 0.349, F 0.227)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.768] [G acc: 0.188]\n",
      "3068 [D loss: (0.556)(R 0.276, F 0.836)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.991] [G acc: 0.312]\n",
      "3069 [D loss: (0.402)(R 0.413, F 0.391)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.728] [G acc: 0.250]\n",
      "3070 [D loss: (0.365)(R 0.160, F 0.569)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.550] [G acc: 0.438]\n",
      "3071 [D loss: (0.241)(R 0.139, F 0.343)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.097] [G acc: 0.250]\n",
      "3072 [D loss: (0.457)(R 0.366, F 0.548)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.920] [G acc: 0.312]\n",
      "3073 [D loss: (0.466)(R 0.394, F 0.537)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.974] [G acc: 0.250]\n",
      "3074 [D loss: (0.389)(R 0.260, F 0.518)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.799] [G acc: 0.188]\n",
      "3075 [D loss: (0.299)(R 0.355, F 0.243)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.789] [G acc: 0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3076 [D loss: (0.353)(R 0.173, F 0.532)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.532] [G acc: 0.188]\n",
      "3077 [D loss: (0.243)(R 0.235, F 0.250)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.182] [G acc: 0.188]\n",
      "3078 [D loss: (0.356)(R 0.243, F 0.469)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.174] [G acc: 0.125]\n",
      "3079 [D loss: (0.447)(R 0.372, F 0.521)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.854] [G acc: 0.312]\n",
      "3080 [D loss: (0.315)(R 0.163, F 0.467)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.690] [G acc: 0.312]\n",
      "3081 [D loss: (0.432)(R 0.339, F 0.525)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.324] [G acc: 0.250]\n",
      "3082 [D loss: (0.317)(R 0.192, F 0.442)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.381] [G acc: 0.250]\n",
      "3083 [D loss: (0.273)(R 0.364, F 0.182)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.051] [G acc: 0.250]\n",
      "3084 [D loss: (0.351)(R 0.328, F 0.374)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.706] [G acc: 0.375]\n",
      "3085 [D loss: (0.691)(R 0.647, F 0.735)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.719] [G acc: 0.250]\n",
      "3086 [D loss: (0.286)(R 0.229, F 0.342)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.032] [G acc: 0.375]\n",
      "3087 [D loss: (0.433)(R 0.378, F 0.489)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.519] [G acc: 0.188]\n",
      "3088 [D loss: (0.395)(R 0.203, F 0.588)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.001] [G acc: 0.312]\n",
      "3089 [D loss: (0.364)(R 0.413, F 0.314)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.852] [G acc: 0.438]\n",
      "3090 [D loss: (0.352)(R 0.310, F 0.395)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.953] [G acc: 0.375]\n",
      "3091 [D loss: (0.345)(R 0.249, F 0.441)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.727] [G acc: 0.188]\n",
      "3092 [D loss: (0.461)(R 0.451, F 0.471)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.410] [G acc: 0.438]\n",
      "3093 [D loss: (0.960)(R 1.260, F 0.660)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.417] [G acc: 0.250]\n",
      "3094 [D loss: (0.443)(R 0.382, F 0.504)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.726] [G acc: 0.250]\n",
      "3095 [D loss: (0.687)(R 0.804, F 0.571)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.179] [G acc: 0.188]\n",
      "3096 [D loss: (0.449)(R 0.468, F 0.430)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.549] [G acc: 0.312]\n",
      "3097 [D loss: (0.422)(R 0.271, F 0.574)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.234] [G acc: 0.250]\n",
      "3098 [D loss: (0.309)(R 0.273, F 0.345)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.389] [G acc: 0.250]\n",
      "3099 [D loss: (0.254)(R 0.265, F 0.243)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.679] [G acc: 0.250]\n",
      "3100 [D loss: (0.267)(R 0.269, F 0.264)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.173] [G acc: 0.125]\n",
      "3101 [D loss: (0.218)(R 0.160, F 0.277)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.920] [G acc: 0.188]\n",
      "3102 [D loss: (0.154)(R 0.218, F 0.091)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.111] [G acc: 0.125]\n",
      "3103 [D loss: (0.448)(R 0.371, F 0.525)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.916] [G acc: 0.188]\n",
      "3104 [D loss: (0.320)(R 0.230, F 0.411)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.235] [G acc: 0.062]\n",
      "3105 [D loss: (0.388)(R 0.327, F 0.450)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.610] [G acc: 0.062]\n",
      "3106 [D loss: (0.455)(R 0.540, F 0.370)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.244] [G acc: 0.125]\n",
      "3107 [D loss: (0.535)(R 0.541, F 0.529)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.835] [G acc: 0.125]\n",
      "3108 [D loss: (0.326)(R 0.241, F 0.411)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.047] [G acc: 0.250]\n",
      "3109 [D loss: (0.191)(R 0.210, F 0.171)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.604] [G acc: 0.188]\n",
      "3110 [D loss: (0.408)(R 0.365, F 0.450)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.330] [G acc: 0.188]\n",
      "3111 [D loss: (0.703)(R 1.107, F 0.298)] [D acc: (0.812)(0.750, 0.875)] [G loss: 4.012] [G acc: 0.188]\n",
      "3112 [D loss: (0.200)(R 0.214, F 0.186)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.159] [G acc: 0.250]\n",
      "3113 [D loss: (0.280)(R 0.223, F 0.337)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.686] [G acc: 0.312]\n",
      "3114 [D loss: (0.344)(R 0.214, F 0.474)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.517] [G acc: 0.250]\n",
      "3115 [D loss: (0.372)(R 0.337, F 0.408)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.291] [G acc: 0.125]\n",
      "3116 [D loss: (0.322)(R 0.228, F 0.416)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.390] [G acc: 0.312]\n",
      "3117 [D loss: (0.404)(R 0.597, F 0.210)] [D acc: (0.875)(0.812, 0.938)] [G loss: 5.069] [G acc: 0.188]\n",
      "3118 [D loss: (0.248)(R 0.190, F 0.307)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.685] [G acc: 0.375]\n",
      "3119 [D loss: (0.172)(R 0.186, F 0.157)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.054] [G acc: 0.312]\n",
      "3120 [D loss: (0.588)(R 0.835, F 0.341)] [D acc: (0.812)(0.750, 0.875)] [G loss: 3.333] [G acc: 0.375]\n",
      "3121 [D loss: (0.245)(R 0.354, F 0.136)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.744] [G acc: 0.250]\n",
      "3122 [D loss: (0.391)(R 0.300, F 0.481)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.042] [G acc: 0.312]\n",
      "3123 [D loss: (0.269)(R 0.183, F 0.355)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.392] [G acc: 0.125]\n",
      "3124 [D loss: (0.389)(R 0.311, F 0.466)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.779] [G acc: 0.188]\n",
      "3125 [D loss: (0.830)(R 1.014, F 0.646)] [D acc: (0.719)(0.812, 0.625)] [G loss: 4.251] [G acc: 0.312]\n",
      "3126 [D loss: (0.448)(R 0.381, F 0.515)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.210] [G acc: 0.188]\n",
      "3127 [D loss: (0.635)(R 0.517, F 0.754)] [D acc: (0.688)(0.875, 0.500)] [G loss: 3.225] [G acc: 0.188]\n",
      "3128 [D loss: (0.208)(R 0.268, F 0.149)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.669] [G acc: 0.312]\n",
      "3129 [D loss: (0.362)(R 0.226, F 0.497)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.872] [G acc: 0.312]\n",
      "3130 [D loss: (0.316)(R 0.392, F 0.239)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.827] [G acc: 0.125]\n",
      "3131 [D loss: (0.271)(R 0.171, F 0.371)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.634] [G acc: 0.250]\n",
      "3132 [D loss: (0.435)(R 0.388, F 0.483)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.073] [G acc: 0.188]\n",
      "3133 [D loss: (0.202)(R 0.227, F 0.178)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.565] [G acc: 0.375]\n",
      "3134 [D loss: (0.531)(R 0.519, F 0.544)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.930] [G acc: 0.375]\n",
      "3135 [D loss: (0.314)(R 0.180, F 0.448)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.641] [G acc: 0.188]\n",
      "3136 [D loss: (0.334)(R 0.324, F 0.344)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.109] [G acc: 0.312]\n",
      "3137 [D loss: (0.340)(R 0.204, F 0.476)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.315] [G acc: 0.250]\n",
      "3138 [D loss: (0.372)(R 0.256, F 0.488)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.514] [G acc: 0.250]\n",
      "3139 [D loss: (0.268)(R 0.262, F 0.273)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.303] [G acc: 0.250]\n",
      "3140 [D loss: (0.296)(R 0.247, F 0.344)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.746] [G acc: 0.250]\n",
      "3141 [D loss: (0.417)(R 0.453, F 0.381)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.978] [G acc: 0.125]\n",
      "3142 [D loss: (0.368)(R 0.593, F 0.142)] [D acc: (0.875)(0.812, 0.938)] [G loss: 3.339] [G acc: 0.312]\n",
      "3143 [D loss: (0.713)(R 0.846, F 0.580)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.774] [G acc: 0.250]\n",
      "3144 [D loss: (0.316)(R 0.354, F 0.277)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.109] [G acc: 0.062]\n",
      "3145 [D loss: (0.274)(R 0.215, F 0.333)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.435] [G acc: 0.250]\n",
      "3146 [D loss: (0.347)(R 0.183, F 0.512)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.149] [G acc: 0.312]\n",
      "3147 [D loss: (0.376)(R 0.355, F 0.397)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.869] [G acc: 0.312]\n",
      "3148 [D loss: (0.272)(R 0.185, F 0.359)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.748] [G acc: 0.312]\n",
      "3149 [D loss: (0.367)(R 0.234, F 0.501)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.839] [G acc: 0.250]\n",
      "3150 [D loss: (0.416)(R 0.360, F 0.472)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.182] [G acc: 0.250]\n",
      "3151 [D loss: (0.614)(R 0.590, F 0.637)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.802] [G acc: 0.312]\n",
      "3152 [D loss: (0.383)(R 0.169, F 0.596)] [D acc: (0.781)(1.000, 0.562)] [G loss: 3.575] [G acc: 0.188]\n",
      "3153 [D loss: (0.611)(R 1.085, F 0.136)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.336] [G acc: 0.188]\n",
      "3154 [D loss: (0.395)(R 0.525, F 0.266)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.436] [G acc: 0.188]\n",
      "3155 [D loss: (0.212)(R 0.203, F 0.222)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.197] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3156 [D loss: (0.222)(R 0.224, F 0.220)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.742] [G acc: 0.312]\n",
      "3157 [D loss: (0.382)(R 0.313, F 0.451)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.413] [G acc: 0.062]\n",
      "3158 [D loss: (0.324)(R 0.406, F 0.242)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.045] [G acc: 0.125]\n",
      "3159 [D loss: (0.309)(R 0.171, F 0.447)] [D acc: (0.844)(1.000, 0.688)] [G loss: 6.203] [G acc: 0.062]\n",
      "3160 [D loss: (0.132)(R 0.197, F 0.067)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.337] [G acc: 0.250]\n",
      "3161 [D loss: (0.404)(R 0.416, F 0.391)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.561] [G acc: 0.375]\n",
      "3162 [D loss: (0.525)(R 0.685, F 0.365)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.850] [G acc: 0.250]\n",
      "3163 [D loss: (0.872)(R 1.278, F 0.466)] [D acc: (0.750)(0.750, 0.750)] [G loss: 4.115] [G acc: 0.312]\n",
      "3164 [D loss: (0.386)(R 0.301, F 0.471)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.243] [G acc: 0.250]\n",
      "3165 [D loss: (0.274)(R 0.174, F 0.374)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.902] [G acc: 0.312]\n",
      "3166 [D loss: (0.272)(R 0.183, F 0.361)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.749] [G acc: 0.125]\n",
      "3167 [D loss: (0.314)(R 0.188, F 0.441)] [D acc: (0.875)(1.000, 0.750)] [G loss: 6.030] [G acc: 0.250]\n",
      "3168 [D loss: (0.253)(R 0.223, F 0.283)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.636] [G acc: 0.062]\n",
      "3169 [D loss: (0.390)(R 0.264, F 0.517)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.091] [G acc: 0.250]\n",
      "3170 [D loss: (0.496)(R 0.329, F 0.662)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.485] [G acc: 0.250]\n",
      "3171 [D loss: (0.282)(R 0.428, F 0.135)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.676] [G acc: 0.188]\n",
      "3172 [D loss: (0.399)(R 0.179, F 0.619)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.167] [G acc: 0.188]\n",
      "3173 [D loss: (0.569)(R 0.606, F 0.531)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.300] [G acc: 0.125]\n",
      "3174 [D loss: (0.323)(R 0.203, F 0.444)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.967] [G acc: 0.250]\n",
      "3175 [D loss: (0.236)(R 0.202, F 0.269)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.843] [G acc: 0.312]\n",
      "3176 [D loss: (0.376)(R 0.216, F 0.536)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.955] [G acc: 0.250]\n",
      "3177 [D loss: (0.194)(R 0.265, F 0.124)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.303] [G acc: 0.250]\n",
      "3178 [D loss: (0.374)(R 0.491, F 0.257)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.649] [G acc: 0.250]\n",
      "3179 [D loss: (0.404)(R 0.206, F 0.602)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.598] [G acc: 0.250]\n",
      "3180 [D loss: (0.546)(R 0.358, F 0.734)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.803] [G acc: 0.250]\n",
      "3181 [D loss: (0.378)(R 0.351, F 0.405)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.491] [G acc: 0.125]\n",
      "3182 [D loss: (0.430)(R 0.253, F 0.607)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.636] [G acc: 0.188]\n",
      "3183 [D loss: (0.331)(R 0.292, F 0.369)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.124] [G acc: 0.312]\n",
      "3184 [D loss: (0.316)(R 0.236, F 0.396)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.586] [G acc: 0.250]\n",
      "3185 [D loss: (0.418)(R 0.504, F 0.331)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.793] [G acc: 0.312]\n",
      "3186 [D loss: (0.395)(R 0.161, F 0.628)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.340] [G acc: 0.188]\n",
      "3187 [D loss: (0.523)(R 0.656, F 0.389)] [D acc: (0.781)(0.750, 0.812)] [G loss: 3.525] [G acc: 0.250]\n",
      "3188 [D loss: (0.428)(R 0.562, F 0.294)] [D acc: (0.781)(0.750, 0.812)] [G loss: 4.259] [G acc: 0.250]\n",
      "3189 [D loss: (0.244)(R 0.312, F 0.177)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.618] [G acc: 0.125]\n",
      "3190 [D loss: (0.429)(R 0.739, F 0.119)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.500] [G acc: 0.312]\n",
      "3191 [D loss: (0.450)(R 0.210, F 0.689)] [D acc: (0.781)(1.000, 0.562)] [G loss: 3.776] [G acc: 0.188]\n",
      "3192 [D loss: (0.320)(R 0.277, F 0.363)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.154] [G acc: 0.062]\n",
      "3193 [D loss: (0.287)(R 0.313, F 0.260)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.437] [G acc: 0.125]\n",
      "3194 [D loss: (0.122)(R 0.191, F 0.052)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.239] [G acc: 0.250]\n",
      "3195 [D loss: (0.360)(R 0.294, F 0.426)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.408] [G acc: 0.312]\n",
      "3196 [D loss: (0.317)(R 0.551, F 0.082)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.865] [G acc: 0.250]\n",
      "3197 [D loss: (0.646)(R 0.850, F 0.442)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.637] [G acc: 0.250]\n",
      "3198 [D loss: (0.166)(R 0.199, F 0.133)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.024] [G acc: 0.250]\n",
      "3199 [D loss: (0.443)(R 0.343, F 0.542)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.426] [G acc: 0.188]\n",
      "3200 [D loss: (0.270)(R 0.197, F 0.343)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.982] [G acc: 0.250]\n",
      "3201 [D loss: (0.427)(R 0.267, F 0.586)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.275] [G acc: 0.250]\n",
      "3202 [D loss: (0.336)(R 0.270, F 0.403)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.648] [G acc: 0.188]\n",
      "3203 [D loss: (0.476)(R 0.278, F 0.674)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.112] [G acc: 0.250]\n",
      "3204 [D loss: (0.294)(R 0.295, F 0.294)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.091] [G acc: 0.125]\n",
      "3205 [D loss: (0.612)(R 0.459, F 0.766)] [D acc: (0.750)(0.938, 0.562)] [G loss: 5.030] [G acc: 0.062]\n",
      "3206 [D loss: (0.318)(R 0.322, F 0.313)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.501] [G acc: 0.188]\n",
      "3207 [D loss: (0.257)(R 0.190, F 0.325)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.613] [G acc: 0.188]\n",
      "3208 [D loss: (0.366)(R 0.208, F 0.525)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.561] [G acc: 0.125]\n",
      "3209 [D loss: (0.749)(R 0.867, F 0.632)] [D acc: (0.656)(0.688, 0.625)] [G loss: 3.075] [G acc: 0.250]\n",
      "3210 [D loss: (0.389)(R 0.461, F 0.317)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.181] [G acc: 0.375]\n",
      "3211 [D loss: (0.172)(R 0.192, F 0.151)] [D acc: (0.969)(1.000, 0.938)] [G loss: 2.970] [G acc: 0.375]\n",
      "3212 [D loss: (0.450)(R 0.487, F 0.413)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.789] [G acc: 0.312]\n",
      "3213 [D loss: (0.415)(R 0.321, F 0.510)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.588] [G acc: 0.250]\n",
      "3214 [D loss: (0.319)(R 0.340, F 0.298)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.065] [G acc: 0.312]\n",
      "3215 [D loss: (0.327)(R 0.300, F 0.353)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.044] [G acc: 0.312]\n",
      "3216 [D loss: (0.458)(R 0.207, F 0.709)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.423] [G acc: 0.062]\n",
      "3217 [D loss: (0.408)(R 0.387, F 0.429)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.850] [G acc: 0.250]\n",
      "3218 [D loss: (0.341)(R 0.198, F 0.484)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.805] [G acc: 0.250]\n",
      "3219 [D loss: (0.241)(R 0.202, F 0.280)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.754] [G acc: 0.188]\n",
      "3220 [D loss: (0.517)(R 0.589, F 0.446)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.209] [G acc: 0.188]\n",
      "3221 [D loss: (0.328)(R 0.279, F 0.378)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.077] [G acc: 0.250]\n",
      "3222 [D loss: (0.536)(R 0.321, F 0.751)] [D acc: (0.719)(0.938, 0.500)] [G loss: 4.060] [G acc: 0.125]\n",
      "3223 [D loss: (0.276)(R 0.404, F 0.149)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.724] [G acc: 0.188]\n",
      "3224 [D loss: (0.425)(R 0.300, F 0.551)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.516] [G acc: 0.125]\n",
      "3225 [D loss: (0.249)(R 0.254, F 0.245)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.380] [G acc: 0.062]\n",
      "3226 [D loss: (0.360)(R 0.231, F 0.489)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.736] [G acc: 0.250]\n",
      "3227 [D loss: (0.259)(R 0.284, F 0.235)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.731] [G acc: 0.188]\n",
      "3228 [D loss: (0.370)(R 0.243, F 0.498)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.840] [G acc: 0.188]\n",
      "3229 [D loss: (0.280)(R 0.288, F 0.272)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.148] [G acc: 0.188]\n",
      "3230 [D loss: (0.477)(R 0.678, F 0.275)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.295] [G acc: 0.188]\n",
      "3231 [D loss: (0.403)(R 0.247, F 0.560)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.302] [G acc: 0.250]\n",
      "3232 [D loss: (0.398)(R 0.210, F 0.585)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.472] [G acc: 0.312]\n",
      "3233 [D loss: (0.407)(R 0.388, F 0.426)] [D acc: (0.781)(0.812, 0.750)] [G loss: 4.183] [G acc: 0.188]\n",
      "3234 [D loss: (1.352)(R 1.874, F 0.830)] [D acc: (0.625)(0.812, 0.438)] [G loss: 3.570] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3235 [D loss: (0.550)(R 0.523, F 0.577)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.981] [G acc: 0.250]\n",
      "3236 [D loss: (0.421)(R 0.224, F 0.618)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.891] [G acc: 0.188]\n",
      "3237 [D loss: (0.370)(R 0.174, F 0.565)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.873] [G acc: 0.375]\n",
      "3238 [D loss: (0.374)(R 0.223, F 0.525)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.247] [G acc: 0.188]\n",
      "3239 [D loss: (0.171)(R 0.233, F 0.110)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.949] [G acc: 0.312]\n",
      "3240 [D loss: (0.591)(R 0.511, F 0.672)] [D acc: (0.719)(0.812, 0.625)] [G loss: 3.323] [G acc: 0.062]\n",
      "3241 [D loss: (0.156)(R 0.187, F 0.124)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.247] [G acc: 0.125]\n",
      "3242 [D loss: (0.308)(R 0.289, F 0.327)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.836] [G acc: 0.125]\n",
      "3243 [D loss: (0.586)(R 0.596, F 0.575)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.862] [G acc: 0.250]\n",
      "3244 [D loss: (0.367)(R 0.444, F 0.289)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.000] [G acc: 0.250]\n",
      "3245 [D loss: (0.694)(R 0.718, F 0.670)] [D acc: (0.688)(0.750, 0.625)] [G loss: 3.690] [G acc: 0.188]\n",
      "3246 [D loss: (0.456)(R 0.360, F 0.552)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.758] [G acc: 0.250]\n",
      "3247 [D loss: (0.465)(R 0.270, F 0.661)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.176] [G acc: 0.188]\n",
      "3248 [D loss: (0.314)(R 0.273, F 0.356)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.571] [G acc: 0.188]\n",
      "3249 [D loss: (0.270)(R 0.263, F 0.276)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.592] [G acc: 0.125]\n",
      "3250 [D loss: (0.287)(R 0.250, F 0.324)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.039] [G acc: 0.188]\n",
      "3251 [D loss: (0.343)(R 0.259, F 0.428)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.087] [G acc: 0.188]\n",
      "3252 [D loss: (0.346)(R 0.294, F 0.398)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.135] [G acc: 0.250]\n",
      "3253 [D loss: (0.455)(R 0.319, F 0.591)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.277] [G acc: 0.250]\n",
      "3254 [D loss: (0.389)(R 0.359, F 0.418)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.438] [G acc: 0.125]\n",
      "3255 [D loss: (0.241)(R 0.231, F 0.250)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.649] [G acc: 0.312]\n",
      "3256 [D loss: (0.388)(R 0.212, F 0.564)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.016] [G acc: 0.250]\n",
      "3257 [D loss: (0.488)(R 0.345, F 0.632)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.611] [G acc: 0.250]\n",
      "3258 [D loss: (0.460)(R 0.325, F 0.596)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.460] [G acc: 0.312]\n",
      "3259 [D loss: (0.286)(R 0.242, F 0.329)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.483] [G acc: 0.125]\n",
      "3260 [D loss: (0.324)(R 0.354, F 0.294)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.662] [G acc: 0.188]\n",
      "3261 [D loss: (0.389)(R 0.440, F 0.338)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.078] [G acc: 0.250]\n",
      "3262 [D loss: (0.307)(R 0.184, F 0.430)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.550] [G acc: 0.188]\n",
      "3263 [D loss: (0.389)(R 0.319, F 0.458)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.638] [G acc: 0.188]\n",
      "3264 [D loss: (0.380)(R 0.314, F 0.446)] [D acc: (0.781)(0.875, 0.688)] [G loss: 6.029] [G acc: 0.062]\n",
      "3265 [D loss: (0.524)(R 0.832, F 0.215)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.085] [G acc: 0.188]\n",
      "3266 [D loss: (0.370)(R 0.191, F 0.548)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.055] [G acc: 0.188]\n",
      "3267 [D loss: (0.473)(R 0.252, F 0.694)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.647] [G acc: 0.375]\n",
      "3268 [D loss: (0.452)(R 0.371, F 0.533)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.632] [G acc: 0.125]\n",
      "3269 [D loss: (0.305)(R 0.218, F 0.392)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.498] [G acc: 0.188]\n",
      "3270 [D loss: (0.524)(R 0.564, F 0.483)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.394] [G acc: 0.312]\n",
      "3271 [D loss: (0.337)(R 0.244, F 0.431)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.383] [G acc: 0.250]\n",
      "3272 [D loss: (0.869)(R 1.570, F 0.169)] [D acc: (0.844)(0.750, 0.938)] [G loss: 4.819] [G acc: 0.188]\n",
      "3273 [D loss: (0.204)(R 0.199, F 0.209)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.386] [G acc: 0.250]\n",
      "3274 [D loss: (0.241)(R 0.368, F 0.115)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.344] [G acc: 0.250]\n",
      "3275 [D loss: (0.385)(R 0.523, F 0.246)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.068] [G acc: 0.312]\n",
      "3276 [D loss: (0.334)(R 0.376, F 0.292)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.081] [G acc: 0.250]\n",
      "3277 [D loss: (0.345)(R 0.185, F 0.505)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.610] [G acc: 0.438]\n",
      "3278 [D loss: (0.185)(R 0.156, F 0.215)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.694] [G acc: 0.250]\n",
      "3279 [D loss: (0.368)(R 0.311, F 0.425)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.672] [G acc: 0.312]\n",
      "3280 [D loss: (0.619)(R 0.586, F 0.653)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.930] [G acc: 0.250]\n",
      "3281 [D loss: (0.456)(R 0.309, F 0.603)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.596] [G acc: 0.312]\n",
      "3282 [D loss: (0.402)(R 0.187, F 0.616)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.479] [G acc: 0.312]\n",
      "3283 [D loss: (0.388)(R 0.598, F 0.179)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.461] [G acc: 0.188]\n",
      "3284 [D loss: (0.258)(R 0.286, F 0.230)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.408] [G acc: 0.188]\n",
      "3285 [D loss: (0.305)(R 0.167, F 0.442)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.303] [G acc: 0.312]\n",
      "3286 [D loss: (0.442)(R 0.421, F 0.463)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.944] [G acc: 0.312]\n",
      "3287 [D loss: (0.502)(R 0.497, F 0.507)] [D acc: (0.750)(0.812, 0.688)] [G loss: 4.543] [G acc: 0.125]\n",
      "3288 [D loss: (0.453)(R 0.535, F 0.370)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.558] [G acc: 0.188]\n",
      "3289 [D loss: (0.230)(R 0.193, F 0.268)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.846] [G acc: 0.062]\n",
      "3290 [D loss: (0.406)(R 0.530, F 0.282)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.579] [G acc: 0.125]\n",
      "3291 [D loss: (0.447)(R 0.372, F 0.522)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.977] [G acc: 0.250]\n",
      "3292 [D loss: (0.318)(R 0.180, F 0.456)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.352] [G acc: 0.312]\n",
      "3293 [D loss: (0.430)(R 0.344, F 0.516)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.343] [G acc: 0.125]\n",
      "3294 [D loss: (0.401)(R 0.498, F 0.304)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.371] [G acc: 0.250]\n",
      "3295 [D loss: (0.332)(R 0.243, F 0.421)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.914] [G acc: 0.312]\n",
      "3296 [D loss: (0.332)(R 0.231, F 0.433)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.749] [G acc: 0.312]\n",
      "3297 [D loss: (0.350)(R 0.222, F 0.479)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.754] [G acc: 0.125]\n",
      "3298 [D loss: (0.461)(R 0.276, F 0.646)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.217] [G acc: 0.312]\n",
      "3299 [D loss: (0.362)(R 0.291, F 0.433)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.762] [G acc: 0.312]\n",
      "3300 [D loss: (0.397)(R 0.247, F 0.546)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.487] [G acc: 0.375]\n",
      "3301 [D loss: (0.252)(R 0.201, F 0.303)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.414] [G acc: 0.250]\n",
      "3302 [D loss: (0.390)(R 0.328, F 0.451)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.774] [G acc: 0.250]\n",
      "3303 [D loss: (0.395)(R 0.623, F 0.166)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.215] [G acc: 0.312]\n",
      "3304 [D loss: (0.363)(R 0.264, F 0.463)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.571] [G acc: 0.375]\n",
      "3305 [D loss: (0.164)(R 0.229, F 0.099)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.413] [G acc: 0.188]\n",
      "3306 [D loss: (0.311)(R 0.181, F 0.441)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.354] [G acc: 0.312]\n",
      "3307 [D loss: (0.614)(R 0.505, F 0.722)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.446] [G acc: 0.312]\n",
      "3308 [D loss: (0.352)(R 0.199, F 0.504)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.809] [G acc: 0.375]\n",
      "3309 [D loss: (0.316)(R 0.222, F 0.410)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.096] [G acc: 0.250]\n",
      "3310 [D loss: (0.214)(R 0.191, F 0.237)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.208] [G acc: 0.188]\n",
      "3311 [D loss: (0.421)(R 0.373, F 0.468)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.369] [G acc: 0.375]\n",
      "3312 [D loss: (0.347)(R 0.266, F 0.429)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.341] [G acc: 0.312]\n",
      "3313 [D loss: (0.410)(R 0.224, F 0.596)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.813] [G acc: 0.188]\n",
      "3314 [D loss: (0.270)(R 0.225, F 0.315)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.571] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3315 [D loss: (0.402)(R 0.214, F 0.589)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.554] [G acc: 0.250]\n",
      "3316 [D loss: (0.358)(R 0.192, F 0.524)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.660] [G acc: 0.188]\n",
      "3317 [D loss: (0.250)(R 0.212, F 0.288)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.921] [G acc: 0.250]\n",
      "3318 [D loss: (0.172)(R 0.253, F 0.091)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.482] [G acc: 0.250]\n",
      "3319 [D loss: (0.297)(R 0.351, F 0.243)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.765] [G acc: 0.312]\n",
      "3320 [D loss: (0.302)(R 0.184, F 0.419)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.900] [G acc: 0.312]\n",
      "3321 [D loss: (0.355)(R 0.519, F 0.190)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.355] [G acc: 0.250]\n",
      "3322 [D loss: (0.291)(R 0.298, F 0.284)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.152] [G acc: 0.188]\n",
      "3323 [D loss: (0.334)(R 0.511, F 0.156)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.821] [G acc: 0.062]\n",
      "3324 [D loss: (0.346)(R 0.216, F 0.476)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.552] [G acc: 0.312]\n",
      "3325 [D loss: (0.389)(R 0.566, F 0.212)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.349] [G acc: 0.250]\n",
      "3326 [D loss: (0.397)(R 0.382, F 0.412)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.779] [G acc: 0.312]\n",
      "3327 [D loss: (0.481)(R 0.203, F 0.760)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.098] [G acc: 0.250]\n",
      "3328 [D loss: (0.389)(R 0.363, F 0.416)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.076] [G acc: 0.125]\n",
      "3329 [D loss: (0.347)(R 0.156, F 0.537)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.626] [G acc: 0.375]\n",
      "3330 [D loss: (0.389)(R 0.262, F 0.515)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.333] [G acc: 0.062]\n",
      "3331 [D loss: (0.213)(R 0.253, F 0.172)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.043] [G acc: 0.188]\n",
      "3332 [D loss: (0.259)(R 0.232, F 0.285)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.987] [G acc: 0.125]\n",
      "3333 [D loss: (0.239)(R 0.190, F 0.288)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.174] [G acc: 0.188]\n",
      "3334 [D loss: (0.232)(R 0.267, F 0.197)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.587] [G acc: 0.188]\n",
      "3335 [D loss: (0.574)(R 0.606, F 0.543)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.225] [G acc: 0.250]\n",
      "3336 [D loss: (0.212)(R 0.212, F 0.211)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.566] [G acc: 0.375]\n",
      "3337 [D loss: (0.266)(R 0.256, F 0.276)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.129] [G acc: 0.188]\n",
      "3338 [D loss: (0.366)(R 0.240, F 0.492)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.204] [G acc: 0.250]\n",
      "3339 [D loss: (0.631)(R 0.283, F 0.980)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.664] [G acc: 0.250]\n",
      "3340 [D loss: (0.224)(R 0.166, F 0.281)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.800] [G acc: 0.125]\n",
      "3341 [D loss: (0.414)(R 0.500, F 0.329)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.494] [G acc: 0.125]\n",
      "3342 [D loss: (0.274)(R 0.186, F 0.362)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.850] [G acc: 0.188]\n",
      "3343 [D loss: (0.367)(R 0.247, F 0.488)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.686] [G acc: 0.312]\n",
      "3344 [D loss: (0.326)(R 0.230, F 0.421)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.102] [G acc: 0.375]\n",
      "3345 [D loss: (0.608)(R 0.628, F 0.588)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.584] [G acc: 0.250]\n",
      "3346 [D loss: (0.408)(R 0.199, F 0.618)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.540] [G acc: 0.312]\n",
      "3347 [D loss: (0.415)(R 0.192, F 0.638)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.444] [G acc: 0.312]\n",
      "3348 [D loss: (0.390)(R 0.192, F 0.588)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.231] [G acc: 0.312]\n",
      "3349 [D loss: (0.399)(R 0.285, F 0.514)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.538] [G acc: 0.312]\n",
      "3350 [D loss: (0.222)(R 0.259, F 0.185)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.716] [G acc: 0.188]\n",
      "3351 [D loss: (0.114)(R 0.192, F 0.035)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.601] [G acc: 0.188]\n",
      "3352 [D loss: (0.299)(R 0.266, F 0.333)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.640] [G acc: 0.125]\n",
      "3353 [D loss: (0.422)(R 0.423, F 0.421)] [D acc: (0.812)(0.875, 0.750)] [G loss: 5.441] [G acc: 0.062]\n",
      "3354 [D loss: (0.258)(R 0.321, F 0.194)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.822] [G acc: 0.062]\n",
      "3355 [D loss: (0.185)(R 0.187, F 0.183)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.030] [G acc: 0.188]\n",
      "3356 [D loss: (0.257)(R 0.193, F 0.321)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.608] [G acc: 0.375]\n",
      "3357 [D loss: (0.366)(R 0.175, F 0.557)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.836] [G acc: 0.188]\n",
      "3358 [D loss: (0.313)(R 0.240, F 0.385)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.663] [G acc: 0.312]\n",
      "3359 [D loss: (0.257)(R 0.196, F 0.317)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.851] [G acc: 0.312]\n",
      "3360 [D loss: (0.381)(R 0.207, F 0.556)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.757] [G acc: 0.250]\n",
      "3361 [D loss: (0.390)(R 0.330, F 0.450)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.680] [G acc: 0.250]\n",
      "3362 [D loss: (0.247)(R 0.210, F 0.283)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.245] [G acc: 0.250]\n",
      "3363 [D loss: (0.664)(R 0.810, F 0.517)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.439] [G acc: 0.250]\n",
      "3364 [D loss: (0.670)(R 0.737, F 0.603)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.376] [G acc: 0.312]\n",
      "3365 [D loss: (0.212)(R 0.251, F 0.174)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.126] [G acc: 0.375]\n",
      "3366 [D loss: (0.277)(R 0.153, F 0.401)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.648] [G acc: 0.188]\n",
      "3367 [D loss: (0.213)(R 0.190, F 0.237)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.961] [G acc: 0.188]\n",
      "3368 [D loss: (0.251)(R 0.198, F 0.304)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.339] [G acc: 0.188]\n",
      "3369 [D loss: (0.296)(R 0.213, F 0.380)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.683] [G acc: 0.250]\n",
      "3370 [D loss: (0.260)(R 0.285, F 0.235)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.804] [G acc: 0.312]\n",
      "3371 [D loss: (0.141)(R 0.197, F 0.084)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.508] [G acc: 0.312]\n",
      "3372 [D loss: (0.417)(R 0.262, F 0.572)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.572] [G acc: 0.250]\n",
      "3373 [D loss: (0.405)(R 0.394, F 0.415)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.777] [G acc: 0.375]\n",
      "3374 [D loss: (0.484)(R 0.246, F 0.722)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.332] [G acc: 0.250]\n",
      "3375 [D loss: (0.362)(R 0.223, F 0.501)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.916] [G acc: 0.188]\n",
      "3376 [D loss: (0.443)(R 0.356, F 0.530)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.800] [G acc: 0.250]\n",
      "3377 [D loss: (0.288)(R 0.313, F 0.262)] [D acc: (0.906)(0.938, 0.875)] [G loss: 5.420] [G acc: 0.125]\n",
      "3378 [D loss: (0.449)(R 0.569, F 0.328)] [D acc: (0.844)(0.812, 0.875)] [G loss: 5.012] [G acc: 0.188]\n",
      "3379 [D loss: (0.213)(R 0.304, F 0.122)] [D acc: (0.906)(0.875, 0.938)] [G loss: 4.203] [G acc: 0.188]\n",
      "3380 [D loss: (0.315)(R 0.200, F 0.430)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.072] [G acc: 0.188]\n",
      "3381 [D loss: (0.283)(R 0.267, F 0.298)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.939] [G acc: 0.250]\n",
      "3382 [D loss: (0.294)(R 0.192, F 0.397)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.405] [G acc: 0.188]\n",
      "3383 [D loss: (0.579)(R 0.660, F 0.498)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.901] [G acc: 0.250]\n",
      "3384 [D loss: (0.390)(R 0.281, F 0.499)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.668] [G acc: 0.125]\n",
      "3385 [D loss: (0.718)(R 0.892, F 0.543)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.763] [G acc: 0.188]\n",
      "3386 [D loss: (0.306)(R 0.218, F 0.394)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.398] [G acc: 0.312]\n",
      "3387 [D loss: (0.442)(R 0.599, F 0.285)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.991] [G acc: 0.312]\n",
      "3388 [D loss: (0.334)(R 0.207, F 0.460)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.457] [G acc: 0.312]\n",
      "3389 [D loss: (0.270)(R 0.204, F 0.335)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.908] [G acc: 0.312]\n",
      "3390 [D loss: (0.295)(R 0.456, F 0.133)] [D acc: (0.875)(0.812, 0.938)] [G loss: 4.335] [G acc: 0.312]\n",
      "3391 [D loss: (0.407)(R 0.309, F 0.505)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.315] [G acc: 0.375]\n",
      "3392 [D loss: (0.368)(R 0.239, F 0.496)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.254] [G acc: 0.375]\n",
      "3393 [D loss: (0.150)(R 0.236, F 0.063)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.926] [G acc: 0.375]\n",
      "3394 [D loss: (0.282)(R 0.258, F 0.307)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.885] [G acc: 0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3395 [D loss: (1.060)(R 1.486, F 0.634)] [D acc: (0.719)(0.750, 0.688)] [G loss: 4.092] [G acc: 0.188]\n",
      "3396 [D loss: (0.468)(R 0.229, F 0.707)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.063] [G acc: 0.312]\n",
      "3397 [D loss: (0.349)(R 0.458, F 0.240)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.017] [G acc: 0.375]\n",
      "3398 [D loss: (0.389)(R 0.194, F 0.584)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.982] [G acc: 0.250]\n",
      "3399 [D loss: (0.259)(R 0.239, F 0.279)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.927] [G acc: 0.250]\n",
      "3400 [D loss: (0.267)(R 0.144, F 0.390)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.155] [G acc: 0.438]\n",
      "3401 [D loss: (0.348)(R 0.164, F 0.532)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.205] [G acc: 0.250]\n",
      "3402 [D loss: (0.352)(R 0.278, F 0.427)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.368] [G acc: 0.312]\n",
      "3403 [D loss: (0.335)(R 0.254, F 0.417)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.255] [G acc: 0.312]\n",
      "3404 [D loss: (0.569)(R 0.205, F 0.932)] [D acc: (0.750)(1.000, 0.500)] [G loss: 3.825] [G acc: 0.312]\n",
      "3405 [D loss: (0.421)(R 0.277, F 0.565)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.983] [G acc: 0.312]\n",
      "3406 [D loss: (0.572)(R 0.526, F 0.618)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.631] [G acc: 0.125]\n",
      "3407 [D loss: (0.693)(R 1.035, F 0.350)] [D acc: (0.750)(0.750, 0.750)] [G loss: 4.002] [G acc: 0.125]\n",
      "3408 [D loss: (0.272)(R 0.185, F 0.360)] [D acc: (0.906)(1.000, 0.812)] [G loss: 5.122] [G acc: 0.062]\n",
      "3409 [D loss: (0.383)(R 0.258, F 0.507)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.160] [G acc: 0.125]\n",
      "3410 [D loss: (0.381)(R 0.265, F 0.497)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.380] [G acc: 0.062]\n",
      "3411 [D loss: (0.408)(R 0.254, F 0.561)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.976] [G acc: 0.125]\n",
      "3412 [D loss: (0.598)(R 0.559, F 0.637)] [D acc: (0.719)(0.875, 0.562)] [G loss: 3.214] [G acc: 0.250]\n",
      "3413 [D loss: (0.479)(R 0.354, F 0.603)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.931] [G acc: 0.250]\n",
      "3414 [D loss: (0.277)(R 0.252, F 0.301)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.421] [G acc: 0.062]\n",
      "3415 [D loss: (0.283)(R 0.198, F 0.368)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.239] [G acc: 0.062]\n",
      "3416 [D loss: (0.356)(R 0.268, F 0.445)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.364] [G acc: 0.312]\n",
      "3417 [D loss: (0.490)(R 0.335, F 0.646)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.806] [G acc: 0.250]\n",
      "3418 [D loss: (0.243)(R 0.270, F 0.215)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.738] [G acc: 0.375]\n",
      "3419 [D loss: (0.426)(R 0.628, F 0.224)] [D acc: (0.906)(0.938, 0.875)] [G loss: 6.123] [G acc: 0.062]\n",
      "3420 [D loss: (0.269)(R 0.263, F 0.276)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.909] [G acc: 0.375]\n",
      "3421 [D loss: (0.628)(R 0.890, F 0.366)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.278] [G acc: 0.188]\n",
      "3422 [D loss: (0.688)(R 0.638, F 0.738)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.839] [G acc: 0.375]\n",
      "3423 [D loss: (0.313)(R 0.262, F 0.364)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.213] [G acc: 0.188]\n",
      "3424 [D loss: (0.490)(R 0.223, F 0.757)] [D acc: (0.750)(1.000, 0.500)] [G loss: 5.636] [G acc: 0.312]\n",
      "3425 [D loss: (0.397)(R 0.284, F 0.510)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.530] [G acc: 0.188]\n",
      "3426 [D loss: (0.418)(R 0.692, F 0.145)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.985] [G acc: 0.250]\n",
      "3427 [D loss: (0.603)(R 0.539, F 0.668)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.313] [G acc: 0.250]\n",
      "3428 [D loss: (0.292)(R 0.234, F 0.350)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.367] [G acc: 0.125]\n",
      "3429 [D loss: (0.461)(R 0.695, F 0.227)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.456] [G acc: 0.188]\n",
      "3430 [D loss: (0.285)(R 0.241, F 0.329)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.497] [G acc: 0.125]\n",
      "3431 [D loss: (0.230)(R 0.198, F 0.262)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.463] [G acc: 0.312]\n",
      "3432 [D loss: (0.195)(R 0.227, F 0.163)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.148] [G acc: 0.250]\n",
      "3433 [D loss: (0.309)(R 0.340, F 0.279)] [D acc: (0.844)(0.812, 0.875)] [G loss: 4.651] [G acc: 0.250]\n",
      "3434 [D loss: (0.288)(R 0.337, F 0.240)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.361] [G acc: 0.438]\n",
      "3435 [D loss: (0.296)(R 0.391, F 0.201)] [D acc: (0.844)(0.812, 0.875)] [G loss: 5.055] [G acc: 0.188]\n",
      "3436 [D loss: (0.284)(R 0.374, F 0.194)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.675] [G acc: 0.375]\n",
      "3437 [D loss: (0.407)(R 0.390, F 0.424)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.337] [G acc: 0.375]\n",
      "3438 [D loss: (0.493)(R 0.463, F 0.524)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.210] [G acc: 0.312]\n",
      "3439 [D loss: (0.266)(R 0.314, F 0.218)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.777] [G acc: 0.000]\n",
      "3440 [D loss: (0.343)(R 0.397, F 0.290)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.149] [G acc: 0.125]\n",
      "3441 [D loss: (0.229)(R 0.353, F 0.105)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.717] [G acc: 0.312]\n",
      "3442 [D loss: (0.331)(R 0.221, F 0.442)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.512] [G acc: 0.250]\n",
      "3443 [D loss: (0.357)(R 0.186, F 0.528)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.276] [G acc: 0.188]\n",
      "3444 [D loss: (0.326)(R 0.268, F 0.384)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.842] [G acc: 0.250]\n",
      "3445 [D loss: (0.321)(R 0.242, F 0.400)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.246] [G acc: 0.312]\n",
      "3446 [D loss: (0.160)(R 0.155, F 0.165)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.724] [G acc: 0.250]\n",
      "3447 [D loss: (0.451)(R 0.275, F 0.627)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.708] [G acc: 0.375]\n",
      "3448 [D loss: (0.282)(R 0.204, F 0.360)] [D acc: (0.875)(1.000, 0.750)] [G loss: 5.332] [G acc: 0.188]\n",
      "3449 [D loss: (0.346)(R 0.364, F 0.329)] [D acc: (0.875)(0.875, 0.875)] [G loss: 5.332] [G acc: 0.125]\n",
      "3450 [D loss: (0.273)(R 0.292, F 0.253)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.256] [G acc: 0.312]\n",
      "3451 [D loss: (0.483)(R 0.631, F 0.335)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.339] [G acc: 0.188]\n",
      "3452 [D loss: (0.655)(R 0.900, F 0.411)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.625] [G acc: 0.250]\n",
      "3453 [D loss: (0.185)(R 0.204, F 0.166)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.522] [G acc: 0.250]\n",
      "3454 [D loss: (0.093)(R 0.179, F 0.007)] [D acc: (1.000)(1.000, 1.000)] [G loss: 6.104] [G acc: 0.125]\n",
      "3455 [D loss: (0.374)(R 0.237, F 0.511)] [D acc: (0.781)(0.938, 0.625)] [G loss: 5.630] [G acc: 0.125]\n",
      "3456 [D loss: (0.281)(R 0.475, F 0.088)] [D acc: (0.938)(0.875, 1.000)] [G loss: 4.418] [G acc: 0.188]\n",
      "3457 [D loss: (0.455)(R 0.337, F 0.573)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.833] [G acc: 0.000]\n",
      "3458 [D loss: (0.377)(R 0.375, F 0.378)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.558] [G acc: 0.312]\n",
      "3459 [D loss: (0.263)(R 0.194, F 0.332)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.389] [G acc: 0.250]\n",
      "3460 [D loss: (0.568)(R 0.595, F 0.542)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.791] [G acc: 0.312]\n",
      "3461 [D loss: (0.384)(R 0.229, F 0.539)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.397] [G acc: 0.438]\n",
      "3462 [D loss: (0.276)(R 0.204, F 0.348)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.979] [G acc: 0.375]\n",
      "3463 [D loss: (0.406)(R 0.179, F 0.633)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.710] [G acc: 0.188]\n",
      "3464 [D loss: (0.434)(R 0.324, F 0.545)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.013] [G acc: 0.375]\n",
      "3465 [D loss: (0.314)(R 0.195, F 0.433)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.310] [G acc: 0.188]\n",
      "3466 [D loss: (0.281)(R 0.219, F 0.344)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.524] [G acc: 0.250]\n",
      "3467 [D loss: (0.356)(R 0.198, F 0.514)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.724] [G acc: 0.312]\n",
      "3468 [D loss: (0.539)(R 0.505, F 0.573)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.984] [G acc: 0.188]\n",
      "3469 [D loss: (0.302)(R 0.264, F 0.341)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.654] [G acc: 0.250]\n",
      "3470 [D loss: (0.430)(R 0.299, F 0.561)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.261] [G acc: 0.188]\n",
      "3471 [D loss: (0.373)(R 0.334, F 0.412)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.173] [G acc: 0.312]\n",
      "3472 [D loss: (0.722)(R 0.441, F 1.003)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.341] [G acc: 0.250]\n",
      "3473 [D loss: (0.465)(R 0.319, F 0.610)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.063] [G acc: 0.250]\n",
      "3474 [D loss: (0.351)(R 0.314, F 0.387)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.188] [G acc: 0.250]\n",
      "3475 [D loss: (0.377)(R 0.240, F 0.515)] [D acc: (0.812)(0.938, 0.688)] [G loss: 5.454] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3476 [D loss: (0.365)(R 0.364, F 0.366)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.722] [G acc: 0.250]\n",
      "3477 [D loss: (0.209)(R 0.207, F 0.210)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.138] [G acc: 0.188]\n",
      "3478 [D loss: (0.408)(R 0.294, F 0.522)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.555] [G acc: 0.312]\n",
      "3479 [D loss: (0.460)(R 0.577, F 0.343)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.249] [G acc: 0.125]\n",
      "3480 [D loss: (0.285)(R 0.350, F 0.220)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.554] [G acc: 0.188]\n",
      "3481 [D loss: (0.117)(R 0.184, F 0.051)] [D acc: (1.000)(1.000, 1.000)] [G loss: 4.846] [G acc: 0.375]\n",
      "3482 [D loss: (0.269)(R 0.282, F 0.256)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.697] [G acc: 0.250]\n",
      "3483 [D loss: (0.198)(R 0.230, F 0.166)] [D acc: (0.938)(1.000, 0.875)] [G loss: 5.167] [G acc: 0.125]\n",
      "3484 [D loss: (0.195)(R 0.309, F 0.080)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.344] [G acc: 0.375]\n",
      "3485 [D loss: (0.523)(R 0.421, F 0.626)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.520] [G acc: 0.312]\n",
      "3486 [D loss: (0.410)(R 0.218, F 0.602)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.117] [G acc: 0.250]\n",
      "3487 [D loss: (0.331)(R 0.334, F 0.328)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.753] [G acc: 0.250]\n",
      "3488 [D loss: (0.427)(R 0.247, F 0.608)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.788] [G acc: 0.312]\n",
      "3489 [D loss: (0.394)(R 0.491, F 0.298)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.537] [G acc: 0.375]\n",
      "3490 [D loss: (0.507)(R 0.640, F 0.374)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.570] [G acc: 0.250]\n",
      "3491 [D loss: (0.288)(R 0.273, F 0.303)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.030] [G acc: 0.312]\n",
      "3492 [D loss: (0.297)(R 0.462, F 0.132)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.063] [G acc: 0.250]\n",
      "3493 [D loss: (0.257)(R 0.186, F 0.328)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.519] [G acc: 0.188]\n",
      "3494 [D loss: (1.832)(R 2.918, F 0.745)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.817] [G acc: 0.312]\n",
      "3495 [D loss: (0.251)(R 0.263, F 0.238)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.577] [G acc: 0.062]\n",
      "3496 [D loss: (0.332)(R 0.282, F 0.382)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.239] [G acc: 0.188]\n",
      "3497 [D loss: (0.358)(R 0.326, F 0.390)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.712] [G acc: 0.375]\n",
      "3498 [D loss: (0.216)(R 0.203, F 0.230)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.317] [G acc: 0.250]\n",
      "3499 [D loss: (0.800)(R 1.113, F 0.487)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.934] [G acc: 0.250]\n",
      "3500 [D loss: (0.591)(R 0.262, F 0.920)] [D acc: (0.719)(1.000, 0.438)] [G loss: 3.672] [G acc: 0.375]\n",
      "3501 [D loss: (0.274)(R 0.356, F 0.192)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.967] [G acc: 0.312]\n",
      "3502 [D loss: (0.396)(R 0.388, F 0.404)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.522] [G acc: 0.375]\n",
      "3503 [D loss: (0.561)(R 0.275, F 0.847)] [D acc: (0.688)(1.000, 0.375)] [G loss: 2.736] [G acc: 0.312]\n",
      "3504 [D loss: (0.402)(R 0.238, F 0.566)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.016] [G acc: 0.250]\n",
      "3505 [D loss: (0.154)(R 0.172, F 0.137)] [D acc: (0.969)(1.000, 0.938)] [G loss: 5.232] [G acc: 0.188]\n",
      "3506 [D loss: (0.152)(R 0.217, F 0.087)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.653] [G acc: 0.188]\n",
      "3507 [D loss: (0.426)(R 0.451, F 0.401)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.662] [G acc: 0.125]\n",
      "3508 [D loss: (0.492)(R 0.279, F 0.705)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.023] [G acc: 0.312]\n",
      "3509 [D loss: (0.411)(R 0.292, F 0.529)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.342] [G acc: 0.312]\n",
      "3510 [D loss: (0.509)(R 0.487, F 0.531)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.540] [G acc: 0.375]\n",
      "3511 [D loss: (0.274)(R 0.297, F 0.250)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.468] [G acc: 0.250]\n",
      "3512 [D loss: (0.402)(R 0.387, F 0.417)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.985] [G acc: 0.312]\n",
      "3513 [D loss: (0.556)(R 0.203, F 0.908)] [D acc: (0.750)(1.000, 0.500)] [G loss: 4.353] [G acc: 0.125]\n",
      "3514 [D loss: (0.249)(R 0.278, F 0.219)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.532] [G acc: 0.188]\n",
      "3515 [D loss: (0.245)(R 0.477, F 0.012)] [D acc: (0.969)(0.938, 1.000)] [G loss: 4.358] [G acc: 0.250]\n",
      "3516 [D loss: (0.536)(R 0.544, F 0.529)] [D acc: (0.719)(0.750, 0.688)] [G loss: 4.036] [G acc: 0.250]\n",
      "3517 [D loss: (0.390)(R 0.289, F 0.492)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.810] [G acc: 0.375]\n",
      "3518 [D loss: (0.264)(R 0.239, F 0.288)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.269] [G acc: 0.188]\n",
      "3519 [D loss: (0.178)(R 0.172, F 0.183)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.472] [G acc: 0.250]\n",
      "3520 [D loss: (0.631)(R 0.296, F 0.967)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.591] [G acc: 0.250]\n",
      "3521 [D loss: (0.484)(R 0.448, F 0.519)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.871] [G acc: 0.438]\n",
      "3522 [D loss: (0.211)(R 0.243, F 0.179)] [D acc: (0.938)(0.938, 0.938)] [G loss: 4.215] [G acc: 0.312]\n",
      "3523 [D loss: (0.586)(R 0.646, F 0.525)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.944] [G acc: 0.250]\n",
      "3524 [D loss: (0.448)(R 0.434, F 0.461)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.233] [G acc: 0.375]\n",
      "3525 [D loss: (0.268)(R 0.256, F 0.279)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.874] [G acc: 0.250]\n",
      "3526 [D loss: (0.408)(R 0.209, F 0.607)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.103] [G acc: 0.250]\n",
      "3527 [D loss: (0.448)(R 0.484, F 0.412)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.559] [G acc: 0.125]\n",
      "3528 [D loss: (0.378)(R 0.307, F 0.450)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.761] [G acc: 0.375]\n",
      "3529 [D loss: (0.233)(R 0.176, F 0.291)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.263] [G acc: 0.188]\n",
      "3530 [D loss: (0.485)(R 0.582, F 0.389)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.585] [G acc: 0.312]\n",
      "3531 [D loss: (0.805)(R 1.325, F 0.284)] [D acc: (0.812)(0.750, 0.875)] [G loss: 3.198] [G acc: 0.312]\n",
      "3532 [D loss: (0.257)(R 0.252, F 0.262)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.454] [G acc: 0.250]\n",
      "3533 [D loss: (0.414)(R 0.321, F 0.507)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.221] [G acc: 0.125]\n",
      "3534 [D loss: (0.258)(R 0.296, F 0.220)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.356] [G acc: 0.188]\n",
      "3535 [D loss: (0.342)(R 0.353, F 0.331)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.604] [G acc: 0.250]\n",
      "3536 [D loss: (0.652)(R 0.433, F 0.870)] [D acc: (0.688)(0.875, 0.500)] [G loss: 3.052] [G acc: 0.312]\n",
      "3537 [D loss: (0.499)(R 0.334, F 0.664)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.391] [G acc: 0.312]\n",
      "3538 [D loss: (0.320)(R 0.301, F 0.340)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.096] [G acc: 0.125]\n",
      "3539 [D loss: (0.304)(R 0.218, F 0.390)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.616] [G acc: 0.312]\n",
      "3540 [D loss: (0.232)(R 0.299, F 0.166)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.568] [G acc: 0.250]\n",
      "3541 [D loss: (0.186)(R 0.275, F 0.098)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.883] [G acc: 0.250]\n",
      "3542 [D loss: (0.286)(R 0.188, F 0.383)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.762] [G acc: 0.250]\n",
      "3543 [D loss: (0.264)(R 0.355, F 0.174)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.946] [G acc: 0.188]\n",
      "3544 [D loss: (0.357)(R 0.317, F 0.398)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.978] [G acc: 0.188]\n",
      "3545 [D loss: (0.414)(R 0.498, F 0.330)] [D acc: (0.688)(0.688, 0.688)] [G loss: 3.934] [G acc: 0.125]\n",
      "3546 [D loss: (0.445)(R 0.319, F 0.570)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.602] [G acc: 0.312]\n",
      "3547 [D loss: (0.395)(R 0.512, F 0.277)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.591] [G acc: 0.312]\n",
      "3548 [D loss: (0.367)(R 0.270, F 0.465)] [D acc: (0.781)(0.938, 0.625)] [G loss: 4.039] [G acc: 0.188]\n",
      "3549 [D loss: (0.293)(R 0.211, F 0.375)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.271] [G acc: 0.312]\n",
      "3550 [D loss: (0.284)(R 0.274, F 0.294)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.098] [G acc: 0.312]\n",
      "3551 [D loss: (0.533)(R 0.618, F 0.448)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.582] [G acc: 0.375]\n",
      "3552 [D loss: (0.277)(R 0.192, F 0.363)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.347] [G acc: 0.250]\n",
      "3553 [D loss: (0.320)(R 0.194, F 0.446)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.457] [G acc: 0.188]\n",
      "3554 [D loss: (0.560)(R 0.886, F 0.235)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.576] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3555 [D loss: (0.476)(R 0.471, F 0.482)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.000] [G acc: 0.312]\n",
      "3556 [D loss: (0.321)(R 0.255, F 0.388)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.798] [G acc: 0.188]\n",
      "3557 [D loss: (0.437)(R 0.381, F 0.493)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.588] [G acc: 0.188]\n",
      "3558 [D loss: (0.294)(R 0.324, F 0.263)] [D acc: (0.844)(0.875, 0.812)] [G loss: 5.559] [G acc: 0.250]\n",
      "3559 [D loss: (0.491)(R 0.695, F 0.287)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.480] [G acc: 0.375]\n",
      "3560 [D loss: (0.721)(R 0.747, F 0.695)] [D acc: (0.688)(0.750, 0.625)] [G loss: 2.820] [G acc: 0.312]\n",
      "3561 [D loss: (0.414)(R 0.721, F 0.107)] [D acc: (0.938)(0.875, 1.000)] [G loss: 2.631] [G acc: 0.312]\n",
      "3562 [D loss: (0.703)(R 0.834, F 0.571)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.584] [G acc: 0.375]\n",
      "3563 [D loss: (0.300)(R 0.331, F 0.269)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.596] [G acc: 0.062]\n",
      "3564 [D loss: (0.193)(R 0.209, F 0.178)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.211] [G acc: 0.250]\n",
      "3565 [D loss: (0.356)(R 0.406, F 0.307)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.368] [G acc: 0.312]\n",
      "3566 [D loss: (0.454)(R 0.316, F 0.592)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.896] [G acc: 0.188]\n",
      "3567 [D loss: (0.525)(R 0.407, F 0.642)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.351] [G acc: 0.250]\n",
      "3568 [D loss: (0.601)(R 0.501, F 0.700)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.413] [G acc: 0.312]\n",
      "3569 [D loss: (0.448)(R 0.244, F 0.653)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.081] [G acc: 0.438]\n",
      "3570 [D loss: (0.527)(R 0.290, F 0.765)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.593] [G acc: 0.438]\n",
      "3571 [D loss: (0.520)(R 0.196, F 0.844)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.508] [G acc: 0.250]\n",
      "3572 [D loss: (0.443)(R 0.402, F 0.483)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.945] [G acc: 0.375]\n",
      "3573 [D loss: (0.366)(R 0.299, F 0.434)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.673] [G acc: 0.250]\n",
      "3574 [D loss: (0.399)(R 0.291, F 0.508)] [D acc: (0.844)(0.938, 0.750)] [G loss: 5.016] [G acc: 0.188]\n",
      "3575 [D loss: (0.325)(R 0.219, F 0.431)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.603] [G acc: 0.375]\n",
      "3576 [D loss: (0.227)(R 0.395, F 0.059)] [D acc: (0.938)(0.875, 1.000)] [G loss: 3.039] [G acc: 0.375]\n",
      "3577 [D loss: (0.730)(R 0.943, F 0.517)] [D acc: (0.812)(0.938, 0.688)] [G loss: 4.062] [G acc: 0.188]\n",
      "3578 [D loss: (0.351)(R 0.293, F 0.410)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.767] [G acc: 0.188]\n",
      "3579 [D loss: (0.374)(R 0.277, F 0.471)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.621] [G acc: 0.375]\n",
      "3580 [D loss: (0.487)(R 0.585, F 0.389)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.656] [G acc: 0.312]\n",
      "3581 [D loss: (0.629)(R 0.657, F 0.602)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.914] [G acc: 0.188]\n",
      "3582 [D loss: (0.543)(R 0.628, F 0.457)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.167] [G acc: 0.250]\n",
      "3583 [D loss: (0.888)(R 1.324, F 0.453)] [D acc: (0.719)(0.750, 0.688)] [G loss: 3.696] [G acc: 0.188]\n",
      "3584 [D loss: (0.284)(R 0.203, F 0.365)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.077] [G acc: 0.125]\n",
      "3585 [D loss: (0.486)(R 0.320, F 0.652)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.387] [G acc: 0.375]\n",
      "3586 [D loss: (0.350)(R 0.232, F 0.468)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.218] [G acc: 0.250]\n",
      "3587 [D loss: (0.764)(R 1.121, F 0.408)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.713] [G acc: 0.188]\n",
      "3588 [D loss: (0.671)(R 0.384, F 0.958)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.877] [G acc: 0.312]\n",
      "3589 [D loss: (0.370)(R 0.277, F 0.463)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.048] [G acc: 0.250]\n",
      "3590 [D loss: (0.545)(R 0.242, F 0.848)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.503] [G acc: 0.188]\n",
      "3591 [D loss: (0.287)(R 0.223, F 0.351)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.339] [G acc: 0.188]\n",
      "3592 [D loss: (0.357)(R 0.285, F 0.430)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.691] [G acc: 0.250]\n",
      "3593 [D loss: (0.309)(R 0.321, F 0.297)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.631] [G acc: 0.250]\n",
      "3594 [D loss: (0.436)(R 0.495, F 0.376)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.787] [G acc: 0.250]\n",
      "3595 [D loss: (0.336)(R 0.283, F 0.388)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.307] [G acc: 0.188]\n",
      "3596 [D loss: (0.324)(R 0.355, F 0.293)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.505] [G acc: 0.125]\n",
      "3597 [D loss: (0.243)(R 0.287, F 0.198)] [D acc: (0.906)(0.875, 0.938)] [G loss: 5.341] [G acc: 0.062]\n",
      "3598 [D loss: (0.744)(R 0.997, F 0.491)] [D acc: (0.781)(0.875, 0.688)] [G loss: 4.716] [G acc: 0.188]\n",
      "3599 [D loss: (0.660)(R 0.750, F 0.571)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.156] [G acc: 0.250]\n",
      "3600 [D loss: (0.217)(R 0.237, F 0.197)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.654] [G acc: 0.188]\n",
      "3601 [D loss: (0.355)(R 0.272, F 0.438)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.008] [G acc: 0.250]\n",
      "3602 [D loss: (0.305)(R 0.457, F 0.153)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.379] [G acc: 0.250]\n",
      "3603 [D loss: (0.277)(R 0.327, F 0.227)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.336] [G acc: 0.312]\n",
      "3604 [D loss: (0.457)(R 0.542, F 0.371)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.995] [G acc: 0.250]\n",
      "3605 [D loss: (0.442)(R 0.239, F 0.645)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.247] [G acc: 0.125]\n",
      "3606 [D loss: (0.494)(R 0.606, F 0.382)] [D acc: (0.781)(0.812, 0.750)] [G loss: 3.336] [G acc: 0.250]\n",
      "3607 [D loss: (0.382)(R 0.593, F 0.170)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.580] [G acc: 0.188]\n",
      "3608 [D loss: (0.372)(R 0.212, F 0.531)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.350] [G acc: 0.062]\n",
      "3609 [D loss: (0.354)(R 0.264, F 0.444)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.935] [G acc: 0.188]\n",
      "3610 [D loss: (0.372)(R 0.297, F 0.447)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.872] [G acc: 0.250]\n",
      "3611 [D loss: (0.956)(R 0.989, F 0.923)] [D acc: (0.625)(0.750, 0.500)] [G loss: 2.740] [G acc: 0.188]\n",
      "3612 [D loss: (0.443)(R 0.267, F 0.620)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.830] [G acc: 0.188]\n",
      "3613 [D loss: (0.414)(R 0.531, F 0.297)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.577] [G acc: 0.250]\n",
      "3614 [D loss: (0.564)(R 0.392, F 0.736)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.074] [G acc: 0.312]\n",
      "3615 [D loss: (0.598)(R 0.594, F 0.602)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.332] [G acc: 0.375]\n",
      "3616 [D loss: (0.611)(R 0.647, F 0.574)] [D acc: (0.688)(0.688, 0.688)] [G loss: 3.150] [G acc: 0.312]\n",
      "3617 [D loss: (0.458)(R 0.492, F 0.424)] [D acc: (0.875)(0.938, 0.812)] [G loss: 1.605] [G acc: 0.500]\n",
      "3618 [D loss: (0.546)(R 0.501, F 0.591)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.814] [G acc: 0.375]\n",
      "3619 [D loss: (0.349)(R 0.432, F 0.265)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.134] [G acc: 0.312]\n",
      "3620 [D loss: (0.444)(R 0.323, F 0.565)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.677] [G acc: 0.438]\n",
      "3621 [D loss: (0.555)(R 0.527, F 0.582)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.770] [G acc: 0.250]\n",
      "3622 [D loss: (0.248)(R 0.226, F 0.269)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.875] [G acc: 0.312]\n",
      "3623 [D loss: (0.387)(R 0.425, F 0.349)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.138] [G acc: 0.375]\n",
      "3624 [D loss: (0.430)(R 0.312, F 0.548)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.649] [G acc: 0.312]\n",
      "3625 [D loss: (0.502)(R 0.354, F 0.650)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.352] [G acc: 0.375]\n",
      "3626 [D loss: (0.384)(R 0.293, F 0.474)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.944] [G acc: 0.312]\n",
      "3627 [D loss: (0.325)(R 0.209, F 0.440)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.611] [G acc: 0.438]\n",
      "3628 [D loss: (0.575)(R 0.198, F 0.951)] [D acc: (0.656)(1.000, 0.312)] [G loss: 3.379] [G acc: 0.188]\n",
      "3629 [D loss: (0.477)(R 0.413, F 0.540)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.658] [G acc: 0.250]\n",
      "3630 [D loss: (0.622)(R 0.668, F 0.576)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.553] [G acc: 0.312]\n",
      "3631 [D loss: (0.499)(R 0.338, F 0.661)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.488] [G acc: 0.312]\n",
      "3632 [D loss: (0.258)(R 0.265, F 0.251)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.498] [G acc: 0.312]\n",
      "3633 [D loss: (0.484)(R 0.398, F 0.570)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.688] [G acc: 0.312]\n",
      "3634 [D loss: (0.540)(R 0.271, F 0.810)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.541] [G acc: 0.250]\n",
      "3635 [D loss: (0.549)(R 0.596, F 0.503)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.320] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3636 [D loss: (0.369)(R 0.303, F 0.436)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.130] [G acc: 0.312]\n",
      "3637 [D loss: (0.353)(R 0.319, F 0.388)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.596] [G acc: 0.062]\n",
      "3638 [D loss: (0.245)(R 0.223, F 0.268)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.656] [G acc: 0.188]\n",
      "3639 [D loss: (0.465)(R 0.406, F 0.525)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.155] [G acc: 0.312]\n",
      "3640 [D loss: (0.329)(R 0.273, F 0.385)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.369] [G acc: 0.250]\n",
      "3641 [D loss: (0.810)(R 0.602, F 1.017)] [D acc: (0.562)(0.812, 0.312)] [G loss: 3.313] [G acc: 0.312]\n",
      "3642 [D loss: (0.249)(R 0.240, F 0.257)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.374] [G acc: 0.188]\n",
      "3643 [D loss: (0.400)(R 0.337, F 0.463)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.104] [G acc: 0.375]\n",
      "3644 [D loss: (0.336)(R 0.254, F 0.419)] [D acc: (0.875)(1.000, 0.750)] [G loss: 1.457] [G acc: 0.312]\n",
      "3645 [D loss: (0.465)(R 0.423, F 0.508)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.178] [G acc: 0.250]\n",
      "3646 [D loss: (0.408)(R 0.649, F 0.168)] [D acc: (0.875)(0.812, 0.938)] [G loss: 4.499] [G acc: 0.062]\n",
      "3647 [D loss: (0.168)(R 0.279, F 0.058)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.931] [G acc: 0.125]\n",
      "3648 [D loss: (0.232)(R 0.336, F 0.127)] [D acc: (0.906)(0.875, 0.938)] [G loss: 3.033] [G acc: 0.188]\n",
      "3649 [D loss: (0.603)(R 0.449, F 0.758)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.601] [G acc: 0.250]\n",
      "3650 [D loss: (0.286)(R 0.308, F 0.263)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.951] [G acc: 0.188]\n",
      "3651 [D loss: (0.373)(R 0.308, F 0.438)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.168] [G acc: 0.312]\n",
      "3652 [D loss: (0.452)(R 0.421, F 0.483)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.044] [G acc: 0.250]\n",
      "3653 [D loss: (0.504)(R 0.427, F 0.580)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.737] [G acc: 0.250]\n",
      "3654 [D loss: (0.369)(R 0.255, F 0.484)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.434] [G acc: 0.500]\n",
      "3655 [D loss: (0.397)(R 0.360, F 0.434)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.419] [G acc: 0.375]\n",
      "3656 [D loss: (0.456)(R 0.325, F 0.588)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.556] [G acc: 0.312]\n",
      "3657 [D loss: (0.402)(R 0.412, F 0.392)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.605] [G acc: 0.438]\n",
      "3658 [D loss: (0.446)(R 0.313, F 0.580)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.567] [G acc: 0.250]\n",
      "3659 [D loss: (0.494)(R 0.225, F 0.763)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.267] [G acc: 0.312]\n",
      "3660 [D loss: (0.563)(R 0.359, F 0.767)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.637] [G acc: 0.312]\n",
      "3661 [D loss: (0.325)(R 0.241, F 0.408)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.506] [G acc: 0.250]\n",
      "3662 [D loss: (0.540)(R 0.683, F 0.397)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.593] [G acc: 0.250]\n",
      "3663 [D loss: (0.612)(R 0.643, F 0.582)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.715] [G acc: 0.375]\n",
      "3664 [D loss: (0.517)(R 0.410, F 0.624)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.821] [G acc: 0.375]\n",
      "3665 [D loss: (0.429)(R 0.406, F 0.453)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.577] [G acc: 0.438]\n",
      "3666 [D loss: (0.337)(R 0.292, F 0.382)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.275] [G acc: 0.375]\n",
      "3667 [D loss: (0.324)(R 0.273, F 0.375)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.035] [G acc: 0.312]\n",
      "3668 [D loss: (0.416)(R 0.573, F 0.260)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.474] [G acc: 0.375]\n",
      "3669 [D loss: (0.399)(R 0.265, F 0.533)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.162] [G acc: 0.312]\n",
      "3670 [D loss: (0.569)(R 0.354, F 0.784)] [D acc: (0.688)(0.875, 0.500)] [G loss: 3.869] [G acc: 0.250]\n",
      "3671 [D loss: (0.517)(R 0.284, F 0.750)] [D acc: (0.719)(1.000, 0.438)] [G loss: 3.086] [G acc: 0.312]\n",
      "3672 [D loss: (0.620)(R 0.632, F 0.608)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.237] [G acc: 0.250]\n",
      "3673 [D loss: (0.464)(R 0.361, F 0.567)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.082] [G acc: 0.188]\n",
      "3674 [D loss: (0.414)(R 0.237, F 0.591)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.105] [G acc: 0.312]\n",
      "3675 [D loss: (0.443)(R 0.460, F 0.426)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.474] [G acc: 0.250]\n",
      "3676 [D loss: (0.591)(R 0.441, F 0.740)] [D acc: (0.656)(0.875, 0.438)] [G loss: 3.016] [G acc: 0.250]\n",
      "3677 [D loss: (0.388)(R 0.289, F 0.486)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.845] [G acc: 0.312]\n",
      "3678 [D loss: (0.411)(R 0.454, F 0.367)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.470] [G acc: 0.250]\n",
      "3679 [D loss: (0.339)(R 0.302, F 0.376)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.360] [G acc: 0.312]\n",
      "3680 [D loss: (0.378)(R 0.278, F 0.479)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.214] [G acc: 0.188]\n",
      "3681 [D loss: (0.466)(R 0.423, F 0.508)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.278] [G acc: 0.312]\n",
      "3682 [D loss: (0.277)(R 0.251, F 0.302)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.498] [G acc: 0.188]\n",
      "3683 [D loss: (0.431)(R 0.235, F 0.628)] [D acc: (0.781)(1.000, 0.562)] [G loss: 3.129] [G acc: 0.250]\n",
      "3684 [D loss: (0.339)(R 0.246, F 0.433)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.401] [G acc: 0.312]\n",
      "3685 [D loss: (0.439)(R 0.325, F 0.553)] [D acc: (0.844)(1.000, 0.688)] [G loss: 4.269] [G acc: 0.188]\n",
      "3686 [D loss: (0.906)(R 1.188, F 0.624)] [D acc: (0.750)(0.812, 0.688)] [G loss: 4.370] [G acc: 0.312]\n",
      "3687 [D loss: (0.497)(R 0.446, F 0.548)] [D acc: (0.719)(0.812, 0.625)] [G loss: 3.017] [G acc: 0.375]\n",
      "3688 [D loss: (0.524)(R 0.384, F 0.664)] [D acc: (0.719)(0.875, 0.562)] [G loss: 3.244] [G acc: 0.125]\n",
      "3689 [D loss: (0.828)(R 1.057, F 0.598)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.531] [G acc: 0.375]\n",
      "3690 [D loss: (0.498)(R 0.631, F 0.364)] [D acc: (0.812)(0.812, 0.812)] [G loss: 4.066] [G acc: 0.188]\n",
      "3691 [D loss: (0.440)(R 0.262, F 0.618)] [D acc: (0.750)(1.000, 0.500)] [G loss: 3.560] [G acc: 0.312]\n",
      "3692 [D loss: (0.183)(R 0.306, F 0.061)] [D acc: (1.000)(1.000, 1.000)] [G loss: 3.183] [G acc: 0.250]\n",
      "3693 [D loss: (0.348)(R 0.241, F 0.455)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.140] [G acc: 0.188]\n",
      "3694 [D loss: (0.309)(R 0.439, F 0.178)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.692] [G acc: 0.062]\n",
      "3695 [D loss: (0.394)(R 0.341, F 0.448)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.597] [G acc: 0.312]\n",
      "3696 [D loss: (0.758)(R 0.504, F 1.012)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.489] [G acc: 0.125]\n",
      "3697 [D loss: (0.302)(R 0.261, F 0.342)] [D acc: (0.906)(1.000, 0.812)] [G loss: 4.586] [G acc: 0.000]\n",
      "3698 [D loss: (0.670)(R 0.848, F 0.492)] [D acc: (0.750)(0.750, 0.750)] [G loss: 2.384] [G acc: 0.312]\n",
      "3699 [D loss: (0.383)(R 0.287, F 0.479)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.463] [G acc: 0.188]\n",
      "3700 [D loss: (0.361)(R 0.554, F 0.169)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.676] [G acc: 0.312]\n",
      "3701 [D loss: (0.440)(R 0.361, F 0.519)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.586] [G acc: 0.250]\n",
      "3702 [D loss: (0.305)(R 0.241, F 0.368)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.392] [G acc: 0.312]\n",
      "3703 [D loss: (0.267)(R 0.224, F 0.310)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.422] [G acc: 0.312]\n",
      "3704 [D loss: (0.490)(R 0.534, F 0.446)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.168] [G acc: 0.312]\n",
      "3705 [D loss: (0.570)(R 0.294, F 0.847)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.995] [G acc: 0.375]\n",
      "3706 [D loss: (0.412)(R 0.355, F 0.468)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.056] [G acc: 0.250]\n",
      "3707 [D loss: (0.942)(R 1.325, F 0.559)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.166] [G acc: 0.250]\n",
      "3708 [D loss: (0.521)(R 0.372, F 0.671)] [D acc: (0.688)(0.875, 0.500)] [G loss: 3.424] [G acc: 0.312]\n",
      "3709 [D loss: (0.416)(R 0.396, F 0.436)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.576] [G acc: 0.375]\n",
      "3710 [D loss: (0.511)(R 0.652, F 0.369)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.991] [G acc: 0.375]\n",
      "3711 [D loss: (0.859)(R 0.976, F 0.743)] [D acc: (0.656)(0.812, 0.500)] [G loss: 2.202] [G acc: 0.250]\n",
      "3712 [D loss: (0.336)(R 0.265, F 0.408)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.770] [G acc: 0.312]\n",
      "3713 [D loss: (0.300)(R 0.345, F 0.255)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.555] [G acc: 0.125]\n",
      "3714 [D loss: (0.556)(R 0.573, F 0.538)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.192] [G acc: 0.312]\n",
      "3715 [D loss: (0.414)(R 0.337, F 0.491)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.892] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3716 [D loss: (0.584)(R 0.719, F 0.449)] [D acc: (0.750)(0.750, 0.750)] [G loss: 3.081] [G acc: 0.250]\n",
      "3717 [D loss: (0.313)(R 0.257, F 0.369)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.909] [G acc: 0.250]\n",
      "3718 [D loss: (0.494)(R 0.634, F 0.354)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.657] [G acc: 0.312]\n",
      "3719 [D loss: (0.427)(R 0.362, F 0.492)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.575] [G acc: 0.312]\n",
      "3720 [D loss: (0.531)(R 0.618, F 0.445)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.577] [G acc: 0.250]\n",
      "3721 [D loss: (0.432)(R 0.527, F 0.337)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.869] [G acc: 0.375]\n",
      "3722 [D loss: (0.555)(R 0.385, F 0.724)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.584] [G acc: 0.312]\n",
      "3723 [D loss: (0.486)(R 0.348, F 0.623)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.783] [G acc: 0.312]\n",
      "3724 [D loss: (0.433)(R 0.273, F 0.594)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.419] [G acc: 0.250]\n",
      "3725 [D loss: (0.486)(R 0.498, F 0.475)] [D acc: (0.750)(0.875, 0.625)] [G loss: 4.088] [G acc: 0.250]\n",
      "3726 [D loss: (0.749)(R 1.228, F 0.269)] [D acc: (0.781)(0.750, 0.812)] [G loss: 2.233] [G acc: 0.312]\n",
      "3727 [D loss: (0.533)(R 0.797, F 0.268)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.310] [G acc: 0.250]\n",
      "3728 [D loss: (0.323)(R 0.260, F 0.387)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.083] [G acc: 0.312]\n",
      "3729 [D loss: (0.431)(R 0.362, F 0.500)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.277] [G acc: 0.125]\n",
      "3730 [D loss: (0.403)(R 0.426, F 0.380)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.112] [G acc: 0.250]\n",
      "3731 [D loss: (0.320)(R 0.262, F 0.378)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.542] [G acc: 0.250]\n",
      "3732 [D loss: (0.496)(R 0.841, F 0.151)] [D acc: (0.969)(0.938, 1.000)] [G loss: 2.649] [G acc: 0.312]\n",
      "3733 [D loss: (0.364)(R 0.431, F 0.298)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.672] [G acc: 0.312]\n",
      "3734 [D loss: (0.677)(R 0.785, F 0.570)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.195] [G acc: 0.250]\n",
      "3735 [D loss: (1.013)(R 1.461, F 0.566)] [D acc: (0.656)(0.625, 0.688)] [G loss: 2.444] [G acc: 0.000]\n",
      "3736 [D loss: (0.760)(R 0.626, F 0.893)] [D acc: (0.594)(0.812, 0.375)] [G loss: 2.616] [G acc: 0.312]\n",
      "3737 [D loss: (0.375)(R 0.273, F 0.476)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.198] [G acc: 0.375]\n",
      "3738 [D loss: (0.494)(R 0.448, F 0.539)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.473] [G acc: 0.312]\n",
      "3739 [D loss: (0.331)(R 0.274, F 0.388)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.007] [G acc: 0.125]\n",
      "3740 [D loss: (0.506)(R 0.432, F 0.580)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.666] [G acc: 0.312]\n",
      "3741 [D loss: (0.349)(R 0.311, F 0.386)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.629] [G acc: 0.312]\n",
      "3742 [D loss: (0.688)(R 0.840, F 0.536)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.914] [G acc: 0.312]\n",
      "3743 [D loss: (0.298)(R 0.238, F 0.359)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.463] [G acc: 0.125]\n",
      "3744 [D loss: (0.452)(R 0.382, F 0.523)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.176] [G acc: 0.438]\n",
      "3745 [D loss: (0.587)(R 0.457, F 0.717)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.607] [G acc: 0.312]\n",
      "3746 [D loss: (0.453)(R 0.489, F 0.417)] [D acc: (0.781)(0.750, 0.812)] [G loss: 2.504] [G acc: 0.500]\n",
      "3747 [D loss: (0.461)(R 0.509, F 0.413)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.574] [G acc: 0.188]\n",
      "3748 [D loss: (0.763)(R 0.711, F 0.815)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.043] [G acc: 0.312]\n",
      "3749 [D loss: (0.592)(R 0.373, F 0.812)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.763] [G acc: 0.500]\n",
      "3750 [D loss: (0.800)(R 0.863, F 0.736)] [D acc: (0.625)(0.812, 0.438)] [G loss: 2.980] [G acc: 0.312]\n",
      "3751 [D loss: (0.518)(R 0.437, F 0.598)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.049] [G acc: 0.312]\n",
      "3752 [D loss: (0.490)(R 0.326, F 0.655)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.580] [G acc: 0.312]\n",
      "3753 [D loss: (0.444)(R 0.429, F 0.458)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.261] [G acc: 0.250]\n",
      "3754 [D loss: (0.282)(R 0.299, F 0.265)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.254] [G acc: 0.125]\n",
      "3755 [D loss: (0.511)(R 0.599, F 0.423)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.880] [G acc: 0.250]\n",
      "3756 [D loss: (0.650)(R 0.761, F 0.538)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.876] [G acc: 0.375]\n",
      "3757 [D loss: (0.595)(R 0.385, F 0.804)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.843] [G acc: 0.188]\n",
      "3758 [D loss: (0.611)(R 0.450, F 0.772)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.925] [G acc: 0.312]\n",
      "3759 [D loss: (0.272)(R 0.373, F 0.170)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.080] [G acc: 0.250]\n",
      "3760 [D loss: (0.334)(R 0.343, F 0.325)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.829] [G acc: 0.312]\n",
      "3761 [D loss: (0.511)(R 0.503, F 0.519)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.491] [G acc: 0.250]\n",
      "3762 [D loss: (0.507)(R 0.530, F 0.485)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.067] [G acc: 0.375]\n",
      "3763 [D loss: (0.668)(R 0.791, F 0.544)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.795] [G acc: 0.188]\n",
      "3764 [D loss: (0.545)(R 0.494, F 0.595)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.118] [G acc: 0.250]\n",
      "3765 [D loss: (0.565)(R 0.532, F 0.599)] [D acc: (0.719)(0.812, 0.625)] [G loss: 3.487] [G acc: 0.250]\n",
      "3766 [D loss: (0.254)(R 0.294, F 0.214)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.813] [G acc: 0.125]\n",
      "3767 [D loss: (0.508)(R 0.275, F 0.740)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.021] [G acc: 0.375]\n",
      "3768 [D loss: (0.562)(R 0.713, F 0.411)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.276] [G acc: 0.312]\n",
      "3769 [D loss: (0.511)(R 0.454, F 0.567)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.226] [G acc: 0.438]\n",
      "3770 [D loss: (0.537)(R 0.519, F 0.556)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.933] [G acc: 0.375]\n",
      "3771 [D loss: (0.364)(R 0.379, F 0.350)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.398] [G acc: 0.312]\n",
      "3772 [D loss: (0.390)(R 0.380, F 0.400)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.670] [G acc: 0.312]\n",
      "3773 [D loss: (0.554)(R 0.495, F 0.614)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.654] [G acc: 0.312]\n",
      "3774 [D loss: (0.385)(R 0.301, F 0.469)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.457] [G acc: 0.438]\n",
      "3775 [D loss: (0.471)(R 0.458, F 0.483)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.556] [G acc: 0.250]\n",
      "3776 [D loss: (0.490)(R 0.587, F 0.392)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.220] [G acc: 0.375]\n",
      "3777 [D loss: (0.411)(R 0.281, F 0.541)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.565] [G acc: 0.312]\n",
      "3778 [D loss: (0.477)(R 0.577, F 0.377)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.273] [G acc: 0.375]\n",
      "3779 [D loss: (0.556)(R 0.387, F 0.725)] [D acc: (0.688)(0.938, 0.438)] [G loss: 3.326] [G acc: 0.312]\n",
      "3780 [D loss: (0.574)(R 0.753, F 0.396)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.937] [G acc: 0.375]\n",
      "3781 [D loss: (0.739)(R 0.887, F 0.591)] [D acc: (0.625)(0.688, 0.562)] [G loss: 2.358] [G acc: 0.312]\n",
      "3782 [D loss: (0.521)(R 0.435, F 0.607)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.185] [G acc: 0.250]\n",
      "3783 [D loss: (0.366)(R 0.398, F 0.333)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.428] [G acc: 0.312]\n",
      "3784 [D loss: (0.550)(R 0.507, F 0.593)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.113] [G acc: 0.312]\n",
      "3785 [D loss: (0.678)(R 0.819, F 0.537)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.472] [G acc: 0.312]\n",
      "3786 [D loss: (0.560)(R 0.360, F 0.759)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.140] [G acc: 0.375]\n",
      "3787 [D loss: (0.374)(R 0.489, F 0.259)] [D acc: (0.844)(0.812, 0.875)] [G loss: 3.022] [G acc: 0.250]\n",
      "3788 [D loss: (0.451)(R 0.495, F 0.406)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.326] [G acc: 0.312]\n",
      "3789 [D loss: (0.570)(R 0.689, F 0.451)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.648] [G acc: 0.062]\n",
      "3790 [D loss: (0.283)(R 0.278, F 0.288)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.267] [G acc: 0.125]\n",
      "3791 [D loss: (0.444)(R 0.265, F 0.624)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.870] [G acc: 0.250]\n",
      "3792 [D loss: (0.652)(R 0.834, F 0.471)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.699] [G acc: 0.375]\n",
      "3793 [D loss: (0.300)(R 0.389, F 0.211)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.351] [G acc: 0.312]\n",
      "3794 [D loss: (0.316)(R 0.248, F 0.384)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.759] [G acc: 0.312]\n",
      "3795 [D loss: (0.833)(R 1.120, F 0.545)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.376] [G acc: 0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3796 [D loss: (0.314)(R 0.287, F 0.341)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.633] [G acc: 0.375]\n",
      "3797 [D loss: (0.454)(R 0.325, F 0.584)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.772] [G acc: 0.188]\n",
      "3798 [D loss: (0.456)(R 0.434, F 0.479)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.101] [G acc: 0.312]\n",
      "3799 [D loss: (0.496)(R 0.613, F 0.380)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.829] [G acc: 0.250]\n",
      "3800 [D loss: (0.644)(R 0.600, F 0.687)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.764] [G acc: 0.312]\n",
      "3801 [D loss: (0.457)(R 0.378, F 0.536)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.022] [G acc: 0.500]\n",
      "3802 [D loss: (0.619)(R 0.494, F 0.743)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.013] [G acc: 0.250]\n",
      "3803 [D loss: (0.463)(R 0.490, F 0.436)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.157] [G acc: 0.312]\n",
      "3804 [D loss: (0.568)(R 0.748, F 0.388)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.625] [G acc: 0.188]\n",
      "3805 [D loss: (0.385)(R 0.306, F 0.464)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.715] [G acc: 0.312]\n",
      "3806 [D loss: (0.437)(R 0.520, F 0.354)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.978] [G acc: 0.312]\n",
      "3807 [D loss: (0.385)(R 0.410, F 0.359)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.120] [G acc: 0.375]\n",
      "3808 [D loss: (0.537)(R 0.412, F 0.662)] [D acc: (0.625)(0.875, 0.375)] [G loss: 2.518] [G acc: 0.312]\n",
      "3809 [D loss: (0.487)(R 0.427, F 0.546)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.680] [G acc: 0.250]\n",
      "3810 [D loss: (0.536)(R 0.659, F 0.412)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.335] [G acc: 0.188]\n",
      "3811 [D loss: (0.457)(R 0.479, F 0.434)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.664] [G acc: 0.250]\n",
      "3812 [D loss: (0.358)(R 0.369, F 0.346)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.369] [G acc: 0.375]\n",
      "3813 [D loss: (0.462)(R 0.481, F 0.442)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.820] [G acc: 0.312]\n",
      "3814 [D loss: (0.537)(R 0.480, F 0.594)] [D acc: (0.656)(0.812, 0.500)] [G loss: 3.459] [G acc: 0.375]\n",
      "3815 [D loss: (0.435)(R 0.544, F 0.326)] [D acc: (0.781)(0.750, 0.812)] [G loss: 2.281] [G acc: 0.438]\n",
      "3816 [D loss: (0.677)(R 0.662, F 0.692)] [D acc: (0.656)(0.750, 0.562)] [G loss: 2.354] [G acc: 0.375]\n",
      "3817 [D loss: (0.418)(R 0.303, F 0.532)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.607] [G acc: 0.250]\n",
      "3818 [D loss: (0.438)(R 0.445, F 0.430)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.608] [G acc: 0.312]\n",
      "3819 [D loss: (0.500)(R 0.343, F 0.658)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.681] [G acc: 0.438]\n",
      "3820 [D loss: (0.715)(R 0.731, F 0.700)] [D acc: (0.625)(0.750, 0.500)] [G loss: 2.752] [G acc: 0.250]\n",
      "3821 [D loss: (0.650)(R 0.645, F 0.656)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.039] [G acc: 0.438]\n",
      "3822 [D loss: (0.360)(R 0.329, F 0.390)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.461] [G acc: 0.312]\n",
      "3823 [D loss: (0.349)(R 0.434, F 0.264)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.848] [G acc: 0.312]\n",
      "3824 [D loss: (0.365)(R 0.491, F 0.239)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.192] [G acc: 0.312]\n",
      "3825 [D loss: (0.759)(R 0.419, F 1.098)] [D acc: (0.562)(0.812, 0.312)] [G loss: 2.649] [G acc: 0.375]\n",
      "3826 [D loss: (0.629)(R 0.554, F 0.705)] [D acc: (0.594)(0.688, 0.500)] [G loss: 2.743] [G acc: 0.375]\n",
      "3827 [D loss: (0.590)(R 0.427, F 0.752)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.325] [G acc: 0.375]\n",
      "3828 [D loss: (0.539)(R 0.408, F 0.670)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.184] [G acc: 0.375]\n",
      "3829 [D loss: (0.591)(R 0.432, F 0.751)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.197] [G acc: 0.250]\n",
      "3830 [D loss: (0.620)(R 0.547, F 0.692)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.607] [G acc: 0.188]\n",
      "3831 [D loss: (0.691)(R 0.682, F 0.701)] [D acc: (0.594)(0.688, 0.500)] [G loss: 2.560] [G acc: 0.250]\n",
      "3832 [D loss: (0.363)(R 0.572, F 0.154)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.010] [G acc: 0.375]\n",
      "3833 [D loss: (0.429)(R 0.262, F 0.596)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.657] [G acc: 0.375]\n",
      "3834 [D loss: (0.503)(R 0.450, F 0.555)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.383] [G acc: 0.250]\n",
      "3835 [D loss: (0.488)(R 0.449, F 0.528)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.326] [G acc: 0.250]\n",
      "3836 [D loss: (0.584)(R 0.648, F 0.519)] [D acc: (0.719)(0.875, 0.562)] [G loss: 3.868] [G acc: 0.188]\n",
      "3837 [D loss: (0.452)(R 0.395, F 0.509)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.806] [G acc: 0.375]\n",
      "3838 [D loss: (0.699)(R 0.459, F 0.939)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.532] [G acc: 0.375]\n",
      "3839 [D loss: (0.429)(R 0.360, F 0.497)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.407] [G acc: 0.375]\n",
      "3840 [D loss: (0.280)(R 0.304, F 0.257)] [D acc: (0.969)(1.000, 0.938)] [G loss: 2.394] [G acc: 0.375]\n",
      "3841 [D loss: (0.633)(R 0.561, F 0.705)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.552] [G acc: 0.250]\n",
      "3842 [D loss: (0.442)(R 0.468, F 0.416)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.536] [G acc: 0.375]\n",
      "3843 [D loss: (0.397)(R 0.357, F 0.437)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.209] [G acc: 0.250]\n",
      "3844 [D loss: (0.649)(R 0.787, F 0.511)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.050] [G acc: 0.375]\n",
      "3845 [D loss: (0.442)(R 0.364, F 0.519)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.349] [G acc: 0.375]\n",
      "3846 [D loss: (0.733)(R 0.464, F 1.001)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.900] [G acc: 0.438]\n",
      "3847 [D loss: (0.423)(R 0.326, F 0.520)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.977] [G acc: 0.375]\n",
      "3848 [D loss: (0.655)(R 0.680, F 0.630)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.281] [G acc: 0.188]\n",
      "3849 [D loss: (0.529)(R 0.719, F 0.339)] [D acc: (0.844)(0.812, 0.875)] [G loss: 1.564] [G acc: 0.500]\n",
      "3850 [D loss: (0.583)(R 0.459, F 0.706)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.583] [G acc: 0.500]\n",
      "3851 [D loss: (0.486)(R 0.479, F 0.493)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.036] [G acc: 0.375]\n",
      "3852 [D loss: (0.573)(R 0.307, F 0.839)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.771] [G acc: 0.375]\n",
      "3853 [D loss: (0.387)(R 0.273, F 0.501)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.145] [G acc: 0.312]\n",
      "3854 [D loss: (0.526)(R 0.505, F 0.547)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.340] [G acc: 0.375]\n",
      "3855 [D loss: (0.547)(R 0.455, F 0.639)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.837] [G acc: 0.312]\n",
      "3856 [D loss: (0.552)(R 0.429, F 0.675)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.542] [G acc: 0.562]\n",
      "3857 [D loss: (0.432)(R 0.353, F 0.511)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.198] [G acc: 0.375]\n",
      "3858 [D loss: (0.582)(R 0.433, F 0.731)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.862] [G acc: 0.438]\n",
      "3859 [D loss: (0.419)(R 0.417, F 0.421)] [D acc: (0.875)(0.875, 0.875)] [G loss: 1.792] [G acc: 0.375]\n",
      "3860 [D loss: (0.498)(R 0.303, F 0.694)] [D acc: (0.719)(1.000, 0.438)] [G loss: 2.337] [G acc: 0.312]\n",
      "3861 [D loss: (0.709)(R 1.092, F 0.326)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.754] [G acc: 0.188]\n",
      "3862 [D loss: (0.228)(R 0.310, F 0.146)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.749] [G acc: 0.125]\n",
      "3863 [D loss: (0.449)(R 0.326, F 0.572)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.626] [G acc: 0.125]\n",
      "3864 [D loss: (0.471)(R 0.323, F 0.620)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.213] [G acc: 0.250]\n",
      "3865 [D loss: (0.547)(R 0.788, F 0.305)] [D acc: (0.781)(0.750, 0.812)] [G loss: 2.988] [G acc: 0.312]\n",
      "3866 [D loss: (0.535)(R 0.510, F 0.559)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.118] [G acc: 0.250]\n",
      "3867 [D loss: (0.633)(R 0.505, F 0.761)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.593] [G acc: 0.312]\n",
      "3868 [D loss: (0.487)(R 0.469, F 0.505)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.887] [G acc: 0.438]\n",
      "3869 [D loss: (0.434)(R 0.340, F 0.529)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.759] [G acc: 0.375]\n",
      "3870 [D loss: (0.528)(R 0.432, F 0.624)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.516] [G acc: 0.250]\n",
      "3871 [D loss: (0.591)(R 0.407, F 0.775)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.401] [G acc: 0.250]\n",
      "3872 [D loss: (0.413)(R 0.353, F 0.472)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.637] [G acc: 0.375]\n",
      "3873 [D loss: (0.416)(R 0.303, F 0.528)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.661] [G acc: 0.188]\n",
      "3874 [D loss: (0.522)(R 0.619, F 0.425)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.567] [G acc: 0.312]\n",
      "3875 [D loss: (0.288)(R 0.406, F 0.170)] [D acc: (0.938)(0.938, 0.938)] [G loss: 1.794] [G acc: 0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3876 [D loss: (0.598)(R 0.648, F 0.548)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.626] [G acc: 0.438]\n",
      "3877 [D loss: (0.652)(R 0.802, F 0.503)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.521] [G acc: 0.250]\n",
      "3878 [D loss: (0.304)(R 0.300, F 0.308)] [D acc: (0.938)(1.000, 0.875)] [G loss: 1.708] [G acc: 0.312]\n",
      "3879 [D loss: (0.490)(R 0.332, F 0.648)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.582] [G acc: 0.312]\n",
      "3880 [D loss: (0.408)(R 0.336, F 0.480)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.514] [G acc: 0.250]\n",
      "3881 [D loss: (0.374)(R 0.324, F 0.424)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.897] [G acc: 0.375]\n",
      "3882 [D loss: (0.425)(R 0.279, F 0.570)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.674] [G acc: 0.375]\n",
      "3883 [D loss: (0.445)(R 0.290, F 0.600)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.485] [G acc: 0.188]\n",
      "3884 [D loss: (0.610)(R 0.559, F 0.662)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.857] [G acc: 0.500]\n",
      "3885 [D loss: (0.355)(R 0.277, F 0.434)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.301] [G acc: 0.375]\n",
      "3886 [D loss: (0.598)(R 0.305, F 0.890)] [D acc: (0.688)(1.000, 0.375)] [G loss: 2.464] [G acc: 0.375]\n",
      "3887 [D loss: (0.494)(R 0.299, F 0.690)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.970] [G acc: 0.250]\n",
      "3888 [D loss: (0.445)(R 0.440, F 0.451)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.079] [G acc: 0.188]\n",
      "3889 [D loss: (0.610)(R 0.580, F 0.640)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.185] [G acc: 0.312]\n",
      "3890 [D loss: (0.359)(R 0.412, F 0.306)] [D acc: (0.875)(0.875, 0.875)] [G loss: 1.746] [G acc: 0.312]\n",
      "3891 [D loss: (0.391)(R 0.333, F 0.449)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.534] [G acc: 0.312]\n",
      "3892 [D loss: (0.423)(R 0.484, F 0.362)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.553] [G acc: 0.188]\n",
      "3893 [D loss: (0.492)(R 0.625, F 0.359)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.325] [G acc: 0.188]\n",
      "3894 [D loss: (0.409)(R 0.406, F 0.412)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.103] [G acc: 0.438]\n",
      "3895 [D loss: (0.429)(R 0.284, F 0.574)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.931] [G acc: 0.250]\n",
      "3896 [D loss: (0.287)(R 0.312, F 0.262)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.491] [G acc: 0.188]\n",
      "3897 [D loss: (0.264)(R 0.340, F 0.188)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.462] [G acc: 0.125]\n",
      "3898 [D loss: (0.527)(R 0.576, F 0.477)] [D acc: (0.750)(0.812, 0.688)] [G loss: 3.083] [G acc: 0.188]\n",
      "3899 [D loss: (0.352)(R 0.318, F 0.385)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.447] [G acc: 0.312]\n",
      "3900 [D loss: (0.282)(R 0.323, F 0.241)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.861] [G acc: 0.250]\n",
      "3901 [D loss: (0.486)(R 0.438, F 0.534)] [D acc: (0.750)(0.875, 0.625)] [G loss: 3.718] [G acc: 0.312]\n",
      "3902 [D loss: (0.463)(R 0.308, F 0.618)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.695] [G acc: 0.250]\n",
      "3903 [D loss: (0.482)(R 0.425, F 0.539)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.714] [G acc: 0.438]\n",
      "3904 [D loss: (0.637)(R 0.852, F 0.423)] [D acc: (0.719)(0.688, 0.750)] [G loss: 2.085] [G acc: 0.375]\n",
      "3905 [D loss: (0.296)(R 0.327, F 0.265)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.135] [G acc: 0.250]\n",
      "3906 [D loss: (0.450)(R 0.414, F 0.487)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.995] [G acc: 0.250]\n",
      "3907 [D loss: (0.470)(R 0.512, F 0.428)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.638] [G acc: 0.125]\n",
      "3908 [D loss: (0.279)(R 0.292, F 0.266)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.441] [G acc: 0.062]\n",
      "3909 [D loss: (0.231)(R 0.285, F 0.177)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.390] [G acc: 0.125]\n",
      "3910 [D loss: (0.375)(R 0.411, F 0.338)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.629] [G acc: 0.250]\n",
      "3911 [D loss: (0.528)(R 0.540, F 0.515)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.751] [G acc: 0.250]\n",
      "3912 [D loss: (0.467)(R 0.569, F 0.366)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.658] [G acc: 0.438]\n",
      "3913 [D loss: (0.348)(R 0.296, F 0.399)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.364] [G acc: 0.250]\n",
      "3914 [D loss: (0.397)(R 0.271, F 0.523)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.532] [G acc: 0.250]\n",
      "3915 [D loss: (0.376)(R 0.267, F 0.485)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.130] [G acc: 0.375]\n",
      "3916 [D loss: (0.415)(R 0.351, F 0.480)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.200] [G acc: 0.250]\n",
      "3917 [D loss: (0.276)(R 0.261, F 0.291)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.146] [G acc: 0.312]\n",
      "3918 [D loss: (0.675)(R 0.606, F 0.744)] [D acc: (0.625)(0.812, 0.438)] [G loss: 2.717] [G acc: 0.250]\n",
      "3919 [D loss: (0.363)(R 0.357, F 0.369)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.496] [G acc: 0.312]\n",
      "3920 [D loss: (0.328)(R 0.361, F 0.295)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.820] [G acc: 0.250]\n",
      "3921 [D loss: (0.368)(R 0.504, F 0.232)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.577] [G acc: 0.062]\n",
      "3922 [D loss: (0.395)(R 0.433, F 0.357)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.438] [G acc: 0.375]\n",
      "3923 [D loss: (0.411)(R 0.491, F 0.331)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.396] [G acc: 0.438]\n",
      "3924 [D loss: (0.366)(R 0.280, F 0.451)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.525] [G acc: 0.375]\n",
      "3925 [D loss: (0.361)(R 0.477, F 0.244)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.623] [G acc: 0.312]\n",
      "3926 [D loss: (0.348)(R 0.270, F 0.427)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.956] [G acc: 0.312]\n",
      "3927 [D loss: (0.396)(R 0.323, F 0.470)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.462] [G acc: 0.188]\n",
      "3928 [D loss: (0.656)(R 0.390, F 0.921)] [D acc: (0.594)(0.875, 0.312)] [G loss: 2.710] [G acc: 0.250]\n",
      "3929 [D loss: (0.963)(R 1.730, F 0.196)] [D acc: (0.812)(0.688, 0.938)] [G loss: 1.499] [G acc: 0.500]\n",
      "3930 [D loss: (0.435)(R 0.499, F 0.370)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.173] [G acc: 0.188]\n",
      "3931 [D loss: (0.512)(R 0.377, F 0.648)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.177] [G acc: 0.188]\n",
      "3932 [D loss: (0.310)(R 0.266, F 0.354)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.329] [G acc: 0.375]\n",
      "3933 [D loss: (0.477)(R 0.433, F 0.521)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.983] [G acc: 0.375]\n",
      "3934 [D loss: (0.306)(R 0.264, F 0.348)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.716] [G acc: 0.250]\n",
      "3935 [D loss: (0.375)(R 0.278, F 0.472)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.535] [G acc: 0.250]\n",
      "3936 [D loss: (0.248)(R 0.282, F 0.214)] [D acc: (0.969)(1.000, 0.938)] [G loss: 1.990] [G acc: 0.375]\n",
      "3937 [D loss: (0.459)(R 0.464, F 0.453)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.467] [G acc: 0.312]\n",
      "3938 [D loss: (0.316)(R 0.262, F 0.370)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.940] [G acc: 0.250]\n",
      "3939 [D loss: (0.493)(R 0.492, F 0.494)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.086] [G acc: 0.250]\n",
      "3940 [D loss: (0.415)(R 0.329, F 0.502)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.758] [G acc: 0.250]\n",
      "3941 [D loss: (0.206)(R 0.268, F 0.144)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.372] [G acc: 0.312]\n",
      "3942 [D loss: (0.543)(R 0.731, F 0.354)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.211] [G acc: 0.312]\n",
      "3943 [D loss: (0.639)(R 0.448, F 0.830)] [D acc: (0.656)(0.812, 0.500)] [G loss: 2.087] [G acc: 0.375]\n",
      "3944 [D loss: (0.350)(R 0.272, F 0.428)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.729] [G acc: 0.250]\n",
      "3945 [D loss: (0.488)(R 0.266, F 0.710)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.132] [G acc: 0.312]\n",
      "3946 [D loss: (0.426)(R 0.364, F 0.488)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.565] [G acc: 0.125]\n",
      "3947 [D loss: (0.227)(R 0.286, F 0.167)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.125] [G acc: 0.312]\n",
      "3948 [D loss: (0.209)(R 0.217, F 0.202)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.254] [G acc: 0.250]\n",
      "3949 [D loss: (0.471)(R 0.672, F 0.270)] [D acc: (0.844)(0.875, 0.812)] [G loss: 4.771] [G acc: 0.125]\n",
      "3950 [D loss: (0.206)(R 0.238, F 0.174)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.513] [G acc: 0.250]\n",
      "3951 [D loss: (0.393)(R 0.587, F 0.199)] [D acc: (0.875)(0.875, 0.875)] [G loss: 4.585] [G acc: 0.125]\n",
      "3952 [D loss: (0.269)(R 0.295, F 0.242)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.825] [G acc: 0.312]\n",
      "3953 [D loss: (0.392)(R 0.346, F 0.437)] [D acc: (0.812)(0.875, 0.750)] [G loss: 4.387] [G acc: 0.188]\n",
      "3954 [D loss: (0.360)(R 0.507, F 0.212)] [D acc: (0.875)(0.875, 0.875)] [G loss: 3.339] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3955 [D loss: (0.227)(R 0.218, F 0.235)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.675] [G acc: 0.188]\n",
      "3956 [D loss: (0.274)(R 0.220, F 0.328)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.528] [G acc: 0.250]\n",
      "3957 [D loss: (0.590)(R 0.490, F 0.690)] [D acc: (0.750)(0.938, 0.562)] [G loss: 4.182] [G acc: 0.188]\n",
      "3958 [D loss: (0.314)(R 0.248, F 0.379)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.949] [G acc: 0.062]\n",
      "3959 [D loss: (0.351)(R 0.368, F 0.335)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.760] [G acc: 0.188]\n",
      "3960 [D loss: (0.423)(R 0.240, F 0.606)] [D acc: (0.812)(1.000, 0.625)] [G loss: 4.295] [G acc: 0.250]\n",
      "3961 [D loss: (0.361)(R 0.241, F 0.481)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.481] [G acc: 0.312]\n",
      "3962 [D loss: (0.273)(R 0.299, F 0.248)] [D acc: (0.938)(1.000, 0.875)] [G loss: 3.165] [G acc: 0.250]\n",
      "3963 [D loss: (0.503)(R 0.353, F 0.654)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.727] [G acc: 0.312]\n",
      "3964 [D loss: (0.473)(R 0.432, F 0.515)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.953] [G acc: 0.250]\n",
      "3965 [D loss: (0.628)(R 0.850, F 0.406)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.755] [G acc: 0.125]\n",
      "3966 [D loss: (0.366)(R 0.313, F 0.420)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.558] [G acc: 0.250]\n",
      "3967 [D loss: (0.318)(R 0.270, F 0.367)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.718] [G acc: 0.250]\n",
      "3968 [D loss: (0.439)(R 0.373, F 0.505)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.614] [G acc: 0.188]\n",
      "3969 [D loss: (0.585)(R 0.832, F 0.339)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.579] [G acc: 0.312]\n",
      "3970 [D loss: (0.347)(R 0.290, F 0.405)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.643] [G acc: 0.438]\n",
      "3971 [D loss: (0.347)(R 0.301, F 0.392)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.377] [G acc: 0.312]\n",
      "3972 [D loss: (0.429)(R 0.326, F 0.532)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.238] [G acc: 0.312]\n",
      "3973 [D loss: (0.415)(R 0.248, F 0.582)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.537] [G acc: 0.375]\n",
      "3974 [D loss: (0.373)(R 0.374, F 0.371)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.531] [G acc: 0.375]\n",
      "3975 [D loss: (0.341)(R 0.255, F 0.427)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.881] [G acc: 0.312]\n",
      "3976 [D loss: (0.232)(R 0.301, F 0.163)] [D acc: (0.969)(1.000, 0.938)] [G loss: 3.797] [G acc: 0.250]\n",
      "3977 [D loss: (0.312)(R 0.317, F 0.306)] [D acc: (0.875)(0.938, 0.812)] [G loss: 3.940] [G acc: 0.250]\n",
      "3978 [D loss: (0.273)(R 0.303, F 0.243)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.159] [G acc: 0.125]\n",
      "3979 [D loss: (0.311)(R 0.298, F 0.325)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.811] [G acc: 0.375]\n",
      "3980 [D loss: (0.370)(R 0.297, F 0.443)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.009] [G acc: 0.375]\n",
      "3981 [D loss: (0.370)(R 0.275, F 0.465)] [D acc: (0.875)(1.000, 0.750)] [G loss: 4.355] [G acc: 0.188]\n",
      "3982 [D loss: (0.509)(R 0.753, F 0.264)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.746] [G acc: 0.375]\n",
      "3983 [D loss: (0.644)(R 0.566, F 0.722)] [D acc: (0.656)(0.812, 0.500)] [G loss: 2.452] [G acc: 0.188]\n",
      "3984 [D loss: (0.503)(R 0.486, F 0.520)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.784] [G acc: 0.375]\n",
      "3985 [D loss: (0.433)(R 0.266, F 0.601)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.702] [G acc: 0.250]\n",
      "3986 [D loss: (0.727)(R 1.072, F 0.382)] [D acc: (0.750)(0.750, 0.750)] [G loss: 3.908] [G acc: 0.188]\n",
      "3987 [D loss: (0.430)(R 0.259, F 0.601)] [D acc: (0.781)(1.000, 0.562)] [G loss: 3.087] [G acc: 0.250]\n",
      "3988 [D loss: (0.791)(R 0.885, F 0.696)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.897] [G acc: 0.312]\n",
      "3989 [D loss: (0.440)(R 0.240, F 0.641)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.973] [G acc: 0.375]\n",
      "3990 [D loss: (0.474)(R 0.319, F 0.630)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.460] [G acc: 0.312]\n",
      "3991 [D loss: (0.439)(R 0.256, F 0.622)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.888] [G acc: 0.375]\n",
      "3992 [D loss: (0.545)(R 0.480, F 0.610)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.248] [G acc: 0.250]\n",
      "3993 [D loss: (0.409)(R 0.362, F 0.456)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.227] [G acc: 0.312]\n",
      "3994 [D loss: (0.252)(R 0.219, F 0.284)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.884] [G acc: 0.312]\n",
      "3995 [D loss: (0.487)(R 0.383, F 0.592)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.592] [G acc: 0.375]\n",
      "3996 [D loss: (0.732)(R 0.897, F 0.568)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.847] [G acc: 0.312]\n",
      "3997 [D loss: (0.387)(R 0.273, F 0.501)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.145] [G acc: 0.312]\n",
      "3998 [D loss: (0.737)(R 0.993, F 0.482)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.979] [G acc: 0.375]\n",
      "3999 [D loss: (0.412)(R 0.237, F 0.587)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.906] [G acc: 0.312]\n",
      "4000 [D loss: (0.580)(R 0.836, F 0.324)] [D acc: (0.906)(0.938, 0.875)] [G loss: 1.908] [G acc: 0.312]\n",
      "4001 [D loss: (0.915)(R 1.265, F 0.564)] [D acc: (0.656)(0.688, 0.625)] [G loss: 2.616] [G acc: 0.375]\n",
      "4002 [D loss: (0.433)(R 0.307, F 0.559)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.684] [G acc: 0.312]\n",
      "4003 [D loss: (0.364)(R 0.440, F 0.287)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.093] [G acc: 0.375]\n",
      "4004 [D loss: (0.410)(R 0.292, F 0.528)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.180] [G acc: 0.375]\n",
      "4005 [D loss: (0.593)(R 0.582, F 0.603)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.065] [G acc: 0.375]\n",
      "4006 [D loss: (0.412)(R 0.402, F 0.422)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.014] [G acc: 0.375]\n",
      "4007 [D loss: (0.761)(R 0.444, F 1.078)] [D acc: (0.594)(0.938, 0.250)] [G loss: 1.997] [G acc: 0.438]\n",
      "4008 [D loss: (0.577)(R 0.295, F 0.859)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.308] [G acc: 0.312]\n",
      "4009 [D loss: (0.387)(R 0.266, F 0.507)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.944] [G acc: 0.312]\n",
      "4010 [D loss: (0.219)(R 0.314, F 0.124)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.641] [G acc: 0.250]\n",
      "4011 [D loss: (0.795)(R 1.020, F 0.570)] [D acc: (0.719)(0.812, 0.625)] [G loss: 3.259] [G acc: 0.188]\n",
      "4012 [D loss: (0.353)(R 0.401, F 0.305)] [D acc: (0.906)(0.938, 0.875)] [G loss: 4.551] [G acc: 0.062]\n",
      "4013 [D loss: (0.328)(R 0.300, F 0.356)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.226] [G acc: 0.250]\n",
      "4014 [D loss: (0.474)(R 0.361, F 0.586)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.311] [G acc: 0.250]\n",
      "4015 [D loss: (0.605)(R 0.281, F 0.930)] [D acc: (0.688)(1.000, 0.375)] [G loss: 2.649] [G acc: 0.312]\n",
      "4016 [D loss: (0.524)(R 0.356, F 0.691)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.891] [G acc: 0.250]\n",
      "4017 [D loss: (0.412)(R 0.290, F 0.533)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.924] [G acc: 0.438]\n",
      "4018 [D loss: (0.453)(R 0.406, F 0.500)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.722] [G acc: 0.375]\n",
      "4019 [D loss: (0.334)(R 0.316, F 0.351)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.845] [G acc: 0.375]\n",
      "4020 [D loss: (0.456)(R 0.481, F 0.431)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.494] [G acc: 0.375]\n",
      "4021 [D loss: (0.466)(R 0.452, F 0.480)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.507] [G acc: 0.250]\n",
      "4022 [D loss: (0.428)(R 0.404, F 0.453)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.312] [G acc: 0.375]\n",
      "4023 [D loss: (0.410)(R 0.268, F 0.551)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.601] [G acc: 0.312]\n",
      "4024 [D loss: (0.219)(R 0.272, F 0.166)] [D acc: (0.969)(1.000, 0.938)] [G loss: 4.164] [G acc: 0.125]\n",
      "4025 [D loss: (0.339)(R 0.302, F 0.375)] [D acc: (0.844)(0.938, 0.750)] [G loss: 4.027] [G acc: 0.250]\n",
      "4026 [D loss: (0.292)(R 0.344, F 0.239)] [D acc: (0.875)(0.938, 0.812)] [G loss: 4.041] [G acc: 0.125]\n",
      "4027 [D loss: (0.269)(R 0.349, F 0.189)] [D acc: (0.938)(0.938, 0.938)] [G loss: 3.004] [G acc: 0.312]\n",
      "4028 [D loss: (0.427)(R 0.385, F 0.469)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.603] [G acc: 0.188]\n",
      "4029 [D loss: (0.390)(R 0.348, F 0.433)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.822] [G acc: 0.312]\n",
      "4030 [D loss: (0.386)(R 0.293, F 0.480)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.399] [G acc: 0.312]\n",
      "4031 [D loss: (0.554)(R 0.882, F 0.225)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.809] [G acc: 0.250]\n",
      "4032 [D loss: (0.479)(R 0.692, F 0.265)] [D acc: (0.812)(0.750, 0.875)] [G loss: 2.318] [G acc: 0.375]\n",
      "4033 [D loss: (0.509)(R 0.405, F 0.613)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.813] [G acc: 0.188]\n",
      "4034 [D loss: (0.542)(R 0.329, F 0.756)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.086] [G acc: 0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4035 [D loss: (0.519)(R 0.601, F 0.436)] [D acc: (0.750)(0.750, 0.750)] [G loss: 3.165] [G acc: 0.375]\n",
      "4036 [D loss: (0.406)(R 0.314, F 0.498)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.285] [G acc: 0.312]\n",
      "4037 [D loss: (0.326)(R 0.511, F 0.141)] [D acc: (0.875)(0.812, 0.938)] [G loss: 2.762] [G acc: 0.250]\n",
      "4038 [D loss: (0.427)(R 0.336, F 0.518)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.592] [G acc: 0.250]\n",
      "4039 [D loss: (0.570)(R 0.320, F 0.820)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.243] [G acc: 0.250]\n",
      "4040 [D loss: (0.704)(R 0.748, F 0.661)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.766] [G acc: 0.188]\n",
      "4041 [D loss: (0.468)(R 0.277, F 0.659)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.109] [G acc: 0.250]\n",
      "4042 [D loss: (0.534)(R 0.588, F 0.480)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.129] [G acc: 0.438]\n",
      "4043 [D loss: (0.316)(R 0.308, F 0.324)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.565] [G acc: 0.188]\n",
      "4044 [D loss: (0.568)(R 0.406, F 0.731)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.519] [G acc: 0.312]\n",
      "4045 [D loss: (0.409)(R 0.314, F 0.503)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.306] [G acc: 0.312]\n",
      "4046 [D loss: (0.273)(R 0.286, F 0.259)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.973] [G acc: 0.188]\n",
      "4047 [D loss: (0.619)(R 0.682, F 0.556)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.994] [G acc: 0.188]\n",
      "4048 [D loss: (0.464)(R 0.389, F 0.538)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.498] [G acc: 0.250]\n",
      "4049 [D loss: (0.340)(R 0.429, F 0.250)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.996] [G acc: 0.125]\n",
      "4050 [D loss: (0.396)(R 0.440, F 0.353)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.396] [G acc: 0.438]\n",
      "4051 [D loss: (0.689)(R 0.965, F 0.413)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.365] [G acc: 0.375]\n",
      "4052 [D loss: (0.565)(R 0.642, F 0.488)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.492] [G acc: 0.250]\n",
      "4053 [D loss: (0.531)(R 0.466, F 0.596)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.768] [G acc: 0.250]\n",
      "4054 [D loss: (0.509)(R 0.488, F 0.530)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.618] [G acc: 0.375]\n",
      "4055 [D loss: (0.424)(R 0.261, F 0.588)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.474] [G acc: 0.375]\n",
      "4056 [D loss: (0.344)(R 0.293, F 0.396)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.282] [G acc: 0.375]\n",
      "4057 [D loss: (0.395)(R 0.388, F 0.402)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.962] [G acc: 0.188]\n",
      "4058 [D loss: (0.645)(R 0.752, F 0.539)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.301] [G acc: 0.250]\n",
      "4059 [D loss: (0.584)(R 0.417, F 0.750)] [D acc: (0.656)(0.875, 0.438)] [G loss: 3.856] [G acc: 0.000]\n",
      "4060 [D loss: (0.460)(R 0.638, F 0.283)] [D acc: (0.875)(0.875, 0.875)] [G loss: 1.893] [G acc: 0.312]\n",
      "4061 [D loss: (0.627)(R 0.898, F 0.356)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.755] [G acc: 0.188]\n",
      "4062 [D loss: (0.357)(R 0.249, F 0.464)] [D acc: (0.844)(1.000, 0.688)] [G loss: 3.429] [G acc: 0.125]\n",
      "4063 [D loss: (0.537)(R 0.629, F 0.445)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.188] [G acc: 0.250]\n",
      "4064 [D loss: (0.699)(R 0.820, F 0.579)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.774] [G acc: 0.250]\n",
      "4065 [D loss: (0.719)(R 0.889, F 0.550)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.345] [G acc: 0.250]\n",
      "4066 [D loss: (0.495)(R 0.378, F 0.613)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.616] [G acc: 0.312]\n",
      "4067 [D loss: (0.382)(R 0.353, F 0.411)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.829] [G acc: 0.500]\n",
      "4068 [D loss: (0.426)(R 0.301, F 0.552)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.984] [G acc: 0.125]\n",
      "4069 [D loss: (0.521)(R 0.417, F 0.624)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.983] [G acc: 0.250]\n",
      "4070 [D loss: (0.466)(R 0.608, F 0.323)] [D acc: (0.812)(0.750, 0.875)] [G loss: 2.522] [G acc: 0.250]\n",
      "4071 [D loss: (0.372)(R 0.259, F 0.484)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.062] [G acc: 0.312]\n",
      "4072 [D loss: (0.433)(R 0.426, F 0.439)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.205] [G acc: 0.438]\n",
      "4073 [D loss: (0.426)(R 0.306, F 0.547)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.716] [G acc: 0.188]\n",
      "4074 [D loss: (0.435)(R 0.487, F 0.382)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.953] [G acc: 0.250]\n",
      "4075 [D loss: (0.427)(R 0.325, F 0.530)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.669] [G acc: 0.312]\n",
      "4076 [D loss: (0.456)(R 0.322, F 0.590)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.052] [G acc: 0.438]\n",
      "4077 [D loss: (0.464)(R 0.404, F 0.524)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.404] [G acc: 0.375]\n",
      "4078 [D loss: (0.269)(R 0.250, F 0.287)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.329] [G acc: 0.438]\n",
      "4079 [D loss: (0.877)(R 1.047, F 0.708)] [D acc: (0.625)(0.688, 0.562)] [G loss: 1.850] [G acc: 0.312]\n",
      "4080 [D loss: (0.447)(R 0.328, F 0.567)] [D acc: (0.781)(0.938, 0.625)] [G loss: 3.134] [G acc: 0.250]\n",
      "4081 [D loss: (0.358)(R 0.359, F 0.357)] [D acc: (0.875)(0.938, 0.812)] [G loss: 1.909] [G acc: 0.438]\n",
      "4082 [D loss: (0.442)(R 0.338, F 0.547)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.789] [G acc: 0.375]\n",
      "4083 [D loss: (0.499)(R 0.500, F 0.498)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.492] [G acc: 0.312]\n",
      "4084 [D loss: (0.608)(R 0.621, F 0.595)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.986] [G acc: 0.375]\n",
      "4085 [D loss: (0.478)(R 0.408, F 0.549)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.133] [G acc: 0.312]\n",
      "4086 [D loss: (0.399)(R 0.583, F 0.214)] [D acc: (0.812)(0.750, 0.875)] [G loss: 2.515] [G acc: 0.312]\n",
      "4087 [D loss: (0.553)(R 0.545, F 0.561)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.972] [G acc: 0.312]\n",
      "4088 [D loss: (0.444)(R 0.448, F 0.439)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.833] [G acc: 0.250]\n",
      "4089 [D loss: (0.524)(R 0.342, F 0.705)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.420] [G acc: 0.375]\n",
      "4090 [D loss: (0.575)(R 0.421, F 0.729)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.407] [G acc: 0.312]\n",
      "4091 [D loss: (0.444)(R 0.482, F 0.407)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.945] [G acc: 0.438]\n",
      "4092 [D loss: (0.530)(R 0.754, F 0.306)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.247] [G acc: 0.438]\n",
      "4093 [D loss: (0.451)(R 0.420, F 0.483)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.737] [G acc: 0.375]\n",
      "4094 [D loss: (0.521)(R 0.430, F 0.611)] [D acc: (0.812)(0.938, 0.688)] [G loss: 3.062] [G acc: 0.312]\n",
      "4095 [D loss: (0.473)(R 0.535, F 0.410)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.537] [G acc: 0.375]\n",
      "4096 [D loss: (0.607)(R 0.564, F 0.649)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.214] [G acc: 0.375]\n",
      "4097 [D loss: (0.434)(R 0.270, F 0.598)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.030] [G acc: 0.375]\n",
      "4098 [D loss: (0.579)(R 0.453, F 0.705)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.149] [G acc: 0.375]\n",
      "4099 [D loss: (0.543)(R 0.642, F 0.444)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.342] [G acc: 0.562]\n",
      "4100 [D loss: (0.503)(R 0.378, F 0.627)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.725] [G acc: 0.188]\n",
      "4101 [D loss: (0.553)(R 0.653, F 0.452)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.647] [G acc: 0.250]\n",
      "4102 [D loss: (0.347)(R 0.253, F 0.442)] [D acc: (0.875)(1.000, 0.750)] [G loss: 1.785] [G acc: 0.250]\n",
      "4103 [D loss: (0.468)(R 0.341, F 0.596)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.339] [G acc: 0.438]\n",
      "4104 [D loss: (0.406)(R 0.366, F 0.446)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.638] [G acc: 0.375]\n",
      "4105 [D loss: (0.389)(R 0.303, F 0.474)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.679] [G acc: 0.312]\n",
      "4106 [D loss: (0.443)(R 0.479, F 0.408)] [D acc: (0.844)(0.875, 0.812)] [G loss: 3.431] [G acc: 0.250]\n",
      "4107 [D loss: (0.422)(R 0.419, F 0.425)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.151] [G acc: 0.375]\n",
      "4108 [D loss: (0.491)(R 0.683, F 0.299)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.839] [G acc: 0.250]\n",
      "4109 [D loss: (0.346)(R 0.422, F 0.270)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.316] [G acc: 0.188]\n",
      "4110 [D loss: (0.270)(R 0.250, F 0.289)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.359] [G acc: 0.250]\n",
      "4111 [D loss: (0.448)(R 0.425, F 0.471)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.288] [G acc: 0.250]\n",
      "4112 [D loss: (0.464)(R 0.497, F 0.431)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.265] [G acc: 0.438]\n",
      "4113 [D loss: (0.503)(R 0.444, F 0.562)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.713] [G acc: 0.125]\n",
      "4114 [D loss: (0.589)(R 0.444, F 0.735)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.442] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4115 [D loss: (0.556)(R 0.524, F 0.589)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.762] [G acc: 0.438]\n",
      "4116 [D loss: (0.767)(R 0.779, F 0.756)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.804] [G acc: 0.312]\n",
      "4117 [D loss: (0.464)(R 0.312, F 0.615)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.213] [G acc: 0.312]\n",
      "4118 [D loss: (0.598)(R 0.353, F 0.844)] [D acc: (0.688)(0.938, 0.438)] [G loss: 2.296] [G acc: 0.312]\n",
      "4119 [D loss: (0.603)(R 0.439, F 0.767)] [D acc: (0.750)(0.938, 0.562)] [G loss: 3.298] [G acc: 0.188]\n",
      "4120 [D loss: (0.578)(R 0.602, F 0.554)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.237] [G acc: 0.188]\n",
      "4121 [D loss: (0.563)(R 0.680, F 0.445)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.933] [G acc: 0.250]\n",
      "4122 [D loss: (0.287)(R 0.247, F 0.326)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.467] [G acc: 0.188]\n",
      "4123 [D loss: (0.535)(R 0.515, F 0.556)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.266] [G acc: 0.438]\n",
      "4124 [D loss: (0.315)(R 0.304, F 0.325)] [D acc: (0.844)(0.938, 0.750)] [G loss: 3.395] [G acc: 0.250]\n",
      "4125 [D loss: (0.456)(R 0.346, F 0.567)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.248] [G acc: 0.312]\n",
      "4126 [D loss: (0.649)(R 1.076, F 0.222)] [D acc: (0.781)(0.688, 0.875)] [G loss: 2.492] [G acc: 0.188]\n",
      "4127 [D loss: (0.430)(R 0.410, F 0.450)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.403] [G acc: 0.250]\n",
      "4128 [D loss: (0.418)(R 0.289, F 0.548)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.655] [G acc: 0.312]\n",
      "4129 [D loss: (0.549)(R 0.610, F 0.488)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.392] [G acc: 0.438]\n",
      "4130 [D loss: (0.436)(R 0.427, F 0.444)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.272] [G acc: 0.312]\n",
      "4131 [D loss: (0.438)(R 0.378, F 0.499)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.227] [G acc: 0.375]\n",
      "4132 [D loss: (0.490)(R 0.366, F 0.614)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.700] [G acc: 0.438]\n",
      "4133 [D loss: (0.474)(R 0.399, F 0.548)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.638] [G acc: 0.375]\n",
      "4134 [D loss: (0.448)(R 0.518, F 0.377)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.830] [G acc: 0.312]\n",
      "4135 [D loss: (0.528)(R 0.709, F 0.346)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.317] [G acc: 0.250]\n",
      "4136 [D loss: (0.629)(R 0.415, F 0.843)] [D acc: (0.656)(0.938, 0.375)] [G loss: 2.391] [G acc: 0.250]\n",
      "4137 [D loss: (0.734)(R 0.990, F 0.478)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.868] [G acc: 0.250]\n",
      "4138 [D loss: (0.396)(R 0.279, F 0.512)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.021] [G acc: 0.312]\n",
      "4139 [D loss: (0.769)(R 0.935, F 0.603)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.296] [G acc: 0.438]\n",
      "4140 [D loss: (0.539)(R 0.547, F 0.531)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.987] [G acc: 0.438]\n",
      "4141 [D loss: (0.654)(R 0.618, F 0.691)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.706] [G acc: 0.312]\n",
      "4142 [D loss: (0.550)(R 0.358, F 0.741)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.692] [G acc: 0.375]\n",
      "4143 [D loss: (0.521)(R 0.551, F 0.492)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.353] [G acc: 0.438]\n",
      "4144 [D loss: (0.645)(R 0.486, F 0.805)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.654] [G acc: 0.312]\n",
      "4145 [D loss: (0.287)(R 0.287, F 0.286)] [D acc: (0.938)(1.000, 0.875)] [G loss: 1.790] [G acc: 0.375]\n",
      "4146 [D loss: (0.546)(R 0.302, F 0.791)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.485] [G acc: 0.438]\n",
      "4147 [D loss: (0.512)(R 0.285, F 0.739)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.739] [G acc: 0.188]\n",
      "4148 [D loss: (0.601)(R 0.639, F 0.562)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.113] [G acc: 0.312]\n",
      "4149 [D loss: (0.605)(R 0.660, F 0.551)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.332] [G acc: 0.250]\n",
      "4150 [D loss: (0.342)(R 0.309, F 0.374)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.311] [G acc: 0.250]\n",
      "4151 [D loss: (0.540)(R 0.385, F 0.695)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.627] [G acc: 0.375]\n",
      "4152 [D loss: (0.670)(R 0.736, F 0.605)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.399] [G acc: 0.562]\n",
      "4153 [D loss: (0.444)(R 0.278, F 0.609)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.401] [G acc: 0.188]\n",
      "4154 [D loss: (0.487)(R 0.468, F 0.507)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.791] [G acc: 0.312]\n",
      "4155 [D loss: (0.532)(R 0.495, F 0.570)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.347] [G acc: 0.312]\n",
      "4156 [D loss: (0.476)(R 0.406, F 0.546)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.511] [G acc: 0.312]\n",
      "4157 [D loss: (0.497)(R 0.508, F 0.485)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.689] [G acc: 0.312]\n",
      "4158 [D loss: (0.606)(R 0.665, F 0.548)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.548] [G acc: 0.438]\n",
      "4159 [D loss: (0.509)(R 0.760, F 0.258)] [D acc: (0.812)(0.750, 0.875)] [G loss: 2.056] [G acc: 0.375]\n",
      "4160 [D loss: (0.641)(R 0.538, F 0.745)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.622] [G acc: 0.312]\n",
      "4161 [D loss: (0.487)(R 0.374, F 0.600)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.702] [G acc: 0.312]\n",
      "4162 [D loss: (0.467)(R 0.327, F 0.608)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.381] [G acc: 0.312]\n",
      "4163 [D loss: (0.396)(R 0.285, F 0.508)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.115] [G acc: 0.438]\n",
      "4164 [D loss: (0.673)(R 0.603, F 0.744)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.283] [G acc: 0.312]\n",
      "4165 [D loss: (0.443)(R 0.353, F 0.533)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.742] [G acc: 0.438]\n",
      "4166 [D loss: (0.702)(R 0.830, F 0.574)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.788] [G acc: 0.375]\n",
      "4167 [D loss: (0.568)(R 0.333, F 0.803)] [D acc: (0.750)(1.000, 0.500)] [G loss: 1.941] [G acc: 0.438]\n",
      "4168 [D loss: (0.449)(R 0.430, F 0.467)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.277] [G acc: 0.188]\n",
      "4169 [D loss: (0.421)(R 0.298, F 0.544)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.337] [G acc: 0.312]\n",
      "4170 [D loss: (0.284)(R 0.421, F 0.147)] [D acc: (0.938)(0.938, 0.938)] [G loss: 1.948] [G acc: 0.500]\n",
      "4171 [D loss: (0.696)(R 0.845, F 0.547)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.561] [G acc: 0.188]\n",
      "4172 [D loss: (0.552)(R 0.737, F 0.366)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.743] [G acc: 0.250]\n",
      "4173 [D loss: (0.509)(R 0.315, F 0.703)] [D acc: (0.719)(0.938, 0.500)] [G loss: 3.886] [G acc: 0.125]\n",
      "4174 [D loss: (0.467)(R 0.432, F 0.503)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.388] [G acc: 0.250]\n",
      "4175 [D loss: (0.351)(R 0.286, F 0.417)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.192] [G acc: 0.250]\n",
      "4176 [D loss: (0.663)(R 0.789, F 0.538)] [D acc: (0.656)(0.688, 0.625)] [G loss: 2.018] [G acc: 0.312]\n",
      "4177 [D loss: (0.242)(R 0.308, F 0.175)] [D acc: (0.969)(1.000, 0.938)] [G loss: 2.217] [G acc: 0.250]\n",
      "4178 [D loss: (0.358)(R 0.356, F 0.360)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.586] [G acc: 0.188]\n",
      "4179 [D loss: (0.521)(R 0.606, F 0.435)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.150] [G acc: 0.438]\n",
      "4180 [D loss: (0.511)(R 0.472, F 0.550)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.953] [G acc: 0.438]\n",
      "4181 [D loss: (0.461)(R 0.399, F 0.522)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.631] [G acc: 0.375]\n",
      "4182 [D loss: (0.353)(R 0.299, F 0.408)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.527] [G acc: 0.375]\n",
      "4183 [D loss: (0.457)(R 0.514, F 0.399)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.468] [G acc: 0.438]\n",
      "4184 [D loss: (0.415)(R 0.448, F 0.381)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.174] [G acc: 0.375]\n",
      "4185 [D loss: (0.331)(R 0.303, F 0.359)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.822] [G acc: 0.250]\n",
      "4186 [D loss: (0.661)(R 0.738, F 0.585)] [D acc: (0.688)(0.750, 0.625)] [G loss: 2.353] [G acc: 0.250]\n",
      "4187 [D loss: (0.546)(R 0.755, F 0.338)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.958] [G acc: 0.500]\n",
      "4188 [D loss: (0.390)(R 0.276, F 0.503)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.339] [G acc: 0.250]\n",
      "4189 [D loss: (0.394)(R 0.453, F 0.335)] [D acc: (0.875)(0.938, 0.812)] [G loss: 1.990] [G acc: 0.250]\n",
      "4190 [D loss: (0.369)(R 0.521, F 0.217)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.837] [G acc: 0.250]\n",
      "4191 [D loss: (0.549)(R 0.502, F 0.595)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.225] [G acc: 0.312]\n",
      "4192 [D loss: (0.314)(R 0.303, F 0.325)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.980] [G acc: 0.062]\n",
      "4193 [D loss: (0.371)(R 0.350, F 0.392)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.105] [G acc: 0.312]\n",
      "4194 [D loss: (0.369)(R 0.332, F 0.407)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.683] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4195 [D loss: (0.535)(R 0.551, F 0.520)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.244] [G acc: 0.250]\n",
      "4196 [D loss: (0.332)(R 0.387, F 0.277)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.942] [G acc: 0.188]\n",
      "4197 [D loss: (0.731)(R 0.897, F 0.564)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.302] [G acc: 0.250]\n",
      "4198 [D loss: (0.532)(R 0.531, F 0.533)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.981] [G acc: 0.375]\n",
      "4199 [D loss: (0.330)(R 0.285, F 0.375)] [D acc: (0.906)(1.000, 0.812)] [G loss: 1.870] [G acc: 0.375]\n",
      "4200 [D loss: (0.398)(R 0.316, F 0.481)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.981] [G acc: 0.438]\n",
      "4201 [D loss: (0.565)(R 0.414, F 0.716)] [D acc: (0.625)(0.875, 0.375)] [G loss: 2.149] [G acc: 0.250]\n",
      "4202 [D loss: (0.519)(R 0.471, F 0.567)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.855] [G acc: 0.438]\n",
      "4203 [D loss: (0.465)(R 0.336, F 0.593)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.223] [G acc: 0.375]\n",
      "4204 [D loss: (0.410)(R 0.284, F 0.536)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.646] [G acc: 0.312]\n",
      "4205 [D loss: (0.628)(R 0.666, F 0.590)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.923] [G acc: 0.312]\n",
      "4206 [D loss: (0.538)(R 0.523, F 0.554)] [D acc: (0.688)(0.750, 0.625)] [G loss: 2.736] [G acc: 0.250]\n",
      "4207 [D loss: (0.431)(R 0.346, F 0.515)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.349] [G acc: 0.312]\n",
      "4208 [D loss: (0.446)(R 0.308, F 0.584)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.966] [G acc: 0.188]\n",
      "4209 [D loss: (0.410)(R 0.437, F 0.384)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.659] [G acc: 0.312]\n",
      "4210 [D loss: (0.728)(R 1.157, F 0.298)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.830] [G acc: 0.250]\n",
      "4211 [D loss: (0.685)(R 0.590, F 0.780)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.486] [G acc: 0.250]\n",
      "4212 [D loss: (0.560)(R 0.506, F 0.614)] [D acc: (0.656)(0.750, 0.562)] [G loss: 2.591] [G acc: 0.312]\n",
      "4213 [D loss: (0.607)(R 0.589, F 0.626)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.751] [G acc: 0.312]\n",
      "4214 [D loss: (0.466)(R 0.485, F 0.447)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.874] [G acc: 0.188]\n",
      "4215 [D loss: (0.236)(R 0.348, F 0.123)] [D acc: (0.969)(0.938, 1.000)] [G loss: 3.007] [G acc: 0.312]\n",
      "4216 [D loss: (0.445)(R 0.596, F 0.294)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.736] [G acc: 0.500]\n",
      "4217 [D loss: (0.389)(R 0.284, F 0.495)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.562] [G acc: 0.250]\n",
      "4218 [D loss: (0.346)(R 0.436, F 0.256)] [D acc: (0.906)(0.938, 0.875)] [G loss: 1.815] [G acc: 0.312]\n",
      "4219 [D loss: (0.709)(R 0.770, F 0.648)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.299] [G acc: 0.312]\n",
      "4220 [D loss: (0.385)(R 0.328, F 0.442)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.651] [G acc: 0.312]\n",
      "4221 [D loss: (0.502)(R 0.458, F 0.546)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.402] [G acc: 0.375]\n",
      "4222 [D loss: (0.469)(R 0.341, F 0.597)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.868] [G acc: 0.250]\n",
      "4223 [D loss: (0.369)(R 0.369, F 0.368)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.848] [G acc: 0.438]\n",
      "4224 [D loss: (0.592)(R 0.873, F 0.310)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.527] [G acc: 0.188]\n",
      "4225 [D loss: (0.500)(R 0.669, F 0.332)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.694] [G acc: 0.438]\n",
      "4226 [D loss: (0.639)(R 0.822, F 0.456)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.321] [G acc: 0.312]\n",
      "4227 [D loss: (0.345)(R 0.367, F 0.324)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.828] [G acc: 0.125]\n",
      "4228 [D loss: (0.638)(R 0.563, F 0.714)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.280] [G acc: 0.375]\n",
      "4229 [D loss: (0.531)(R 0.617, F 0.444)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.857] [G acc: 0.438]\n",
      "4230 [D loss: (0.414)(R 0.364, F 0.464)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.291] [G acc: 0.312]\n",
      "4231 [D loss: (0.460)(R 0.431, F 0.490)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.202] [G acc: 0.312]\n",
      "4232 [D loss: (0.596)(R 0.378, F 0.813)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.133] [G acc: 0.250]\n",
      "4233 [D loss: (0.409)(R 0.330, F 0.488)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.147] [G acc: 0.312]\n",
      "4234 [D loss: (0.498)(R 0.377, F 0.618)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.458] [G acc: 0.250]\n",
      "4235 [D loss: (0.443)(R 0.541, F 0.345)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.936] [G acc: 0.250]\n",
      "4236 [D loss: (0.349)(R 0.276, F 0.422)] [D acc: (0.875)(1.000, 0.750)] [G loss: 3.133] [G acc: 0.250]\n",
      "4237 [D loss: (0.375)(R 0.275, F 0.475)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.510] [G acc: 0.250]\n",
      "4238 [D loss: (0.393)(R 0.289, F 0.496)] [D acc: (0.812)(1.000, 0.625)] [G loss: 3.513] [G acc: 0.375]\n",
      "4239 [D loss: (0.346)(R 0.325, F 0.367)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.898] [G acc: 0.250]\n",
      "4240 [D loss: (0.434)(R 0.526, F 0.342)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.875] [G acc: 0.250]\n",
      "4241 [D loss: (0.445)(R 0.310, F 0.580)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.369] [G acc: 0.438]\n",
      "4242 [D loss: (0.347)(R 0.333, F 0.360)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.724] [G acc: 0.250]\n",
      "4243 [D loss: (0.301)(R 0.307, F 0.295)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.448] [G acc: 0.438]\n",
      "4244 [D loss: (0.494)(R 0.566, F 0.421)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.568] [G acc: 0.250]\n",
      "4245 [D loss: (0.475)(R 0.469, F 0.480)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.914] [G acc: 0.375]\n",
      "4246 [D loss: (0.437)(R 0.403, F 0.470)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.763] [G acc: 0.375]\n",
      "4247 [D loss: (0.504)(R 0.495, F 0.514)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.589] [G acc: 0.250]\n",
      "4248 [D loss: (0.449)(R 0.317, F 0.581)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.876] [G acc: 0.250]\n",
      "4249 [D loss: (0.397)(R 0.458, F 0.336)] [D acc: (0.812)(0.812, 0.812)] [G loss: 3.300] [G acc: 0.250]\n",
      "4250 [D loss: (0.336)(R 0.442, F 0.230)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.178] [G acc: 0.250]\n",
      "4251 [D loss: (0.621)(R 0.729, F 0.513)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.865] [G acc: 0.250]\n",
      "4252 [D loss: (0.285)(R 0.308, F 0.261)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.632] [G acc: 0.250]\n",
      "4253 [D loss: (0.415)(R 0.466, F 0.364)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.917] [G acc: 0.125]\n",
      "4254 [D loss: (0.739)(R 0.842, F 0.636)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.550] [G acc: 0.375]\n",
      "4255 [D loss: (0.435)(R 0.347, F 0.522)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.030] [G acc: 0.250]\n",
      "4256 [D loss: (0.578)(R 0.384, F 0.772)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.450] [G acc: 0.250]\n",
      "4257 [D loss: (0.657)(R 0.694, F 0.620)] [D acc: (0.625)(0.688, 0.562)] [G loss: 1.922] [G acc: 0.375]\n",
      "4258 [D loss: (0.375)(R 0.294, F 0.456)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.211] [G acc: 0.250]\n",
      "4259 [D loss: (0.568)(R 0.583, F 0.553)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.073] [G acc: 0.438]\n",
      "4260 [D loss: (0.324)(R 0.370, F 0.278)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.524] [G acc: 0.312]\n",
      "4261 [D loss: (0.412)(R 0.335, F 0.490)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.579] [G acc: 0.375]\n",
      "4262 [D loss: (0.471)(R 0.392, F 0.550)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.984] [G acc: 0.438]\n",
      "4263 [D loss: (0.306)(R 0.344, F 0.268)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.332] [G acc: 0.250]\n",
      "4264 [D loss: (0.440)(R 0.345, F 0.535)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.831] [G acc: 0.500]\n",
      "4265 [D loss: (0.414)(R 0.273, F 0.554)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.566] [G acc: 0.375]\n",
      "4266 [D loss: (0.517)(R 0.265, F 0.768)] [D acc: (0.750)(1.000, 0.500)] [G loss: 2.572] [G acc: 0.312]\n",
      "4267 [D loss: (0.596)(R 0.442, F 0.750)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.973] [G acc: 0.188]\n",
      "4268 [D loss: (0.222)(R 0.292, F 0.153)] [D acc: (1.000)(1.000, 1.000)] [G loss: 2.962] [G acc: 0.125]\n",
      "4269 [D loss: (0.474)(R 0.628, F 0.319)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.474] [G acc: 0.188]\n",
      "4270 [D loss: (0.441)(R 0.418, F 0.464)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.458] [G acc: 0.188]\n",
      "4271 [D loss: (0.406)(R 0.385, F 0.428)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.727] [G acc: 0.062]\n",
      "4272 [D loss: (0.650)(R 0.637, F 0.664)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.314] [G acc: 0.250]\n",
      "4273 [D loss: (0.449)(R 0.431, F 0.467)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.704] [G acc: 0.062]\n",
      "4274 [D loss: (0.600)(R 0.727, F 0.474)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.654] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4275 [D loss: (0.468)(R 0.335, F 0.602)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.721] [G acc: 0.438]\n",
      "4276 [D loss: (0.495)(R 0.390, F 0.600)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.459] [G acc: 0.250]\n",
      "4277 [D loss: (0.344)(R 0.485, F 0.202)] [D acc: (0.844)(0.812, 0.875)] [G loss: 1.989] [G acc: 0.500]\n",
      "4278 [D loss: (0.646)(R 0.974, F 0.318)] [D acc: (0.781)(0.750, 0.812)] [G loss: 2.272] [G acc: 0.250]\n",
      "4279 [D loss: (0.588)(R 0.416, F 0.759)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.878] [G acc: 0.500]\n",
      "4280 [D loss: (0.684)(R 1.145, F 0.223)] [D acc: (0.844)(0.750, 0.938)] [G loss: 1.441] [G acc: 0.438]\n",
      "4281 [D loss: (0.317)(R 0.282, F 0.352)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.427] [G acc: 0.250]\n",
      "4282 [D loss: (0.339)(R 0.359, F 0.318)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.133] [G acc: 0.188]\n",
      "4283 [D loss: (0.595)(R 0.743, F 0.448)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.418] [G acc: 0.250]\n",
      "4284 [D loss: (0.288)(R 0.322, F 0.254)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.970] [G acc: 0.375]\n",
      "4285 [D loss: (0.523)(R 0.439, F 0.607)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.720] [G acc: 0.188]\n",
      "4286 [D loss: (0.447)(R 0.392, F 0.502)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.198] [G acc: 0.562]\n",
      "4287 [D loss: (0.552)(R 0.780, F 0.323)] [D acc: (0.812)(0.750, 0.875)] [G loss: 2.625] [G acc: 0.250]\n",
      "4288 [D loss: (0.662)(R 0.705, F 0.619)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.686] [G acc: 0.250]\n",
      "4289 [D loss: (0.367)(R 0.323, F 0.411)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.313] [G acc: 0.188]\n",
      "4290 [D loss: (0.437)(R 0.342, F 0.532)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.755] [G acc: 0.375]\n",
      "4291 [D loss: (0.468)(R 0.295, F 0.641)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.932] [G acc: 0.438]\n",
      "4292 [D loss: (0.542)(R 0.375, F 0.710)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.197] [G acc: 0.250]\n",
      "4293 [D loss: (0.436)(R 0.445, F 0.426)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.915] [G acc: 0.188]\n",
      "4294 [D loss: (0.425)(R 0.375, F 0.474)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.159] [G acc: 0.312]\n",
      "4295 [D loss: (0.602)(R 0.716, F 0.488)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.927] [G acc: 0.375]\n",
      "4296 [D loss: (0.552)(R 0.464, F 0.641)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.670] [G acc: 0.312]\n",
      "4297 [D loss: (0.319)(R 0.265, F 0.373)] [D acc: (0.906)(1.000, 0.812)] [G loss: 3.330] [G acc: 0.250]\n",
      "4298 [D loss: (0.471)(R 0.424, F 0.518)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.463] [G acc: 0.250]\n",
      "4299 [D loss: (0.369)(R 0.418, F 0.321)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.111] [G acc: 0.312]\n",
      "4300 [D loss: (0.327)(R 0.407, F 0.246)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.858] [G acc: 0.188]\n",
      "4301 [D loss: (0.520)(R 0.510, F 0.529)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.884] [G acc: 0.312]\n",
      "4302 [D loss: (0.628)(R 0.592, F 0.664)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.618] [G acc: 0.312]\n",
      "4303 [D loss: (0.269)(R 0.315, F 0.222)] [D acc: (0.969)(1.000, 0.938)] [G loss: 2.899] [G acc: 0.375]\n",
      "4304 [D loss: (0.344)(R 0.367, F 0.321)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.188] [G acc: 0.312]\n",
      "4305 [D loss: (0.447)(R 0.331, F 0.563)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.408] [G acc: 0.250]\n",
      "4306 [D loss: (0.344)(R 0.434, F 0.253)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.860] [G acc: 0.312]\n",
      "4307 [D loss: (0.537)(R 0.333, F 0.741)] [D acc: (0.656)(0.938, 0.375)] [G loss: 2.421] [G acc: 0.312]\n",
      "4308 [D loss: (0.611)(R 0.686, F 0.535)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.393] [G acc: 0.250]\n",
      "4309 [D loss: (0.476)(R 0.429, F 0.523)] [D acc: (0.781)(0.875, 0.688)] [G loss: 3.201] [G acc: 0.438]\n",
      "4310 [D loss: (0.340)(R 0.286, F 0.394)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.412] [G acc: 0.312]\n",
      "4311 [D loss: (0.351)(R 0.414, F 0.287)] [D acc: (0.906)(0.938, 0.875)] [G loss: 3.285] [G acc: 0.312]\n",
      "4312 [D loss: (0.393)(R 0.342, F 0.444)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.783] [G acc: 0.438]\n",
      "4313 [D loss: (0.397)(R 0.282, F 0.512)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.573] [G acc: 0.438]\n",
      "4314 [D loss: (0.753)(R 0.857, F 0.649)] [D acc: (0.625)(0.625, 0.625)] [G loss: 2.871] [G acc: 0.250]\n",
      "4315 [D loss: (0.555)(R 0.304, F 0.807)] [D acc: (0.688)(1.000, 0.375)] [G loss: 2.881] [G acc: 0.438]\n",
      "4316 [D loss: (0.466)(R 0.591, F 0.342)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.042] [G acc: 0.375]\n",
      "4317 [D loss: (0.377)(R 0.310, F 0.444)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.742] [G acc: 0.438]\n",
      "4318 [D loss: (0.376)(R 0.444, F 0.307)] [D acc: (0.875)(0.875, 0.875)] [G loss: 2.652] [G acc: 0.312]\n",
      "4319 [D loss: (0.400)(R 0.249, F 0.552)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.737] [G acc: 0.375]\n",
      "4320 [D loss: (0.522)(R 0.706, F 0.339)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.615] [G acc: 0.250]\n",
      "4321 [D loss: (0.641)(R 0.621, F 0.662)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.853] [G acc: 0.312]\n",
      "4322 [D loss: (0.618)(R 0.384, F 0.852)] [D acc: (0.688)(0.938, 0.438)] [G loss: 2.825] [G acc: 0.188]\n",
      "4323 [D loss: (0.331)(R 0.325, F 0.338)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.796] [G acc: 0.125]\n",
      "4324 [D loss: (0.338)(R 0.330, F 0.346)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.071] [G acc: 0.312]\n",
      "4325 [D loss: (0.256)(R 0.287, F 0.226)] [D acc: (0.938)(1.000, 0.875)] [G loss: 4.122] [G acc: 0.062]\n",
      "4326 [D loss: (0.413)(R 0.416, F 0.410)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.324] [G acc: 0.312]\n",
      "4327 [D loss: (0.323)(R 0.404, F 0.243)] [D acc: (0.906)(0.875, 0.938)] [G loss: 2.672] [G acc: 0.250]\n",
      "4328 [D loss: (0.491)(R 0.296, F 0.686)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.196] [G acc: 0.438]\n",
      "4329 [D loss: (0.663)(R 0.618, F 0.707)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.303] [G acc: 0.250]\n",
      "4330 [D loss: (0.802)(R 0.986, F 0.618)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.374] [G acc: 0.375]\n",
      "4331 [D loss: (0.550)(R 0.476, F 0.625)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.013] [G acc: 0.375]\n",
      "4332 [D loss: (0.481)(R 0.553, F 0.409)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.657] [G acc: 0.375]\n",
      "4333 [D loss: (0.470)(R 0.308, F 0.631)] [D acc: (0.750)(1.000, 0.500)] [G loss: 2.132] [G acc: 0.375]\n",
      "4334 [D loss: (0.480)(R 0.477, F 0.484)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.338] [G acc: 0.250]\n",
      "4335 [D loss: (0.346)(R 0.396, F 0.295)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.108] [G acc: 0.250]\n",
      "4336 [D loss: (0.632)(R 0.338, F 0.927)] [D acc: (0.688)(0.938, 0.438)] [G loss: 2.912] [G acc: 0.250]\n",
      "4337 [D loss: (0.391)(R 0.405, F 0.377)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.890] [G acc: 0.250]\n",
      "4338 [D loss: (0.687)(R 0.554, F 0.820)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.364] [G acc: 0.312]\n",
      "4339 [D loss: (0.396)(R 0.357, F 0.435)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.560] [G acc: 0.312]\n",
      "4340 [D loss: (0.671)(R 0.620, F 0.722)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.050] [G acc: 0.312]\n",
      "4341 [D loss: (0.764)(R 0.807, F 0.721)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.580] [G acc: 0.500]\n",
      "4342 [D loss: (0.524)(R 0.398, F 0.650)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.813] [G acc: 0.250]\n",
      "4343 [D loss: (0.625)(R 0.691, F 0.558)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.595] [G acc: 0.125]\n",
      "4344 [D loss: (0.380)(R 0.439, F 0.321)] [D acc: (0.844)(0.875, 0.812)] [G loss: 2.615] [G acc: 0.250]\n",
      "4345 [D loss: (0.551)(R 0.718, F 0.385)] [D acc: (0.781)(0.812, 0.750)] [G loss: 2.165] [G acc: 0.375]\n",
      "4346 [D loss: (0.286)(R 0.276, F 0.297)] [D acc: (0.938)(1.000, 0.875)] [G loss: 2.227] [G acc: 0.250]\n",
      "4347 [D loss: (0.361)(R 0.289, F 0.432)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.223] [G acc: 0.250]\n",
      "4348 [D loss: (0.577)(R 0.393, F 0.762)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.469] [G acc: 0.500]\n",
      "4349 [D loss: (0.767)(R 0.341, F 1.193)] [D acc: (0.625)(0.938, 0.312)] [G loss: 2.076] [G acc: 0.250]\n",
      "4350 [D loss: (0.502)(R 0.438, F 0.566)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.001] [G acc: 0.500]\n",
      "4351 [D loss: (0.580)(R 0.659, F 0.501)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.816] [G acc: 0.438]\n",
      "4352 [D loss: (0.406)(R 0.332, F 0.481)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.136] [G acc: 0.250]\n",
      "4353 [D loss: (0.523)(R 0.408, F 0.637)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.172] [G acc: 0.375]\n",
      "4354 [D loss: (0.382)(R 0.425, F 0.339)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.136] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4355 [D loss: (0.446)(R 0.247, F 0.646)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.822] [G acc: 0.375]\n",
      "4356 [D loss: (0.494)(R 0.565, F 0.423)] [D acc: (0.688)(0.750, 0.625)] [G loss: 2.013] [G acc: 0.312]\n",
      "4357 [D loss: (0.405)(R 0.446, F 0.365)] [D acc: (0.812)(0.812, 0.812)] [G loss: 2.839] [G acc: 0.312]\n",
      "4358 [D loss: (0.298)(R 0.388, F 0.209)] [D acc: (0.938)(0.938, 0.938)] [G loss: 2.407] [G acc: 0.375]\n",
      "4359 [D loss: (0.667)(R 0.975, F 0.359)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.300] [G acc: 0.375]\n",
      "4360 [D loss: (0.462)(R 0.366, F 0.558)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.895] [G acc: 0.312]\n",
      "4361 [D loss: (0.752)(R 0.837, F 0.667)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.855] [G acc: 0.312]\n",
      "4362 [D loss: (0.464)(R 0.432, F 0.497)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.565] [G acc: 0.250]\n",
      "4363 [D loss: (0.726)(R 0.859, F 0.593)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.943] [G acc: 0.500]\n",
      "4364 [D loss: (0.620)(R 0.458, F 0.783)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.906] [G acc: 0.438]\n",
      "4365 [D loss: (0.830)(R 1.296, F 0.365)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.716] [G acc: 0.375]\n",
      "4366 [D loss: (0.437)(R 0.387, F 0.488)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.212] [G acc: 0.250]\n",
      "4367 [D loss: (0.365)(R 0.339, F 0.391)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.342] [G acc: 0.375]\n",
      "4368 [D loss: (0.432)(R 0.366, F 0.498)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.748] [G acc: 0.438]\n",
      "4369 [D loss: (0.546)(R 0.394, F 0.697)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.840] [G acc: 0.312]\n",
      "4370 [D loss: (0.414)(R 0.291, F 0.538)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.763] [G acc: 0.188]\n",
      "4371 [D loss: (0.662)(R 0.759, F 0.565)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.586] [G acc: 0.375]\n",
      "4372 [D loss: (0.323)(R 0.287, F 0.359)] [D acc: (0.875)(1.000, 0.750)] [G loss: 1.684] [G acc: 0.438]\n",
      "4373 [D loss: (0.372)(R 0.377, F 0.367)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.099] [G acc: 0.375]\n",
      "4374 [D loss: (0.587)(R 0.575, F 0.599)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.827] [G acc: 0.375]\n",
      "4375 [D loss: (0.449)(R 0.300, F 0.598)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.052] [G acc: 0.375]\n",
      "4376 [D loss: (0.487)(R 0.476, F 0.498)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.744] [G acc: 0.375]\n",
      "4377 [D loss: (0.513)(R 0.430, F 0.596)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.175] [G acc: 0.312]\n",
      "4378 [D loss: (0.429)(R 0.524, F 0.333)] [D acc: (0.844)(0.812, 0.875)] [G loss: 2.246] [G acc: 0.312]\n",
      "4379 [D loss: (0.379)(R 0.280, F 0.479)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.929] [G acc: 0.250]\n",
      "4380 [D loss: (0.489)(R 0.534, F 0.445)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.599] [G acc: 0.500]\n",
      "4381 [D loss: (0.443)(R 0.262, F 0.624)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.207] [G acc: 0.375]\n",
      "4382 [D loss: (0.421)(R 0.705, F 0.138)] [D acc: (0.906)(0.812, 1.000)] [G loss: 2.112] [G acc: 0.438]\n",
      "4383 [D loss: (0.383)(R 0.287, F 0.478)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.397] [G acc: 0.125]\n",
      "4384 [D loss: (0.432)(R 0.294, F 0.570)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.176] [G acc: 0.438]\n",
      "4385 [D loss: (0.586)(R 0.591, F 0.582)] [D acc: (0.688)(0.750, 0.625)] [G loss: 2.882] [G acc: 0.250]\n",
      "4386 [D loss: (0.614)(R 0.601, F 0.628)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.526] [G acc: 0.375]\n",
      "4387 [D loss: (0.786)(R 0.964, F 0.609)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.068] [G acc: 0.375]\n",
      "4388 [D loss: (0.695)(R 0.865, F 0.524)] [D acc: (0.688)(0.625, 0.750)] [G loss: 2.304] [G acc: 0.312]\n",
      "4389 [D loss: (0.695)(R 0.496, F 0.895)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.237] [G acc: 0.375]\n",
      "4390 [D loss: (0.699)(R 0.849, F 0.549)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.822] [G acc: 0.438]\n",
      "4391 [D loss: (0.664)(R 0.490, F 0.838)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.634] [G acc: 0.500]\n",
      "4392 [D loss: (0.639)(R 0.657, F 0.621)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.633] [G acc: 0.312]\n",
      "4393 [D loss: (0.560)(R 0.256, F 0.865)] [D acc: (0.688)(1.000, 0.375)] [G loss: 1.919] [G acc: 0.438]\n",
      "4394 [D loss: (0.671)(R 0.425, F 0.918)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.518] [G acc: 0.500]\n",
      "4395 [D loss: (0.411)(R 0.371, F 0.451)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.185] [G acc: 0.250]\n",
      "4396 [D loss: (0.601)(R 0.454, F 0.747)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.691] [G acc: 0.375]\n",
      "4397 [D loss: (0.474)(R 0.320, F 0.628)] [D acc: (0.812)(1.000, 0.625)] [G loss: 1.701] [G acc: 0.312]\n",
      "4398 [D loss: (0.378)(R 0.331, F 0.425)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.499] [G acc: 0.375]\n",
      "4399 [D loss: (0.416)(R 0.430, F 0.401)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.542] [G acc: 0.312]\n",
      "4400 [D loss: (0.584)(R 0.598, F 0.570)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.318] [G acc: 0.312]\n",
      "4401 [D loss: (0.529)(R 0.430, F 0.628)] [D acc: (0.750)(0.875, 0.625)] [G loss: 2.271] [G acc: 0.312]\n",
      "4402 [D loss: (0.514)(R 0.385, F 0.643)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.302] [G acc: 0.312]\n",
      "4403 [D loss: (0.454)(R 0.439, F 0.469)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.006] [G acc: 0.500]\n",
      "4404 [D loss: (0.828)(R 0.753, F 0.902)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.548] [G acc: 0.375]\n",
      "4405 [D loss: (0.460)(R 0.416, F 0.503)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.180] [G acc: 0.562]\n",
      "4406 [D loss: (0.647)(R 0.412, F 0.883)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.298] [G acc: 0.562]\n",
      "4407 [D loss: (0.737)(R 0.662, F 0.811)] [D acc: (0.656)(0.812, 0.500)] [G loss: 2.108] [G acc: 0.438]\n",
      "4408 [D loss: (0.483)(R 0.374, F 0.592)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.755] [G acc: 0.375]\n",
      "4409 [D loss: (0.519)(R 0.457, F 0.582)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.626] [G acc: 0.250]\n",
      "4410 [D loss: (0.529)(R 0.614, F 0.445)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.782] [G acc: 0.375]\n",
      "4411 [D loss: (0.534)(R 0.498, F 0.570)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.865] [G acc: 0.500]\n",
      "4412 [D loss: (0.363)(R 0.363, F 0.364)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.182] [G acc: 0.312]\n",
      "4413 [D loss: (0.265)(R 0.276, F 0.254)] [D acc: (0.906)(0.938, 0.875)] [G loss: 2.244] [G acc: 0.438]\n",
      "4414 [D loss: (0.765)(R 0.736, F 0.794)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.025] [G acc: 0.375]\n",
      "4415 [D loss: (0.758)(R 1.010, F 0.506)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.549] [G acc: 0.438]\n",
      "4416 [D loss: (0.511)(R 0.302, F 0.720)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.547] [G acc: 0.438]\n",
      "4417 [D loss: (0.667)(R 0.325, F 1.009)] [D acc: (0.594)(0.938, 0.250)] [G loss: 1.857] [G acc: 0.375]\n",
      "4418 [D loss: (0.458)(R 0.398, F 0.517)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.136] [G acc: 0.250]\n",
      "4419 [D loss: (0.500)(R 0.535, F 0.465)] [D acc: (0.812)(0.875, 0.750)] [G loss: 3.065] [G acc: 0.188]\n",
      "4420 [D loss: (0.318)(R 0.310, F 0.326)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.064] [G acc: 0.312]\n",
      "4421 [D loss: (0.472)(R 0.520, F 0.423)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.465] [G acc: 0.500]\n",
      "4422 [D loss: (0.694)(R 0.482, F 0.906)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.836] [G acc: 0.375]\n",
      "4423 [D loss: (0.606)(R 0.550, F 0.661)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.663] [G acc: 0.500]\n",
      "4424 [D loss: (0.469)(R 0.497, F 0.440)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.679] [G acc: 0.375]\n",
      "4425 [D loss: (0.778)(R 0.647, F 0.909)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.389] [G acc: 0.500]\n",
      "4426 [D loss: (0.538)(R 0.306, F 0.770)] [D acc: (0.750)(1.000, 0.500)] [G loss: 1.544] [G acc: 0.250]\n",
      "4427 [D loss: (0.664)(R 0.386, F 0.943)] [D acc: (0.625)(0.875, 0.375)] [G loss: 2.002] [G acc: 0.375]\n",
      "4428 [D loss: (0.513)(R 0.337, F 0.688)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.493] [G acc: 0.438]\n",
      "4429 [D loss: (0.729)(R 0.487, F 0.971)] [D acc: (0.500)(0.750, 0.250)] [G loss: 1.853] [G acc: 0.250]\n",
      "4430 [D loss: (0.476)(R 0.409, F 0.543)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.723] [G acc: 0.438]\n",
      "4431 [D loss: (0.665)(R 0.726, F 0.605)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.702] [G acc: 0.312]\n",
      "4432 [D loss: (0.452)(R 0.294, F 0.609)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.079] [G acc: 0.312]\n",
      "4433 [D loss: (0.741)(R 0.778, F 0.703)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.600] [G acc: 0.438]\n",
      "4434 [D loss: (0.472)(R 0.410, F 0.533)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.711] [G acc: 0.375]\n",
      "4435 [D loss: (0.506)(R 0.484, F 0.527)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.919] [G acc: 0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4436 [D loss: (0.517)(R 0.464, F 0.570)] [D acc: (0.781)(0.938, 0.625)] [G loss: 2.011] [G acc: 0.375]\n",
      "4437 [D loss: (0.418)(R 0.517, F 0.319)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.616] [G acc: 0.500]\n",
      "4438 [D loss: (0.614)(R 0.640, F 0.588)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.607] [G acc: 0.500]\n",
      "4439 [D loss: (0.597)(R 0.428, F 0.765)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.767] [G acc: 0.375]\n",
      "4440 [D loss: (0.528)(R 0.346, F 0.710)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.365] [G acc: 0.562]\n",
      "4441 [D loss: (0.596)(R 0.413, F 0.780)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.992] [G acc: 0.375]\n",
      "4442 [D loss: (0.612)(R 0.664, F 0.560)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.783] [G acc: 0.375]\n",
      "4443 [D loss: (0.325)(R 0.326, F 0.325)] [D acc: (0.906)(1.000, 0.812)] [G loss: 2.499] [G acc: 0.312]\n",
      "4444 [D loss: (0.451)(R 0.438, F 0.463)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.235] [G acc: 0.312]\n",
      "4445 [D loss: (0.492)(R 0.387, F 0.598)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.640] [G acc: 0.375]\n",
      "4446 [D loss: (0.633)(R 0.599, F 0.668)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.237] [G acc: 0.562]\n",
      "4447 [D loss: (0.521)(R 0.328, F 0.714)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.072] [G acc: 0.438]\n",
      "4448 [D loss: (0.429)(R 0.349, F 0.508)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.131] [G acc: 0.375]\n",
      "4449 [D loss: (0.594)(R 0.805, F 0.384)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.846] [G acc: 0.375]\n",
      "4450 [D loss: (0.592)(R 0.690, F 0.493)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.657] [G acc: 0.500]\n",
      "4451 [D loss: (0.541)(R 0.372, F 0.710)] [D acc: (0.688)(0.938, 0.438)] [G loss: 2.374] [G acc: 0.375]\n",
      "4452 [D loss: (0.511)(R 0.362, F 0.660)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.788] [G acc: 0.438]\n",
      "4453 [D loss: (0.624)(R 0.632, F 0.617)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.695] [G acc: 0.500]\n",
      "4454 [D loss: (0.591)(R 0.379, F 0.803)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.581] [G acc: 0.500]\n",
      "4455 [D loss: (0.434)(R 0.411, F 0.457)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.626] [G acc: 0.438]\n",
      "4456 [D loss: (0.491)(R 0.353, F 0.629)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.140] [G acc: 0.438]\n",
      "4457 [D loss: (0.672)(R 0.608, F 0.736)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.938] [G acc: 0.375]\n",
      "4458 [D loss: (0.498)(R 0.416, F 0.581)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.323] [G acc: 0.500]\n",
      "4459 [D loss: (0.670)(R 0.642, F 0.697)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.855] [G acc: 0.438]\n",
      "4460 [D loss: (0.439)(R 0.326, F 0.553)] [D acc: (0.812)(1.000, 0.625)] [G loss: 1.968] [G acc: 0.438]\n",
      "4461 [D loss: (0.344)(R 0.367, F 0.321)] [D acc: (0.875)(0.938, 0.812)] [G loss: 2.755] [G acc: 0.438]\n",
      "4462 [D loss: (0.578)(R 0.529, F 0.628)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.645] [G acc: 0.375]\n",
      "4463 [D loss: (0.375)(R 0.341, F 0.409)] [D acc: (0.875)(1.000, 0.750)] [G loss: 2.431] [G acc: 0.312]\n",
      "4464 [D loss: (0.578)(R 0.443, F 0.714)] [D acc: (0.625)(0.812, 0.438)] [G loss: 2.350] [G acc: 0.250]\n",
      "4465 [D loss: (0.374)(R 0.311, F 0.437)] [D acc: (0.844)(1.000, 0.688)] [G loss: 2.516] [G acc: 0.188]\n",
      "4466 [D loss: (0.420)(R 0.317, F 0.524)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.281] [G acc: 0.375]\n",
      "4467 [D loss: (0.660)(R 0.601, F 0.719)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.357] [G acc: 0.312]\n",
      "4468 [D loss: (0.692)(R 0.617, F 0.767)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.996] [G acc: 0.375]\n",
      "4469 [D loss: (0.603)(R 0.444, F 0.762)] [D acc: (0.656)(0.750, 0.562)] [G loss: 2.189] [G acc: 0.312]\n",
      "4470 [D loss: (0.545)(R 0.526, F 0.565)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.913] [G acc: 0.438]\n",
      "4471 [D loss: (0.733)(R 0.636, F 0.831)] [D acc: (0.625)(0.812, 0.438)] [G loss: 2.040] [G acc: 0.438]\n",
      "4472 [D loss: (0.648)(R 0.590, F 0.706)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.913] [G acc: 0.438]\n",
      "4473 [D loss: (0.540)(R 0.528, F 0.551)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.200] [G acc: 0.312]\n",
      "4474 [D loss: (0.536)(R 0.445, F 0.628)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.307] [G acc: 0.438]\n",
      "4475 [D loss: (0.707)(R 0.828, F 0.585)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.741] [G acc: 0.250]\n",
      "4476 [D loss: (0.459)(R 0.436, F 0.483)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.896] [G acc: 0.438]\n",
      "4477 [D loss: (0.468)(R 0.335, F 0.602)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.980] [G acc: 0.375]\n",
      "4478 [D loss: (0.757)(R 0.471, F 1.043)] [D acc: (0.594)(0.875, 0.312)] [G loss: 1.740] [G acc: 0.438]\n",
      "4479 [D loss: (0.761)(R 0.842, F 0.680)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.698] [G acc: 0.312]\n",
      "4480 [D loss: (0.680)(R 0.418, F 0.942)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.670] [G acc: 0.438]\n",
      "4481 [D loss: (0.459)(R 0.396, F 0.521)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.339] [G acc: 0.438]\n",
      "4482 [D loss: (0.411)(R 0.411, F 0.412)] [D acc: (0.875)(1.000, 0.750)] [G loss: 1.804] [G acc: 0.375]\n",
      "4483 [D loss: (0.614)(R 0.614, F 0.615)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.948] [G acc: 0.312]\n",
      "4484 [D loss: (0.451)(R 0.402, F 0.501)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.363] [G acc: 0.375]\n",
      "4485 [D loss: (0.452)(R 0.349, F 0.555)] [D acc: (0.812)(1.000, 0.625)] [G loss: 2.510] [G acc: 0.188]\n",
      "4486 [D loss: (0.662)(R 0.554, F 0.769)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.955] [G acc: 0.312]\n",
      "4487 [D loss: (0.705)(R 0.834, F 0.576)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.867] [G acc: 0.312]\n",
      "4488 [D loss: (0.612)(R 0.543, F 0.681)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.622] [G acc: 0.438]\n",
      "4489 [D loss: (0.438)(R 0.438, F 0.438)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.675] [G acc: 0.438]\n",
      "4490 [D loss: (0.615)(R 0.553, F 0.676)] [D acc: (0.625)(0.750, 0.500)] [G loss: 2.165] [G acc: 0.375]\n",
      "4491 [D loss: (0.660)(R 0.564, F 0.757)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.400] [G acc: 0.500]\n",
      "4492 [D loss: (0.566)(R 0.374, F 0.759)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.868] [G acc: 0.438]\n",
      "4493 [D loss: (0.625)(R 0.416, F 0.834)] [D acc: (0.688)(1.000, 0.375)] [G loss: 1.352] [G acc: 0.438]\n",
      "4494 [D loss: (0.471)(R 0.376, F 0.566)] [D acc: (0.781)(1.000, 0.562)] [G loss: 2.486] [G acc: 0.312]\n",
      "4495 [D loss: (0.529)(R 0.683, F 0.376)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.724] [G acc: 0.500]\n",
      "4496 [D loss: (0.586)(R 0.450, F 0.722)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.020] [G acc: 0.312]\n",
      "4497 [D loss: (0.522)(R 0.368, F 0.677)] [D acc: (0.719)(1.000, 0.438)] [G loss: 2.210] [G acc: 0.312]\n",
      "4498 [D loss: (0.523)(R 0.483, F 0.563)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.599] [G acc: 0.438]\n",
      "4499 [D loss: (0.677)(R 0.634, F 0.719)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.596] [G acc: 0.500]\n",
      "4500 [D loss: (0.700)(R 0.649, F 0.751)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.195] [G acc: 0.312]\n",
      "4501 [D loss: (0.541)(R 0.412, F 0.670)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.899] [G acc: 0.375]\n",
      "4502 [D loss: (0.616)(R 0.403, F 0.828)] [D acc: (0.625)(0.938, 0.312)] [G loss: 2.399] [G acc: 0.188]\n",
      "4503 [D loss: (0.685)(R 0.499, F 0.871)] [D acc: (0.562)(0.812, 0.312)] [G loss: 1.927] [G acc: 0.500]\n",
      "4504 [D loss: (0.540)(R 0.619, F 0.462)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.792] [G acc: 0.250]\n",
      "4505 [D loss: (0.621)(R 0.461, F 0.780)] [D acc: (0.625)(0.812, 0.438)] [G loss: 2.493] [G acc: 0.312]\n",
      "4506 [D loss: (0.485)(R 0.544, F 0.426)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.608] [G acc: 0.375]\n",
      "4507 [D loss: (0.515)(R 0.397, F 0.634)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.805] [G acc: 0.438]\n",
      "4508 [D loss: (0.497)(R 0.482, F 0.513)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.844] [G acc: 0.438]\n",
      "4509 [D loss: (0.535)(R 0.440, F 0.631)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.515] [G acc: 0.438]\n",
      "4510 [D loss: (0.603)(R 0.817, F 0.389)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.667] [G acc: 0.375]\n",
      "4511 [D loss: (0.521)(R 0.477, F 0.566)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.529] [G acc: 0.312]\n",
      "4512 [D loss: (0.716)(R 0.846, F 0.586)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.668] [G acc: 0.500]\n",
      "4513 [D loss: (0.621)(R 0.545, F 0.696)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.532] [G acc: 0.500]\n",
      "4514 [D loss: (0.561)(R 0.444, F 0.679)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.925] [G acc: 0.250]\n",
      "4515 [D loss: (0.492)(R 0.521, F 0.462)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.581] [G acc: 0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4516 [D loss: (0.605)(R 0.548, F 0.662)] [D acc: (0.719)(0.875, 0.562)] [G loss: 2.063] [G acc: 0.438]\n",
      "4517 [D loss: (0.598)(R 0.667, F 0.530)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.833] [G acc: 0.375]\n",
      "4518 [D loss: (0.380)(R 0.397, F 0.363)] [D acc: (0.906)(0.938, 0.875)] [G loss: 1.704] [G acc: 0.375]\n",
      "4519 [D loss: (0.431)(R 0.469, F 0.393)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.597] [G acc: 0.375]\n",
      "4520 [D loss: (0.451)(R 0.514, F 0.388)] [D acc: (0.781)(0.875, 0.688)] [G loss: 2.021] [G acc: 0.500]\n",
      "4521 [D loss: (0.491)(R 0.371, F 0.610)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.969] [G acc: 0.375]\n",
      "4522 [D loss: (0.488)(R 0.395, F 0.581)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.887] [G acc: 0.500]\n",
      "4523 [D loss: (0.555)(R 0.402, F 0.709)] [D acc: (0.688)(1.000, 0.375)] [G loss: 2.263] [G acc: 0.375]\n",
      "4524 [D loss: (0.458)(R 0.503, F 0.414)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.078] [G acc: 0.375]\n",
      "4525 [D loss: (0.765)(R 1.172, F 0.357)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.681] [G acc: 0.438]\n",
      "4526 [D loss: (0.501)(R 0.344, F 0.657)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.927] [G acc: 0.438]\n",
      "4527 [D loss: (0.698)(R 0.676, F 0.721)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.887] [G acc: 0.438]\n",
      "4528 [D loss: (0.494)(R 0.503, F 0.485)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.400] [G acc: 0.500]\n",
      "4529 [D loss: (0.492)(R 0.457, F 0.528)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.324] [G acc: 0.125]\n",
      "4530 [D loss: (0.543)(R 0.565, F 0.520)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.839] [G acc: 0.375]\n",
      "4531 [D loss: (0.468)(R 0.519, F 0.417)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.799] [G acc: 0.500]\n",
      "4532 [D loss: (0.487)(R 0.524, F 0.450)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.173] [G acc: 0.438]\n",
      "4533 [D loss: (0.448)(R 0.451, F 0.445)] [D acc: (0.812)(0.875, 0.750)] [G loss: 2.360] [G acc: 0.312]\n",
      "4534 [D loss: (0.482)(R 0.675, F 0.288)] [D acc: (0.875)(0.938, 0.812)] [G loss: 1.822] [G acc: 0.438]\n",
      "4535 [D loss: (0.597)(R 0.348, F 0.847)] [D acc: (0.688)(1.000, 0.375)] [G loss: 1.859] [G acc: 0.500]\n",
      "4536 [D loss: (0.513)(R 0.489, F 0.538)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.922] [G acc: 0.438]\n",
      "4537 [D loss: (0.631)(R 0.843, F 0.418)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.719] [G acc: 0.500]\n",
      "4538 [D loss: (0.600)(R 0.451, F 0.749)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.827] [G acc: 0.375]\n",
      "4539 [D loss: (0.546)(R 0.655, F 0.438)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.222] [G acc: 0.562]\n",
      "4540 [D loss: (0.617)(R 0.546, F 0.689)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.170] [G acc: 0.438]\n",
      "4541 [D loss: (0.606)(R 0.773, F 0.440)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.615] [G acc: 0.438]\n",
      "4542 [D loss: (0.577)(R 0.534, F 0.619)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.085] [G acc: 0.500]\n",
      "4543 [D loss: (0.511)(R 0.426, F 0.596)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.310] [G acc: 0.438]\n",
      "4544 [D loss: (0.557)(R 0.516, F 0.597)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.603] [G acc: 0.500]\n",
      "4545 [D loss: (0.413)(R 0.355, F 0.471)] [D acc: (0.812)(1.000, 0.625)] [G loss: 1.547] [G acc: 0.500]\n",
      "4546 [D loss: (0.565)(R 0.520, F 0.609)] [D acc: (0.688)(0.875, 0.500)] [G loss: 2.048] [G acc: 0.250]\n",
      "4547 [D loss: (0.578)(R 0.607, F 0.548)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.683] [G acc: 0.312]\n",
      "4548 [D loss: (0.508)(R 0.412, F 0.604)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.656] [G acc: 0.312]\n",
      "4549 [D loss: (0.527)(R 0.592, F 0.463)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.626] [G acc: 0.438]\n",
      "4550 [D loss: (0.509)(R 0.347, F 0.671)] [D acc: (0.750)(1.000, 0.500)] [G loss: 2.157] [G acc: 0.438]\n",
      "4551 [D loss: (0.667)(R 0.684, F 0.650)] [D acc: (0.594)(0.688, 0.500)] [G loss: 1.521] [G acc: 0.375]\n",
      "4552 [D loss: (0.734)(R 0.621, F 0.846)] [D acc: (0.625)(0.938, 0.312)] [G loss: 1.499] [G acc: 0.438]\n",
      "4553 [D loss: (0.578)(R 0.436, F 0.719)] [D acc: (0.688)(0.938, 0.438)] [G loss: 2.085] [G acc: 0.188]\n",
      "4554 [D loss: (0.753)(R 0.874, F 0.631)] [D acc: (0.625)(0.688, 0.562)] [G loss: 1.432] [G acc: 0.438]\n",
      "4555 [D loss: (0.530)(R 0.410, F 0.650)] [D acc: (0.750)(0.938, 0.562)] [G loss: 2.089] [G acc: 0.562]\n",
      "4556 [D loss: (0.514)(R 0.572, F 0.456)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.576] [G acc: 0.250]\n",
      "4557 [D loss: (0.535)(R 0.620, F 0.450)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.296] [G acc: 0.375]\n",
      "4558 [D loss: (0.688)(R 0.779, F 0.596)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.189] [G acc: 0.188]\n",
      "4559 [D loss: (0.619)(R 0.665, F 0.573)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.496] [G acc: 0.375]\n",
      "4560 [D loss: (0.603)(R 0.630, F 0.577)] [D acc: (0.562)(0.562, 0.562)] [G loss: 1.251] [G acc: 0.500]\n",
      "4561 [D loss: (0.705)(R 0.885, F 0.526)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.784] [G acc: 0.375]\n",
      "4562 [D loss: (0.661)(R 0.747, F 0.576)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.179] [G acc: 0.438]\n",
      "4563 [D loss: (0.739)(R 0.893, F 0.585)] [D acc: (0.594)(0.500, 0.688)] [G loss: 1.439] [G acc: 0.375]\n",
      "4564 [D loss: (0.638)(R 0.669, F 0.606)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.351] [G acc: 0.375]\n",
      "4565 [D loss: (0.859)(R 1.022, F 0.696)] [D acc: (0.594)(0.688, 0.500)] [G loss: 1.087] [G acc: 0.562]\n",
      "4566 [D loss: (0.567)(R 0.481, F 0.653)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.052] [G acc: 0.562]\n",
      "4567 [D loss: (0.644)(R 0.392, F 0.897)] [D acc: (0.688)(1.000, 0.375)] [G loss: 1.827] [G acc: 0.500]\n",
      "4568 [D loss: (0.536)(R 0.389, F 0.682)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.271] [G acc: 0.500]\n",
      "4569 [D loss: (0.718)(R 0.695, F 0.741)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.709] [G acc: 0.375]\n",
      "4570 [D loss: (0.522)(R 0.502, F 0.543)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.071] [G acc: 0.438]\n",
      "4571 [D loss: (0.453)(R 0.445, F 0.460)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.457] [G acc: 0.500]\n",
      "4572 [D loss: (0.571)(R 0.494, F 0.647)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.458] [G acc: 0.500]\n",
      "4573 [D loss: (0.631)(R 0.630, F 0.631)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.254] [G acc: 0.500]\n",
      "4574 [D loss: (0.580)(R 0.585, F 0.575)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.511] [G acc: 0.438]\n",
      "4575 [D loss: (0.506)(R 0.551, F 0.461)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.458] [G acc: 0.375]\n",
      "4576 [D loss: (0.702)(R 0.625, F 0.780)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.425] [G acc: 0.438]\n",
      "4577 [D loss: (0.525)(R 0.574, F 0.476)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.499] [G acc: 0.375]\n",
      "4578 [D loss: (0.618)(R 0.480, F 0.755)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.374] [G acc: 0.500]\n",
      "4579 [D loss: (0.535)(R 0.392, F 0.678)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.725] [G acc: 0.375]\n",
      "4580 [D loss: (0.545)(R 0.552, F 0.538)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.307] [G acc: 0.438]\n",
      "4581 [D loss: (0.487)(R 0.400, F 0.573)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.538] [G acc: 0.375]\n",
      "4582 [D loss: (0.555)(R 0.424, F 0.686)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.556] [G acc: 0.438]\n",
      "4583 [D loss: (0.630)(R 0.484, F 0.776)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.818] [G acc: 0.438]\n",
      "4584 [D loss: (0.741)(R 0.648, F 0.834)] [D acc: (0.562)(0.812, 0.312)] [G loss: 1.327] [G acc: 0.500]\n",
      "4585 [D loss: (0.585)(R 0.497, F 0.673)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.381] [G acc: 0.438]\n",
      "4586 [D loss: (0.429)(R 0.470, F 0.387)] [D acc: (0.844)(0.812, 0.875)] [G loss: 1.965] [G acc: 0.188]\n",
      "4587 [D loss: (0.526)(R 0.385, F 0.667)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.854] [G acc: 0.562]\n",
      "4588 [D loss: (0.431)(R 0.370, F 0.493)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.948] [G acc: 0.375]\n",
      "4589 [D loss: (0.564)(R 0.561, F 0.566)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.460] [G acc: 0.375]\n",
      "4590 [D loss: (0.431)(R 0.532, F 0.330)] [D acc: (0.875)(0.812, 0.938)] [G loss: 1.774] [G acc: 0.250]\n",
      "4591 [D loss: (0.723)(R 0.643, F 0.803)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.425] [G acc: 0.500]\n",
      "4592 [D loss: (0.474)(R 0.381, F 0.566)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.843] [G acc: 0.312]\n",
      "4593 [D loss: (0.699)(R 0.605, F 0.794)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.565] [G acc: 0.438]\n",
      "4594 [D loss: (0.888)(R 0.950, F 0.825)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.438] [G acc: 0.438]\n",
      "4595 [D loss: (0.703)(R 0.739, F 0.667)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.325] [G acc: 0.500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4596 [D loss: (0.687)(R 0.709, F 0.666)] [D acc: (0.531)(0.625, 0.438)] [G loss: 1.468] [G acc: 0.500]\n",
      "4597 [D loss: (0.701)(R 0.626, F 0.777)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.358] [G acc: 0.562]\n",
      "4598 [D loss: (0.486)(R 0.417, F 0.554)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.527] [G acc: 0.375]\n",
      "4599 [D loss: (0.632)(R 0.568, F 0.697)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.452] [G acc: 0.500]\n",
      "4600 [D loss: (0.528)(R 0.362, F 0.694)] [D acc: (0.750)(1.000, 0.500)] [G loss: 1.645] [G acc: 0.375]\n",
      "4601 [D loss: (0.506)(R 0.482, F 0.529)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.166] [G acc: 0.312]\n",
      "4602 [D loss: (0.571)(R 0.649, F 0.493)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.227] [G acc: 0.438]\n",
      "4603 [D loss: (0.571)(R 0.549, F 0.594)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.381] [G acc: 0.438]\n",
      "4604 [D loss: (0.510)(R 0.506, F 0.515)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.866] [G acc: 0.375]\n",
      "4605 [D loss: (0.312)(R 0.362, F 0.263)] [D acc: (0.969)(1.000, 0.938)] [G loss: 2.127] [G acc: 0.438]\n",
      "4606 [D loss: (0.590)(R 0.600, F 0.579)] [D acc: (0.656)(0.812, 0.500)] [G loss: 2.687] [G acc: 0.250]\n",
      "4607 [D loss: (0.526)(R 0.510, F 0.541)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.525] [G acc: 0.312]\n",
      "4608 [D loss: (0.540)(R 0.455, F 0.624)] [D acc: (0.719)(0.938, 0.500)] [G loss: 2.511] [G acc: 0.250]\n",
      "4609 [D loss: (0.760)(R 0.836, F 0.685)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.938] [G acc: 0.250]\n",
      "4610 [D loss: (0.739)(R 0.871, F 0.608)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.548] [G acc: 0.500]\n",
      "4611 [D loss: (0.709)(R 0.987, F 0.432)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.567] [G acc: 0.312]\n",
      "4612 [D loss: (0.710)(R 0.720, F 0.700)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.827] [G acc: 0.312]\n",
      "4613 [D loss: (0.707)(R 0.762, F 0.653)] [D acc: (0.625)(0.688, 0.562)] [G loss: 1.499] [G acc: 0.438]\n",
      "4614 [D loss: (0.660)(R 0.524, F 0.797)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.524] [G acc: 0.375]\n",
      "4615 [D loss: (0.500)(R 0.476, F 0.523)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.434] [G acc: 0.500]\n",
      "4616 [D loss: (0.542)(R 0.495, F 0.589)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.755] [G acc: 0.375]\n",
      "4617 [D loss: (0.576)(R 0.513, F 0.639)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.402] [G acc: 0.500]\n",
      "4618 [D loss: (0.653)(R 0.724, F 0.582)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.261] [G acc: 0.562]\n",
      "4619 [D loss: (0.598)(R 0.553, F 0.642)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.646] [G acc: 0.375]\n",
      "4620 [D loss: (0.563)(R 0.448, F 0.677)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.351] [G acc: 0.375]\n",
      "4621 [D loss: (0.550)(R 0.555, F 0.546)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.407] [G acc: 0.438]\n",
      "4622 [D loss: (0.667)(R 0.616, F 0.718)] [D acc: (0.594)(0.750, 0.438)] [G loss: 1.424] [G acc: 0.312]\n",
      "4623 [D loss: (0.572)(R 0.528, F 0.616)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.484] [G acc: 0.312]\n",
      "4624 [D loss: (0.500)(R 0.363, F 0.637)] [D acc: (0.750)(1.000, 0.500)] [G loss: 1.670] [G acc: 0.312]\n",
      "4625 [D loss: (0.647)(R 0.646, F 0.648)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.446] [G acc: 0.312]\n",
      "4626 [D loss: (0.574)(R 0.569, F 0.580)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.291] [G acc: 0.438]\n",
      "4627 [D loss: (0.507)(R 0.615, F 0.399)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.772] [G acc: 0.562]\n",
      "4628 [D loss: (0.519)(R 0.478, F 0.561)] [D acc: (0.688)(0.812, 0.562)] [G loss: 2.224] [G acc: 0.375]\n",
      "4629 [D loss: (0.626)(R 0.573, F 0.679)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.708] [G acc: 0.375]\n",
      "4630 [D loss: (0.683)(R 0.912, F 0.454)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.654] [G acc: 0.438]\n",
      "4631 [D loss: (0.733)(R 0.627, F 0.839)] [D acc: (0.531)(0.688, 0.375)] [G loss: 1.711] [G acc: 0.375]\n",
      "4632 [D loss: (0.474)(R 0.437, F 0.510)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.688] [G acc: 0.312]\n",
      "4633 [D loss: (0.554)(R 0.425, F 0.684)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.507] [G acc: 0.438]\n",
      "4634 [D loss: (0.549)(R 0.489, F 0.609)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.532] [G acc: 0.438]\n",
      "4635 [D loss: (0.647)(R 0.695, F 0.599)] [D acc: (0.594)(0.688, 0.500)] [G loss: 1.717] [G acc: 0.375]\n",
      "4636 [D loss: (0.655)(R 0.591, F 0.719)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.335] [G acc: 0.500]\n",
      "4637 [D loss: (0.510)(R 0.495, F 0.525)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.627] [G acc: 0.438]\n",
      "4638 [D loss: (0.757)(R 0.845, F 0.668)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.861] [G acc: 0.375]\n",
      "4639 [D loss: (0.562)(R 0.517, F 0.607)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.162] [G acc: 0.500]\n",
      "4640 [D loss: (0.702)(R 0.757, F 0.647)] [D acc: (0.625)(0.750, 0.500)] [G loss: 0.991] [G acc: 0.500]\n",
      "4641 [D loss: (0.541)(R 0.503, F 0.579)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.117] [G acc: 0.562]\n",
      "4642 [D loss: (0.604)(R 0.414, F 0.794)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.467] [G acc: 0.438]\n",
      "4643 [D loss: (0.509)(R 0.519, F 0.498)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.467] [G acc: 0.438]\n",
      "4644 [D loss: (0.553)(R 0.528, F 0.579)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.775] [G acc: 0.250]\n",
      "4645 [D loss: (0.738)(R 0.605, F 0.871)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.519] [G acc: 0.312]\n",
      "4646 [D loss: (0.569)(R 0.440, F 0.697)] [D acc: (0.750)(1.000, 0.500)] [G loss: 0.988] [G acc: 0.562]\n",
      "4647 [D loss: (0.482)(R 0.349, F 0.615)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.739] [G acc: 0.500]\n",
      "4648 [D loss: (0.482)(R 0.488, F 0.476)] [D acc: (0.812)(0.938, 0.688)] [G loss: 2.351] [G acc: 0.312]\n",
      "4649 [D loss: (0.572)(R 0.447, F 0.696)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.542] [G acc: 0.500]\n",
      "4650 [D loss: (0.491)(R 0.414, F 0.567)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.557] [G acc: 0.438]\n",
      "4651 [D loss: (0.567)(R 0.485, F 0.649)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.945] [G acc: 0.250]\n",
      "4652 [D loss: (0.703)(R 0.873, F 0.533)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.239] [G acc: 0.625]\n",
      "4653 [D loss: (0.586)(R 0.508, F 0.664)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.314] [G acc: 0.438]\n",
      "4654 [D loss: (0.580)(R 0.384, F 0.777)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.510] [G acc: 0.438]\n",
      "4655 [D loss: (0.623)(R 0.573, F 0.673)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.347] [G acc: 0.562]\n",
      "4656 [D loss: (0.802)(R 0.836, F 0.769)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.029] [G acc: 0.500]\n",
      "4657 [D loss: (0.580)(R 0.487, F 0.674)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.342] [G acc: 0.500]\n",
      "4658 [D loss: (0.693)(R 0.959, F 0.427)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.268] [G acc: 0.562]\n",
      "4659 [D loss: (0.614)(R 0.382, F 0.846)] [D acc: (0.594)(0.938, 0.250)] [G loss: 1.167] [G acc: 0.375]\n",
      "4660 [D loss: (0.632)(R 0.511, F 0.753)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.187] [G acc: 0.500]\n",
      "4661 [D loss: (0.491)(R 0.451, F 0.530)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.637] [G acc: 0.438]\n",
      "4662 [D loss: (0.537)(R 0.510, F 0.563)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.619] [G acc: 0.438]\n",
      "4663 [D loss: (0.601)(R 0.446, F 0.757)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.199] [G acc: 0.562]\n",
      "4664 [D loss: (0.761)(R 0.705, F 0.817)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.650] [G acc: 0.500]\n",
      "4665 [D loss: (0.614)(R 0.454, F 0.775)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.643] [G acc: 0.375]\n",
      "4666 [D loss: (0.483)(R 0.478, F 0.489)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.914] [G acc: 0.375]\n",
      "4667 [D loss: (0.588)(R 0.654, F 0.522)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.433] [G acc: 0.375]\n",
      "4668 [D loss: (0.717)(R 0.843, F 0.590)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.430] [G acc: 0.562]\n",
      "4669 [D loss: (0.564)(R 0.554, F 0.574)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.515] [G acc: 0.312]\n",
      "4670 [D loss: (0.815)(R 0.807, F 0.824)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.469] [G acc: 0.438]\n",
      "4671 [D loss: (0.646)(R 0.872, F 0.419)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.025] [G acc: 0.438]\n",
      "4672 [D loss: (0.760)(R 0.689, F 0.830)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.035] [G acc: 0.500]\n",
      "4673 [D loss: (0.688)(R 0.647, F 0.729)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.439] [G acc: 0.375]\n",
      "4674 [D loss: (0.472)(R 0.449, F 0.495)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.487] [G acc: 0.375]\n",
      "4675 [D loss: (0.515)(R 0.555, F 0.475)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.262] [G acc: 0.562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4676 [D loss: (0.687)(R 0.659, F 0.716)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.197] [G acc: 0.438]\n",
      "4677 [D loss: (0.439)(R 0.446, F 0.432)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.084] [G acc: 0.375]\n",
      "4678 [D loss: (0.471)(R 0.468, F 0.474)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.490] [G acc: 0.375]\n",
      "4679 [D loss: (0.604)(R 0.553, F 0.654)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.612] [G acc: 0.312]\n",
      "4680 [D loss: (0.522)(R 0.587, F 0.456)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.581] [G acc: 0.500]\n",
      "4681 [D loss: (0.625)(R 0.733, F 0.517)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.807] [G acc: 0.375]\n",
      "4682 [D loss: (0.443)(R 0.389, F 0.498)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.458] [G acc: 0.500]\n",
      "4683 [D loss: (0.471)(R 0.435, F 0.507)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.542] [G acc: 0.562]\n",
      "4684 [D loss: (0.681)(R 0.711, F 0.651)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.405] [G acc: 0.312]\n",
      "4685 [D loss: (0.664)(R 0.580, F 0.748)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.543] [G acc: 0.438]\n",
      "4686 [D loss: (0.465)(R 0.446, F 0.484)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.634] [G acc: 0.312]\n",
      "4687 [D loss: (0.637)(R 0.701, F 0.574)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.107] [G acc: 0.500]\n",
      "4688 [D loss: (0.469)(R 0.441, F 0.496)] [D acc: (0.812)(1.000, 0.625)] [G loss: 1.290] [G acc: 0.375]\n",
      "4689 [D loss: (0.600)(R 0.493, F 0.707)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.405] [G acc: 0.438]\n",
      "4690 [D loss: (0.681)(R 0.562, F 0.800)] [D acc: (0.531)(0.688, 0.375)] [G loss: 1.392] [G acc: 0.438]\n",
      "4691 [D loss: (0.574)(R 0.453, F 0.695)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.098] [G acc: 0.562]\n",
      "4692 [D loss: (0.519)(R 0.628, F 0.409)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.907] [G acc: 0.500]\n",
      "4693 [D loss: (0.534)(R 0.578, F 0.491)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.635] [G acc: 0.312]\n",
      "4694 [D loss: (0.505)(R 0.408, F 0.603)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.241] [G acc: 0.438]\n",
      "4695 [D loss: (0.559)(R 0.506, F 0.612)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.794] [G acc: 0.250]\n",
      "4696 [D loss: (0.680)(R 0.751, F 0.608)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.544] [G acc: 0.375]\n",
      "4697 [D loss: (0.703)(R 0.833, F 0.573)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.148] [G acc: 0.438]\n",
      "4698 [D loss: (0.702)(R 0.611, F 0.792)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.293] [G acc: 0.500]\n",
      "4699 [D loss: (0.546)(R 0.554, F 0.539)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.116] [G acc: 0.562]\n",
      "4700 [D loss: (0.431)(R 0.383, F 0.479)] [D acc: (0.812)(0.938, 0.688)] [G loss: 0.992] [G acc: 0.500]\n",
      "4701 [D loss: (0.502)(R 0.459, F 0.545)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.255] [G acc: 0.562]\n",
      "4702 [D loss: (0.487)(R 0.458, F 0.516)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.760] [G acc: 0.250]\n",
      "4703 [D loss: (0.607)(R 0.606, F 0.608)] [D acc: (0.625)(0.688, 0.562)] [G loss: 1.513] [G acc: 0.500]\n",
      "4704 [D loss: (0.572)(R 0.535, F 0.610)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.526] [G acc: 0.500]\n",
      "4705 [D loss: (0.584)(R 0.485, F 0.684)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.601] [G acc: 0.500]\n",
      "4706 [D loss: (0.474)(R 0.371, F 0.577)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.319] [G acc: 0.500]\n",
      "4707 [D loss: (0.691)(R 0.667, F 0.715)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.623] [G acc: 0.375]\n",
      "4708 [D loss: (0.561)(R 0.484, F 0.638)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.191] [G acc: 0.625]\n",
      "4709 [D loss: (0.721)(R 0.648, F 0.793)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.276] [G acc: 0.375]\n",
      "4710 [D loss: (0.549)(R 0.574, F 0.524)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.203] [G acc: 0.562]\n",
      "4711 [D loss: (0.573)(R 0.585, F 0.560)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.388] [G acc: 0.438]\n",
      "4712 [D loss: (0.509)(R 0.368, F 0.651)] [D acc: (0.750)(1.000, 0.500)] [G loss: 1.233] [G acc: 0.562]\n",
      "4713 [D loss: (0.721)(R 0.500, F 0.942)] [D acc: (0.562)(0.938, 0.188)] [G loss: 1.457] [G acc: 0.438]\n",
      "4714 [D loss: (0.535)(R 0.415, F 0.655)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.205] [G acc: 0.562]\n",
      "4715 [D loss: (0.741)(R 0.474, F 1.009)] [D acc: (0.500)(0.812, 0.188)] [G loss: 1.568] [G acc: 0.375]\n",
      "4716 [D loss: (0.677)(R 0.651, F 0.704)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.418] [G acc: 0.562]\n",
      "4717 [D loss: (0.583)(R 0.451, F 0.715)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.617] [G acc: 0.438]\n",
      "4718 [D loss: (0.759)(R 0.766, F 0.753)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.211] [G acc: 0.625]\n",
      "4719 [D loss: (0.588)(R 0.615, F 0.561)] [D acc: (0.781)(0.938, 0.625)] [G loss: 0.917] [G acc: 0.625]\n",
      "4720 [D loss: (0.650)(R 0.698, F 0.602)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.045] [G acc: 0.500]\n",
      "4721 [D loss: (0.629)(R 0.569, F 0.689)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.342] [G acc: 0.500]\n",
      "4722 [D loss: (0.553)(R 0.454, F 0.652)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.392] [G acc: 0.562]\n",
      "4723 [D loss: (0.625)(R 0.454, F 0.797)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.525] [G acc: 0.500]\n",
      "4724 [D loss: (0.821)(R 0.682, F 0.959)] [D acc: (0.469)(0.750, 0.188)] [G loss: 0.900] [G acc: 0.688]\n",
      "4725 [D loss: (0.786)(R 0.748, F 0.823)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.428] [G acc: 0.438]\n",
      "4726 [D loss: (0.621)(R 0.698, F 0.544)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.275] [G acc: 0.500]\n",
      "4727 [D loss: (0.517)(R 0.535, F 0.500)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.387] [G acc: 0.500]\n",
      "4728 [D loss: (0.564)(R 0.500, F 0.627)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.108] [G acc: 0.438]\n",
      "4729 [D loss: (0.689)(R 0.512, F 0.865)] [D acc: (0.594)(0.938, 0.250)] [G loss: 1.506] [G acc: 0.312]\n",
      "4730 [D loss: (0.549)(R 0.436, F 0.661)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.369] [G acc: 0.500]\n",
      "4731 [D loss: (0.630)(R 0.525, F 0.735)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.087] [G acc: 0.625]\n",
      "4732 [D loss: (0.584)(R 0.409, F 0.759)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.325] [G acc: 0.438]\n",
      "4733 [D loss: (0.668)(R 0.562, F 0.774)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.411] [G acc: 0.500]\n",
      "4734 [D loss: (0.752)(R 0.714, F 0.790)] [D acc: (0.531)(0.750, 0.312)] [G loss: 1.437] [G acc: 0.438]\n",
      "4735 [D loss: (0.617)(R 0.557, F 0.677)] [D acc: (0.656)(0.812, 0.500)] [G loss: 0.814] [G acc: 0.688]\n",
      "4736 [D loss: (0.689)(R 0.681, F 0.698)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.118] [G acc: 0.562]\n",
      "4737 [D loss: (0.628)(R 0.672, F 0.583)] [D acc: (0.688)(0.812, 0.562)] [G loss: 0.999] [G acc: 0.500]\n",
      "4738 [D loss: (0.535)(R 0.477, F 0.593)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.134] [G acc: 0.562]\n",
      "4739 [D loss: (0.559)(R 0.560, F 0.557)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.044] [G acc: 0.562]\n",
      "4740 [D loss: (0.677)(R 0.598, F 0.757)] [D acc: (0.562)(0.812, 0.312)] [G loss: 1.416] [G acc: 0.250]\n",
      "4741 [D loss: (0.477)(R 0.436, F 0.518)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.264] [G acc: 0.562]\n",
      "4742 [D loss: (0.591)(R 0.496, F 0.685)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.431] [G acc: 0.625]\n",
      "4743 [D loss: (0.594)(R 0.756, F 0.432)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.191] [G acc: 0.562]\n",
      "4744 [D loss: (0.582)(R 0.554, F 0.611)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.770] [G acc: 0.438]\n",
      "4745 [D loss: (0.729)(R 0.790, F 0.668)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.160] [G acc: 0.625]\n",
      "4746 [D loss: (0.628)(R 0.537, F 0.718)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.408] [G acc: 0.562]\n",
      "4747 [D loss: (0.486)(R 0.460, F 0.513)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.448] [G acc: 0.375]\n",
      "4748 [D loss: (0.641)(R 0.656, F 0.627)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.611] [G acc: 0.312]\n",
      "4749 [D loss: (0.587)(R 0.539, F 0.635)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.174] [G acc: 0.438]\n",
      "4750 [D loss: (0.589)(R 0.456, F 0.722)] [D acc: (0.656)(0.938, 0.375)] [G loss: 0.962] [G acc: 0.625]\n",
      "4751 [D loss: (0.745)(R 0.765, F 0.725)] [D acc: (0.531)(0.625, 0.438)] [G loss: 0.799] [G acc: 0.688]\n",
      "4752 [D loss: (0.860)(R 0.855, F 0.865)] [D acc: (0.406)(0.625, 0.188)] [G loss: 1.175] [G acc: 0.500]\n",
      "4753 [D loss: (0.733)(R 0.810, F 0.656)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.996] [G acc: 0.562]\n",
      "4754 [D loss: (0.684)(R 0.550, F 0.818)] [D acc: (0.531)(0.750, 0.312)] [G loss: 1.207] [G acc: 0.500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755 [D loss: (0.540)(R 0.443, F 0.638)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.202] [G acc: 0.438]\n",
      "4756 [D loss: (0.656)(R 0.588, F 0.724)] [D acc: (0.594)(0.750, 0.438)] [G loss: 1.396] [G acc: 0.312]\n",
      "4757 [D loss: (0.595)(R 0.573, F 0.618)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.043] [G acc: 0.562]\n",
      "4758 [D loss: (0.559)(R 0.531, F 0.586)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.322] [G acc: 0.500]\n",
      "4759 [D loss: (0.776)(R 0.912, F 0.640)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.891] [G acc: 0.500]\n",
      "4760 [D loss: (0.564)(R 0.501, F 0.626)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.435] [G acc: 0.500]\n",
      "4761 [D loss: (0.554)(R 0.414, F 0.694)] [D acc: (0.688)(1.000, 0.375)] [G loss: 1.050] [G acc: 0.562]\n",
      "4762 [D loss: (0.723)(R 0.736, F 0.710)] [D acc: (0.531)(0.625, 0.438)] [G loss: 1.253] [G acc: 0.438]\n",
      "4763 [D loss: (0.605)(R 0.569, F 0.642)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.129] [G acc: 0.562]\n",
      "4764 [D loss: (0.730)(R 0.539, F 0.920)] [D acc: (0.531)(0.812, 0.250)] [G loss: 1.464] [G acc: 0.312]\n",
      "4765 [D loss: (0.611)(R 0.502, F 0.720)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.126] [G acc: 0.375]\n",
      "4766 [D loss: (0.633)(R 0.579, F 0.688)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.252] [G acc: 0.500]\n",
      "4767 [D loss: (0.614)(R 0.422, F 0.805)] [D acc: (0.625)(0.938, 0.312)] [G loss: 0.899] [G acc: 0.375]\n",
      "4768 [D loss: (0.595)(R 0.655, F 0.534)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.292] [G acc: 0.250]\n",
      "4769 [D loss: (0.672)(R 0.764, F 0.580)] [D acc: (0.656)(0.688, 0.625)] [G loss: 2.035] [G acc: 0.125]\n",
      "4770 [D loss: (0.866)(R 0.987, F 0.744)] [D acc: (0.531)(0.562, 0.500)] [G loss: 2.041] [G acc: 0.312]\n",
      "4771 [D loss: (0.622)(R 0.779, F 0.464)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.584] [G acc: 0.312]\n",
      "4772 [D loss: (0.629)(R 0.651, F 0.607)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.944] [G acc: 0.312]\n",
      "4773 [D loss: (0.704)(R 0.781, F 0.627)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.095] [G acc: 0.500]\n",
      "4774 [D loss: (0.729)(R 0.702, F 0.756)] [D acc: (0.594)(0.750, 0.438)] [G loss: 1.156] [G acc: 0.438]\n",
      "4775 [D loss: (0.748)(R 0.640, F 0.856)] [D acc: (0.562)(0.812, 0.312)] [G loss: 1.107] [G acc: 0.375]\n",
      "4776 [D loss: (0.595)(R 0.596, F 0.593)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.247] [G acc: 0.438]\n",
      "4777 [D loss: (0.619)(R 0.588, F 0.650)] [D acc: (0.719)(0.812, 0.625)] [G loss: 0.936] [G acc: 0.500]\n",
      "4778 [D loss: (0.678)(R 0.750, F 0.606)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.039] [G acc: 0.500]\n",
      "4779 [D loss: (0.734)(R 0.760, F 0.709)] [D acc: (0.562)(0.688, 0.438)] [G loss: 1.179] [G acc: 0.438]\n",
      "4780 [D loss: (0.597)(R 0.559, F 0.635)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.358] [G acc: 0.438]\n",
      "4781 [D loss: (0.661)(R 0.706, F 0.617)] [D acc: (0.688)(0.750, 0.625)] [G loss: 0.964] [G acc: 0.562]\n",
      "4782 [D loss: (0.690)(R 0.675, F 0.704)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.091] [G acc: 0.375]\n",
      "4783 [D loss: (0.701)(R 0.563, F 0.839)] [D acc: (0.594)(0.875, 0.312)] [G loss: 1.370] [G acc: 0.375]\n",
      "4784 [D loss: (0.714)(R 0.653, F 0.776)] [D acc: (0.500)(0.625, 0.375)] [G loss: 0.803] [G acc: 0.562]\n",
      "4785 [D loss: (0.634)(R 0.498, F 0.770)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.117] [G acc: 0.562]\n",
      "4786 [D loss: (0.552)(R 0.452, F 0.653)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.060] [G acc: 0.562]\n",
      "4787 [D loss: (0.616)(R 0.630, F 0.601)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.064] [G acc: 0.312]\n",
      "4788 [D loss: (0.726)(R 0.701, F 0.751)] [D acc: (0.500)(0.688, 0.312)] [G loss: 1.112] [G acc: 0.375]\n",
      "4789 [D loss: (0.703)(R 0.713, F 0.693)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.025] [G acc: 0.500]\n",
      "4790 [D loss: (0.646)(R 0.531, F 0.762)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.150] [G acc: 0.562]\n",
      "4791 [D loss: (0.521)(R 0.579, F 0.463)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.331] [G acc: 0.438]\n",
      "4792 [D loss: (0.828)(R 0.892, F 0.764)] [D acc: (0.438)(0.562, 0.312)] [G loss: 1.277] [G acc: 0.438]\n",
      "4793 [D loss: (0.630)(R 0.657, F 0.603)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.046] [G acc: 0.375]\n",
      "4794 [D loss: (0.589)(R 0.532, F 0.646)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.083] [G acc: 0.562]\n",
      "4795 [D loss: (0.716)(R 0.810, F 0.621)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.356] [G acc: 0.188]\n",
      "4796 [D loss: (0.504)(R 0.589, F 0.419)] [D acc: (0.844)(0.812, 0.875)] [G loss: 1.147] [G acc: 0.438]\n",
      "4797 [D loss: (0.544)(R 0.591, F 0.496)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.418] [G acc: 0.438]\n",
      "4798 [D loss: (0.467)(R 0.511, F 0.424)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.981] [G acc: 0.375]\n",
      "4799 [D loss: (0.577)(R 0.575, F 0.578)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.338] [G acc: 0.562]\n",
      "4800 [D loss: (0.531)(R 0.504, F 0.558)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.561] [G acc: 0.312]\n",
      "4801 [D loss: (0.589)(R 0.519, F 0.659)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.470] [G acc: 0.312]\n",
      "4802 [D loss: (0.684)(R 0.735, F 0.632)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.167] [G acc: 0.188]\n",
      "4803 [D loss: (0.488)(R 0.619, F 0.357)] [D acc: (0.875)(0.812, 0.938)] [G loss: 1.512] [G acc: 0.312]\n",
      "4804 [D loss: (0.559)(R 0.447, F 0.671)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.111] [G acc: 0.500]\n",
      "4805 [D loss: (0.739)(R 0.948, F 0.530)] [D acc: (0.688)(0.750, 0.625)] [G loss: 0.988] [G acc: 0.562]\n",
      "4806 [D loss: (0.736)(R 0.722, F 0.750)] [D acc: (0.562)(0.688, 0.438)] [G loss: 1.176] [G acc: 0.312]\n",
      "4807 [D loss: (0.630)(R 0.631, F 0.629)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.903] [G acc: 0.688]\n",
      "4808 [D loss: (0.600)(R 0.608, F 0.592)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.023] [G acc: 0.438]\n",
      "4809 [D loss: (0.739)(R 0.654, F 0.824)] [D acc: (0.594)(0.750, 0.438)] [G loss: 0.976] [G acc: 0.562]\n",
      "4810 [D loss: (0.505)(R 0.467, F 0.543)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.030] [G acc: 0.375]\n",
      "4811 [D loss: (0.560)(R 0.475, F 0.644)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.473] [G acc: 0.250]\n",
      "4812 [D loss: (0.547)(R 0.439, F 0.655)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.179] [G acc: 0.312]\n",
      "4813 [D loss: (0.702)(R 0.584, F 0.820)] [D acc: (0.594)(0.875, 0.312)] [G loss: 1.210] [G acc: 0.500]\n",
      "4814 [D loss: (0.596)(R 0.613, F 0.578)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.349] [G acc: 0.375]\n",
      "4815 [D loss: (0.686)(R 0.830, F 0.542)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.252] [G acc: 0.375]\n",
      "4816 [D loss: (0.570)(R 0.554, F 0.586)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.039] [G acc: 0.500]\n",
      "4817 [D loss: (0.496)(R 0.498, F 0.495)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.192] [G acc: 0.188]\n",
      "4818 [D loss: (0.565)(R 0.663, F 0.468)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.326] [G acc: 0.500]\n",
      "4819 [D loss: (0.761)(R 0.832, F 0.690)] [D acc: (0.562)(0.625, 0.500)] [G loss: 1.210] [G acc: 0.312]\n",
      "4820 [D loss: (0.655)(R 0.682, F 0.627)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.287] [G acc: 0.438]\n",
      "4821 [D loss: (0.581)(R 0.507, F 0.656)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.161] [G acc: 0.500]\n",
      "4822 [D loss: (0.486)(R 0.499, F 0.474)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.534] [G acc: 0.375]\n",
      "4823 [D loss: (0.459)(R 0.403, F 0.514)] [D acc: (0.812)(1.000, 0.625)] [G loss: 1.638] [G acc: 0.312]\n",
      "4824 [D loss: (0.576)(R 0.655, F 0.496)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.701] [G acc: 0.312]\n",
      "4825 [D loss: (0.730)(R 0.915, F 0.545)] [D acc: (0.719)(0.812, 0.625)] [G loss: 2.380] [G acc: 0.125]\n",
      "4826 [D loss: (0.524)(R 0.451, F 0.597)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.262] [G acc: 0.438]\n",
      "4827 [D loss: (0.558)(R 0.555, F 0.561)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.371] [G acc: 0.438]\n",
      "4828 [D loss: (0.604)(R 0.493, F 0.714)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.218] [G acc: 0.500]\n",
      "4829 [D loss: (0.655)(R 0.711, F 0.600)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.141] [G acc: 0.562]\n",
      "4830 [D loss: (0.492)(R 0.429, F 0.554)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.368] [G acc: 0.375]\n",
      "4831 [D loss: (0.564)(R 0.603, F 0.526)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.110] [G acc: 0.250]\n",
      "4832 [D loss: (0.628)(R 0.618, F 0.637)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.963] [G acc: 0.312]\n",
      "4833 [D loss: (0.709)(R 0.949, F 0.470)] [D acc: (0.750)(0.812, 0.688)] [G loss: 2.398] [G acc: 0.188]\n",
      "4834 [D loss: (0.518)(R 0.525, F 0.512)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.309] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835 [D loss: (0.656)(R 0.692, F 0.621)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.900] [G acc: 0.500]\n",
      "4836 [D loss: (0.361)(R 0.356, F 0.365)] [D acc: (0.875)(1.000, 0.750)] [G loss: 1.385] [G acc: 0.438]\n",
      "4837 [D loss: (0.966)(R 1.212, F 0.720)] [D acc: (0.562)(0.625, 0.500)] [G loss: 1.026] [G acc: 0.562]\n",
      "4838 [D loss: (0.723)(R 0.670, F 0.775)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.272] [G acc: 0.375]\n",
      "4839 [D loss: (0.563)(R 0.499, F 0.627)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.194] [G acc: 0.250]\n",
      "4840 [D loss: (0.571)(R 0.515, F 0.628)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.338] [G acc: 0.312]\n",
      "4841 [D loss: (0.601)(R 0.519, F 0.683)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.233] [G acc: 0.438]\n",
      "4842 [D loss: (0.554)(R 0.434, F 0.673)] [D acc: (0.750)(1.000, 0.500)] [G loss: 1.419] [G acc: 0.250]\n",
      "4843 [D loss: (0.596)(R 0.650, F 0.541)] [D acc: (0.719)(0.812, 0.625)] [G loss: 1.005] [G acc: 0.375]\n",
      "4844 [D loss: (0.598)(R 0.530, F 0.665)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.253] [G acc: 0.438]\n",
      "4845 [D loss: (0.650)(R 0.567, F 0.734)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.388] [G acc: 0.312]\n",
      "4846 [D loss: (0.577)(R 0.575, F 0.578)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.705] [G acc: 0.375]\n",
      "4847 [D loss: (0.831)(R 0.824, F 0.838)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.039] [G acc: 0.500]\n",
      "4848 [D loss: (0.634)(R 0.509, F 0.759)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.423] [G acc: 0.312]\n",
      "4849 [D loss: (0.656)(R 0.720, F 0.593)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.091] [G acc: 0.438]\n",
      "4850 [D loss: (0.498)(R 0.425, F 0.572)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.353] [G acc: 0.250]\n",
      "4851 [D loss: (0.752)(R 0.557, F 0.947)] [D acc: (0.531)(0.812, 0.250)] [G loss: 1.209] [G acc: 0.500]\n",
      "4852 [D loss: (0.632)(R 0.546, F 0.718)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.045] [G acc: 0.500]\n",
      "4853 [D loss: (0.738)(R 0.846, F 0.629)] [D acc: (0.625)(0.688, 0.562)] [G loss: 1.137] [G acc: 0.500]\n",
      "4854 [D loss: (0.722)(R 0.725, F 0.718)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.997] [G acc: 0.500]\n",
      "4855 [D loss: (0.656)(R 0.495, F 0.818)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.299] [G acc: 0.500]\n",
      "4856 [D loss: (0.690)(R 0.796, F 0.584)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.051] [G acc: 0.312]\n",
      "4857 [D loss: (0.613)(R 0.484, F 0.742)] [D acc: (0.688)(0.875, 0.500)] [G loss: 0.962] [G acc: 0.500]\n",
      "4858 [D loss: (0.488)(R 0.433, F 0.542)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.112] [G acc: 0.500]\n",
      "4859 [D loss: (0.605)(R 0.434, F 0.776)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.466] [G acc: 0.562]\n",
      "4860 [D loss: (0.565)(R 0.583, F 0.547)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.429] [G acc: 0.500]\n",
      "4861 [D loss: (0.474)(R 0.438, F 0.511)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.407] [G acc: 0.375]\n",
      "4862 [D loss: (0.674)(R 0.566, F 0.783)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.099] [G acc: 0.438]\n",
      "4863 [D loss: (0.766)(R 0.837, F 0.695)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.118] [G acc: 0.500]\n",
      "4864 [D loss: (0.586)(R 0.577, F 0.596)] [D acc: (0.719)(0.875, 0.562)] [G loss: 0.976] [G acc: 0.438]\n",
      "4865 [D loss: (0.652)(R 0.683, F 0.621)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.019] [G acc: 0.500]\n",
      "4866 [D loss: (0.630)(R 0.616, F 0.643)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.052] [G acc: 0.438]\n",
      "4867 [D loss: (0.898)(R 0.893, F 0.903)] [D acc: (0.531)(0.812, 0.250)] [G loss: 1.434] [G acc: 0.438]\n",
      "4868 [D loss: (0.642)(R 0.523, F 0.760)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.595] [G acc: 0.438]\n",
      "4869 [D loss: (0.628)(R 0.583, F 0.673)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.210] [G acc: 0.500]\n",
      "4870 [D loss: (0.647)(R 0.714, F 0.581)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.318] [G acc: 0.500]\n",
      "4871 [D loss: (0.642)(R 0.666, F 0.618)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.037] [G acc: 0.500]\n",
      "4872 [D loss: (0.521)(R 0.410, F 0.631)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.896] [G acc: 0.188]\n",
      "4873 [D loss: (0.591)(R 0.582, F 0.600)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.605] [G acc: 0.312]\n",
      "4874 [D loss: (0.722)(R 0.812, F 0.632)] [D acc: (0.562)(0.625, 0.500)] [G loss: 1.629] [G acc: 0.375]\n",
      "4875 [D loss: (0.433)(R 0.407, F 0.460)] [D acc: (0.875)(1.000, 0.750)] [G loss: 1.667] [G acc: 0.250]\n",
      "4876 [D loss: (0.615)(R 0.684, F 0.546)] [D acc: (0.688)(0.750, 0.625)] [G loss: 0.925] [G acc: 0.562]\n",
      "4877 [D loss: (0.561)(R 0.685, F 0.436)] [D acc: (0.781)(0.812, 0.750)] [G loss: 1.086] [G acc: 0.500]\n",
      "4878 [D loss: (0.530)(R 0.487, F 0.573)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.059] [G acc: 0.438]\n",
      "4879 [D loss: (0.745)(R 0.709, F 0.782)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.017] [G acc: 0.500]\n",
      "4880 [D loss: (0.723)(R 0.568, F 0.877)] [D acc: (0.531)(0.750, 0.312)] [G loss: 0.972] [G acc: 0.562]\n",
      "4881 [D loss: (0.576)(R 0.547, F 0.605)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.116] [G acc: 0.438]\n",
      "4882 [D loss: (0.458)(R 0.525, F 0.391)] [D acc: (0.812)(0.812, 0.812)] [G loss: 0.985] [G acc: 0.500]\n",
      "4883 [D loss: (0.539)(R 0.543, F 0.535)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.251] [G acc: 0.500]\n",
      "4884 [D loss: (0.586)(R 0.474, F 0.699)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.162] [G acc: 0.500]\n",
      "4885 [D loss: (0.618)(R 0.732, F 0.504)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.163] [G acc: 0.438]\n",
      "4886 [D loss: (0.639)(R 0.647, F 0.632)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.007] [G acc: 0.562]\n",
      "4887 [D loss: (0.588)(R 0.631, F 0.545)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.060] [G acc: 0.562]\n",
      "4888 [D loss: (0.612)(R 0.521, F 0.702)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.297] [G acc: 0.438]\n",
      "4889 [D loss: (0.531)(R 0.510, F 0.553)] [D acc: (0.719)(0.812, 0.625)] [G loss: 0.880] [G acc: 0.438]\n",
      "4890 [D loss: (0.587)(R 0.465, F 0.709)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.500] [G acc: 0.438]\n",
      "4891 [D loss: (0.615)(R 0.508, F 0.722)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.409] [G acc: 0.500]\n",
      "4892 [D loss: (0.729)(R 0.783, F 0.674)] [D acc: (0.625)(0.812, 0.438)] [G loss: 0.949] [G acc: 0.562]\n",
      "4893 [D loss: (0.503)(R 0.413, F 0.594)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.068] [G acc: 0.375]\n",
      "4894 [D loss: (0.651)(R 0.766, F 0.536)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.241] [G acc: 0.375]\n",
      "4895 [D loss: (0.708)(R 0.625, F 0.792)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.306] [G acc: 0.375]\n",
      "4896 [D loss: (0.609)(R 0.464, F 0.754)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.285] [G acc: 0.438]\n",
      "4897 [D loss: (0.707)(R 0.573, F 0.842)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.183] [G acc: 0.500]\n",
      "4898 [D loss: (0.600)(R 0.474, F 0.726)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.226] [G acc: 0.438]\n",
      "4899 [D loss: (0.560)(R 0.472, F 0.648)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.215] [G acc: 0.500]\n",
      "4900 [D loss: (0.506)(R 0.427, F 0.584)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.526] [G acc: 0.375]\n",
      "4901 [D loss: (0.628)(R 0.732, F 0.524)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.289] [G acc: 0.375]\n",
      "4902 [D loss: (0.603)(R 0.533, F 0.673)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.060] [G acc: 0.562]\n",
      "4903 [D loss: (0.668)(R 0.575, F 0.761)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.172] [G acc: 0.438]\n",
      "4904 [D loss: (0.654)(R 0.591, F 0.717)] [D acc: (0.625)(0.750, 0.500)] [G loss: 0.884] [G acc: 0.562]\n",
      "4905 [D loss: (0.708)(R 0.626, F 0.789)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.151] [G acc: 0.438]\n",
      "4906 [D loss: (0.570)(R 0.559, F 0.580)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.209] [G acc: 0.312]\n",
      "4907 [D loss: (0.714)(R 0.620, F 0.809)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.198] [G acc: 0.375]\n",
      "4908 [D loss: (0.568)(R 0.413, F 0.723)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.230] [G acc: 0.500]\n",
      "4909 [D loss: (0.455)(R 0.451, F 0.459)] [D acc: (0.844)(0.938, 0.750)] [G loss: 1.486] [G acc: 0.250]\n",
      "4910 [D loss: (0.710)(R 0.672, F 0.748)] [D acc: (0.562)(0.750, 0.375)] [G loss: 1.077] [G acc: 0.562]\n",
      "4911 [D loss: (0.653)(R 0.544, F 0.761)] [D acc: (0.594)(0.875, 0.312)] [G loss: 1.152] [G acc: 0.562]\n",
      "4912 [D loss: (0.521)(R 0.559, F 0.483)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.303] [G acc: 0.500]\n",
      "4913 [D loss: (0.652)(R 0.433, F 0.871)] [D acc: (0.625)(0.938, 0.312)] [G loss: 1.365] [G acc: 0.562]\n",
      "4914 [D loss: (0.572)(R 0.497, F 0.648)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.279] [G acc: 0.438]\n",
      "4915 [D loss: (0.579)(R 0.531, F 0.627)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.549] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4916 [D loss: (0.548)(R 0.532, F 0.563)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.864] [G acc: 0.312]\n",
      "4917 [D loss: (0.749)(R 0.818, F 0.680)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.398] [G acc: 0.562]\n",
      "4918 [D loss: (0.462)(R 0.466, F 0.458)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.762] [G acc: 0.438]\n",
      "4919 [D loss: (0.900)(R 1.039, F 0.761)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.249] [G acc: 0.500]\n",
      "4920 [D loss: (0.624)(R 0.429, F 0.820)] [D acc: (0.625)(1.000, 0.250)] [G loss: 1.284] [G acc: 0.438]\n",
      "4921 [D loss: (0.452)(R 0.435, F 0.470)] [D acc: (0.844)(1.000, 0.688)] [G loss: 1.516] [G acc: 0.500]\n",
      "4922 [D loss: (0.560)(R 0.474, F 0.646)] [D acc: (0.656)(0.875, 0.438)] [G loss: 2.304] [G acc: 0.312]\n",
      "4923 [D loss: (0.588)(R 0.459, F 0.718)] [D acc: (0.688)(1.000, 0.375)] [G loss: 1.337] [G acc: 0.500]\n",
      "4924 [D loss: (0.564)(R 0.534, F 0.594)] [D acc: (0.656)(0.812, 0.500)] [G loss: 2.323] [G acc: 0.375]\n",
      "4925 [D loss: (0.702)(R 0.706, F 0.698)] [D acc: (0.594)(0.812, 0.375)] [G loss: 1.534] [G acc: 0.375]\n",
      "4926 [D loss: (0.888)(R 1.128, F 0.647)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.171] [G acc: 0.500]\n",
      "4927 [D loss: (0.452)(R 0.448, F 0.457)] [D acc: (0.781)(0.875, 0.688)] [G loss: 1.171] [G acc: 0.250]\n",
      "4928 [D loss: (0.706)(R 0.907, F 0.504)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.314] [G acc: 0.438]\n",
      "4929 [D loss: (0.550)(R 0.498, F 0.602)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.540] [G acc: 0.312]\n",
      "4930 [D loss: (0.718)(R 0.756, F 0.680)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.410] [G acc: 0.375]\n",
      "4931 [D loss: (0.440)(R 0.446, F 0.434)] [D acc: (0.781)(0.938, 0.625)] [G loss: 1.134] [G acc: 0.375]\n",
      "4932 [D loss: (0.582)(R 0.512, F 0.651)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.391] [G acc: 0.562]\n",
      "4933 [D loss: (0.515)(R 0.619, F 0.411)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.266] [G acc: 0.375]\n",
      "4934 [D loss: (0.623)(R 0.561, F 0.685)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.677] [G acc: 0.438]\n",
      "4935 [D loss: (0.564)(R 0.566, F 0.562)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.793] [G acc: 0.312]\n",
      "4936 [D loss: (0.660)(R 0.680, F 0.641)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.443] [G acc: 0.562]\n",
      "4937 [D loss: (0.545)(R 0.478, F 0.612)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.203] [G acc: 0.438]\n",
      "4938 [D loss: (0.675)(R 0.708, F 0.642)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.299] [G acc: 0.375]\n",
      "4939 [D loss: (0.527)(R 0.556, F 0.498)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.423] [G acc: 0.188]\n",
      "4940 [D loss: (0.536)(R 0.594, F 0.477)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.052] [G acc: 0.562]\n",
      "4941 [D loss: (0.521)(R 0.482, F 0.561)] [D acc: (0.812)(0.938, 0.688)] [G loss: 1.183] [G acc: 0.562]\n",
      "4942 [D loss: (0.622)(R 0.571, F 0.673)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.530] [G acc: 0.375]\n",
      "4943 [D loss: (0.416)(R 0.465, F 0.366)] [D acc: (0.844)(0.875, 0.812)] [G loss: 1.107] [G acc: 0.500]\n",
      "4944 [D loss: (0.583)(R 0.469, F 0.697)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.170] [G acc: 0.562]\n",
      "4945 [D loss: (0.576)(R 0.459, F 0.694)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.279] [G acc: 0.562]\n",
      "4946 [D loss: (0.663)(R 0.618, F 0.708)] [D acc: (0.625)(0.812, 0.438)] [G loss: 2.097] [G acc: 0.250]\n",
      "4947 [D loss: (0.537)(R 0.487, F 0.588)] [D acc: (0.750)(0.875, 0.625)] [G loss: 1.471] [G acc: 0.188]\n",
      "4948 [D loss: (0.565)(R 0.706, F 0.425)] [D acc: (0.719)(0.750, 0.688)] [G loss: 2.453] [G acc: 0.125]\n",
      "4949 [D loss: (0.489)(R 0.462, F 0.515)] [D acc: (0.844)(0.938, 0.750)] [G loss: 2.006] [G acc: 0.500]\n",
      "4950 [D loss: (0.505)(R 0.607, F 0.403)] [D acc: (0.812)(0.812, 0.812)] [G loss: 1.358] [G acc: 0.312]\n",
      "4951 [D loss: (0.555)(R 0.516, F 0.593)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.863] [G acc: 0.188]\n",
      "4952 [D loss: (0.588)(R 0.651, F 0.526)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.169] [G acc: 0.438]\n",
      "4953 [D loss: (0.637)(R 0.758, F 0.516)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.096] [G acc: 0.625]\n",
      "4954 [D loss: (0.640)(R 0.651, F 0.629)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.477] [G acc: 0.250]\n",
      "4955 [D loss: (0.634)(R 0.598, F 0.670)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.514] [G acc: 0.375]\n",
      "4956 [D loss: (0.508)(R 0.429, F 0.588)] [D acc: (0.781)(1.000, 0.562)] [G loss: 1.626] [G acc: 0.375]\n",
      "4957 [D loss: (0.537)(R 0.622, F 0.453)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.404] [G acc: 0.500]\n",
      "4958 [D loss: (0.644)(R 0.642, F 0.647)] [D acc: (0.656)(0.812, 0.500)] [G loss: 1.485] [G acc: 0.438]\n",
      "4959 [D loss: (0.541)(R 0.505, F 0.578)] [D acc: (0.688)(0.812, 0.562)] [G loss: 1.562] [G acc: 0.500]\n",
      "4960 [D loss: (0.656)(R 0.679, F 0.634)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.336] [G acc: 0.375]\n",
      "4961 [D loss: (0.561)(R 0.451, F 0.672)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.678] [G acc: 0.375]\n",
      "4962 [D loss: (0.694)(R 0.703, F 0.685)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.432] [G acc: 0.438]\n",
      "4963 [D loss: (0.591)(R 0.661, F 0.520)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.345] [G acc: 0.375]\n",
      "4964 [D loss: (0.813)(R 1.083, F 0.543)] [D acc: (0.594)(0.562, 0.625)] [G loss: 1.312] [G acc: 0.625]\n",
      "4965 [D loss: (0.615)(R 0.431, F 0.798)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.592] [G acc: 0.375]\n",
      "4966 [D loss: (0.595)(R 0.479, F 0.711)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.084] [G acc: 0.375]\n",
      "4967 [D loss: (0.589)(R 0.631, F 0.547)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.491] [G acc: 0.438]\n",
      "4968 [D loss: (0.684)(R 0.574, F 0.793)] [D acc: (0.562)(0.812, 0.312)] [G loss: 1.336] [G acc: 0.562]\n",
      "4969 [D loss: (0.633)(R 0.578, F 0.687)] [D acc: (0.594)(0.750, 0.438)] [G loss: 0.984] [G acc: 0.500]\n",
      "4970 [D loss: (0.608)(R 0.501, F 0.714)] [D acc: (0.688)(0.875, 0.500)] [G loss: 1.061] [G acc: 0.562]\n",
      "4971 [D loss: (0.613)(R 0.631, F 0.594)] [D acc: (0.594)(0.688, 0.500)] [G loss: 1.049] [G acc: 0.375]\n",
      "4972 [D loss: (0.580)(R 0.425, F 0.735)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.173] [G acc: 0.438]\n",
      "4973 [D loss: (0.732)(R 0.799, F 0.664)] [D acc: (0.625)(0.750, 0.500)] [G loss: 0.910] [G acc: 0.625]\n",
      "4974 [D loss: (0.672)(R 0.452, F 0.893)] [D acc: (0.594)(0.938, 0.250)] [G loss: 0.974] [G acc: 0.500]\n",
      "4975 [D loss: (0.635)(R 0.710, F 0.561)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.102] [G acc: 0.375]\n",
      "4976 [D loss: (0.543)(R 0.558, F 0.528)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.426] [G acc: 0.500]\n",
      "4977 [D loss: (0.474)(R 0.550, F 0.399)] [D acc: (0.812)(0.875, 0.750)] [G loss: 1.363] [G acc: 0.438]\n",
      "4978 [D loss: (0.596)(R 0.535, F 0.656)] [D acc: (0.656)(0.750, 0.562)] [G loss: 1.973] [G acc: 0.250]\n",
      "4979 [D loss: (0.663)(R 0.640, F 0.686)] [D acc: (0.625)(0.812, 0.438)] [G loss: 1.090] [G acc: 0.625]\n",
      "4980 [D loss: (0.648)(R 0.569, F 0.727)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.142] [G acc: 0.562]\n",
      "4981 [D loss: (0.569)(R 0.477, F 0.660)] [D acc: (0.719)(1.000, 0.438)] [G loss: 1.191] [G acc: 0.312]\n",
      "4982 [D loss: (0.599)(R 0.498, F 0.700)] [D acc: (0.656)(0.875, 0.438)] [G loss: 0.920] [G acc: 0.438]\n",
      "4983 [D loss: (0.673)(R 0.526, F 0.821)] [D acc: (0.594)(0.938, 0.250)] [G loss: 1.444] [G acc: 0.312]\n",
      "4984 [D loss: (0.611)(R 0.588, F 0.634)] [D acc: (0.719)(0.938, 0.500)] [G loss: 1.312] [G acc: 0.500]\n",
      "4985 [D loss: (0.646)(R 0.615, F 0.677)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.294] [G acc: 0.500]\n",
      "4986 [D loss: (0.549)(R 0.437, F 0.661)] [D acc: (0.688)(0.938, 0.438)] [G loss: 1.154] [G acc: 0.438]\n",
      "4987 [D loss: (0.821)(R 0.901, F 0.741)] [D acc: (0.562)(0.750, 0.375)] [G loss: 0.760] [G acc: 0.625]\n",
      "4988 [D loss: (0.619)(R 0.532, F 0.706)] [D acc: (0.594)(0.812, 0.375)] [G loss: 0.971] [G acc: 0.562]\n",
      "4989 [D loss: (0.563)(R 0.514, F 0.611)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.271] [G acc: 0.562]\n",
      "4990 [D loss: (0.594)(R 0.571, F 0.618)] [D acc: (0.719)(0.875, 0.562)] [G loss: 1.337] [G acc: 0.438]\n",
      "4991 [D loss: (0.578)(R 0.636, F 0.521)] [D acc: (0.750)(0.812, 0.688)] [G loss: 1.496] [G acc: 0.438]\n",
      "4992 [D loss: (0.761)(R 0.896, F 0.625)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.229] [G acc: 0.500]\n",
      "4993 [D loss: (0.608)(R 0.509, F 0.707)] [D acc: (0.656)(0.875, 0.438)] [G loss: 1.135] [G acc: 0.375]\n",
      "4994 [D loss: (0.781)(R 0.689, F 0.873)] [D acc: (0.594)(0.875, 0.312)] [G loss: 0.974] [G acc: 0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4995 [D loss: (0.620)(R 0.544, F 0.695)] [D acc: (0.625)(0.750, 0.500)] [G loss: 1.364] [G acc: 0.438]\n",
      "4996 [D loss: (0.672)(R 0.778, F 0.566)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.158] [G acc: 0.312]\n",
      "4997 [D loss: (0.557)(R 0.601, F 0.513)] [D acc: (0.750)(0.938, 0.562)] [G loss: 1.138] [G acc: 0.500]\n",
      "4998 [D loss: (0.629)(R 0.491, F 0.766)] [D acc: (0.625)(0.875, 0.375)] [G loss: 1.069] [G acc: 0.500]\n",
      "4999 [D loss: (0.691)(R 0.583, F 0.799)] [D acc: (0.656)(0.938, 0.375)] [G loss: 1.060] [G acc: 0.375]\n"
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    y_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    ")\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 5000)\n",
    "plt.ylim(0, 12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#n_to_show = 10\n",
    "#example_idx = np.random.choice(range(len(x_test)), n_to_show)\n",
    "#example_images = x_test[example_idx]\n",
    "#example_test   = y_test[example_idx]\n",
    "\n",
    "#gan.generator()\n",
    "\n",
    "#z_points = gan.discriminator.predict(example_images)\n",
    "#print('z_points: ', z_points)\n",
    "#reconst_images = AE.decoder.predict(z_points)\n",
    "#print('reconstituted: {}'.format(reconst_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01123055  0.00792482 -0.00472485  0.01926836  0.01925022  0.03880921\n",
      "   0.06427465  0.00533733  0.00663657  0.01958878  0.00632452  0.0160875\n",
      "   0.01363848 -0.01098616  0.00472126  0.02013531  0.0325086   0.06336999\n",
      "   0.06557898  0.01705048 -0.01801371 -0.01359054  0.04054878 -0.0169384\n",
      "   0.01189579  0.072663    0.08333021  0.15131882  0.04134703  0.05476469\n",
      "   0.02475159  0.06754696  0.02407375  0.0419841   0.06013798]]\n",
      "[[0.58968204]]\n",
      "Prob Sum:  1.0193834\n"
     ]
    }
   ],
   "source": [
    "test_gen = gan.generator.predict([list(np.random.random(z_num))])\n",
    "print(test_gen)\n",
    "print(gan.discriminator.predict(test_gen))\n",
    "print('Prob Sum: ', np.sum(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in gan.d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in gan.d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 5000)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
